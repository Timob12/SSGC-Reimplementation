{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c9ae74",
   "metadata": {},
   "source": [
    "# Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8525eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import configparser\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from script import dataloader, utility, earlystopping\n",
    "from model import models\n",
    "\n",
    "import nni\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "def ConfigSectionMap(section):\n",
    "    dict1 = {}\n",
    "    options = config.options(section)\n",
    "    for option in options:\n",
    "        try:\n",
    "            dict1[option] = config.get(section, option)\n",
    "            if dict1[option] == -1:\n",
    "                logging.debug('skip: %s' % option)\n",
    "        except:\n",
    "            print('exception on %s!' % option)\n",
    "            dict1[option] = None\n",
    "    return dict1\n",
    "\n",
    "def get_parameters(dataset):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    if dataset == \"cora\":\n",
    "        dataset_config_path = './config/data/cora.ini'\n",
    "    elif dataset == \"citeseer\":\n",
    "        dataset_config_path = './config/data/citeseer.ini'\n",
    "    elif dataset == \"pubmed\":\n",
    "        dataset_config_path = './config/data/pubmed.ini'\n",
    "        \n",
    "    model_config_path = './config/model/ssgc_sym.ini'\n",
    "\n",
    "    config.read(dataset_config_path, encoding='utf-8')\n",
    "    dataset_name = ConfigSectionMap('dataset')['name']\n",
    "    data_path = './data/'\n",
    "    graph_path = None\n",
    "    learning_rate = float(ConfigSectionMap('model')['learning_rate'])\n",
    "    weight_decay_rate = float(ConfigSectionMap('model')['weight_decay_rate'])\n",
    "    model_save_path = ConfigSectionMap('model')['model_save_path']\n",
    "    \n",
    "    config.read(model_config_path, encoding='utf-8')\n",
    "    model_name = ConfigSectionMap('model')['name']\n",
    "    renorm_adj_type = ConfigSectionMap('gconv')['renorm_adj_type']\n",
    "    \n",
    "    alpha = 0.05\n",
    "    K = 16\n",
    "    enable_bias = True\n",
    "    epochs = 100#reduced because otherwise it takes too long (original 10000)\n",
    "    opt = \"Adam\"\n",
    "    early_stopping_patience = 10\n",
    "    \n",
    "    model_save_path = model_save_path + model_name + '_' + str(renorm_adj_type) + '_' + str(alpha) + \\\n",
    "                    '_alpha_' + str(K) + '_power' + '.pth'\n",
    "    \n",
    "    return device, dataset_name, data_path, graph_path, learning_rate, weight_decay_rate, model_name, \\\n",
    "        model_save_path, renorm_adj_type, alpha, K, enable_bias, epochs, opt, early_stopping_patience\n",
    "\n",
    "def process_data(device, dataset_name, data_path, graph_path, renorm_adj_type, alpha, K):\n",
    "\n",
    "    if dataset_name == 'cora' or dataset_name == 'citeseer' or dataset_name == 'pubmed':\n",
    "        features, labels, dir_adj, idx_train, idx_val, idx_test = dataloader.load_citation_data(dataset_name, data_path)\n",
    "    elif dataset_name == 'cornell' or dataset_name == 'texas' or dataset_name == 'washington' or dataset_name == 'wisconsin':\n",
    "        features, dir_adj, labels, idx_train, idx_val, idx_test = dataloader.load_webkb_data(dataset_name, data_path)\n",
    "    elif dataset_name == 'arxiv':\n",
    "        dataset = PygNodePropPredDataset(name='ogbn-arxiv')\n",
    "        data = dataset[0]\n",
    "        unique_labels_cnt = len(set([y[0] for y in data.y.tolist()]))\n",
    "        labels = []\n",
    "        for i in range(len(data.x)):\n",
    "            label = [0]*unique_labels_cnt\n",
    "            label[data.y[i]] = 1\n",
    "            labels.append(label)\n",
    "        labels = np.array(labels)\n",
    "        graph = torch_geometric.utils.to_networkx(data)\n",
    "        dir_adj = nx.adjacency_matrix(graph)\n",
    "        dir_adj = scipy.sparse.csc_matrix(dir_adj)\n",
    "        features = data.x.numpy()\n",
    "        row_sum_inv = np.power(np.sum(features, axis=1), -1)\n",
    "        row_sum_inv[np.isinf(row_sum_inv)] = 0.\n",
    "        norm_features = []\n",
    "        for i in range(len(row_sum_inv)):\n",
    "            elems = []\n",
    "            for j in range(features.shape[1]):\n",
    "                elems.append(row_sum_inv[i] * features[i][j]) \n",
    "            norm_features.append(elems)\n",
    "        norm_features = sp.csc_matrix(norm_features)\n",
    "        features = norm_features\n",
    "        split_idx = dataset.get_idx_split()\n",
    "        idx_train = split_idx['train']\n",
    "        idx_val = split_idx['valid']\n",
    "        idx_test = split_idx['test']\n",
    "    elif dataset_name == 'mag':\n",
    "        dataset = PygNodePropPredDataset(name='ogbn-mag')\n",
    "        data = dataset[0]\n",
    "        data = Data(x=data.x_dict['paper'], edge_index=data.edge_index_dict[('paper', 'cites', 'paper')], y=data.y_dict['paper'])\n",
    "        unique_labels_cnt = len(set([y[0] for y in data.y.tolist()]))\n",
    "        labels = []\n",
    "        for i in range(len(data.x)):\n",
    "            label = [0]*unique_labels_cnt\n",
    "            label[data.y[i]] = 1\n",
    "            labels.append(label)\n",
    "        labels = np.array(labels)\n",
    "        graph = torch_geometric.utils.to_networkx(data)\n",
    "        dir_adj = nx.adjacency_matrix(graph)\n",
    "        dir_adj = scipy.sparse.csc_matrix(dir_adj)\n",
    "        features = data.x.numpy()\n",
    "        row_sum_inv = np.power(np.sum(features, axis=1), -1)\n",
    "        row_sum_inv[np.isinf(row_sum_inv)] = 0.\n",
    "        norm_features = []\n",
    "        for i in range(len(row_sum_inv)):\n",
    "            elems = []\n",
    "            for j in range(features.shape[1]):\n",
    "                elems.append(row_sum_inv[i] * features[i][j]) \n",
    "            norm_features.append(elems)\n",
    "        norm_features = sp.csc_matrix(norm_features)\n",
    "        features = norm_features\n",
    "        split_idx = dataset.get_idx_split()\n",
    "        idx_train = split_idx['train']\n",
    "        idx_val = split_idx['valid']\n",
    "        idx_test = split_idx['test']\n",
    "    elif dataset_name == 'reddit':\n",
    "        dataset = Reddit(\"./dataset/reddit\")\n",
    "        data = dataset[0]\n",
    "        unique_label_cnt = len(set(data.y.tolist()))\n",
    "        labels = []\n",
    "        y_aslist = data.y.tolist()\n",
    "        y_len = len(data.y)\n",
    "        for i in range(y_len):\n",
    "            label = [0]*unique_label_cnt\n",
    "            idx = y_aslist[i]\n",
    "            label[idx] = 1\n",
    "            labels.append(label)\n",
    "        graph = torch_geometric.utils.to_networkx(data)\n",
    "        dir_adj = nx.adjacency_matrix(graph)\n",
    "        dir_adj = scipy.sparse.csc_matrix(dir_adj)\n",
    "        features = data.x.numpy()\n",
    "        row_sum_inv = np.power(np.sum(features, axis=1), -1)\n",
    "        row_sum_inv[np.isinf(row_sum_inv)] = 0.\n",
    "        norm_features = []\n",
    "        for i in range(len(row_sum_inv)):\n",
    "            elems = []\n",
    "            for j in range(features.shape[1]):\n",
    "                elems.append(row_sum_inv[i] * features[i][j]) \n",
    "            norm_features.append(elems)\n",
    "        norm_features = sp.csc_matrix(norm_features)\n",
    "        features = norm_features\n",
    "        split_idx = dataset.get_idx_split()\n",
    "        idx_train = split_idx['train']\n",
    "        idx_val = split_idx['valid']\n",
    "        idx_test = split_idx['test']\n",
    "    elif dataset_name == 'products':\n",
    "        dataset = PygNodePropPredDataset(name='ogbn-products')\n",
    "        data = dataset[0]\n",
    "        unique_labels_cnt = len(set([y[0] for y in data.y.tolist()]))\n",
    "        labels = []\n",
    "        for i in range(len(data.x)):\n",
    "            label = [0]*unique_labels_cnt\n",
    "            label[data.y[i]] = 1\n",
    "            labels.append(label)\n",
    "        labels = np.array(labels)\n",
    "        graph = torch_geometric.utils.to_networkx(data)\n",
    "        dir_adj = nx.adjacency_matrix(graph)\n",
    "        dir_adj = scipy.sparse.csc_matrix(dir_adj)\n",
    "        features = data.x.numpy()\n",
    "        row_sum_inv = np.power(np.sum(features, axis=1), -1)\n",
    "        row_sum_inv[np.isinf(row_sum_inv)] = 0.\n",
    "        norm_features = []\n",
    "        for i in range(len(row_sum_inv)):\n",
    "            elems = []\n",
    "            for j in range(features.shape[1]):\n",
    "                elems.append(row_sum_inv[i] * features[i][j]) \n",
    "            norm_features.append(elems)\n",
    "        norm_features = sp.csc_matrix(norm_features)\n",
    "        features = norm_features\n",
    "        split_idx = dataset.get_idx_split()\n",
    "        idx_train = split_idx['train']\n",
    "        idx_val = split_idx['valid']\n",
    "        idx_test = split_idx['test']\n",
    "\n",
    "    n_vertex, n_feat, n_labels, n_class = features.shape[0], features.shape[1], labels.shape[0], labels.shape[1]\n",
    "    labels = np.argmax(labels, axis=1)\n",
    "\n",
    "    if renorm_adj_type == 'sym':\n",
    "        renorm_adj = utility.calc_sym_renorm_adj(dir_adj)\n",
    "    elif renorm_adj_type == 'rw':\n",
    "        renorm_adj = utility.calc_rw_renorm_adj(dir_adj)\n",
    "    \n",
    "    features = utility.calc_adj_mul_feat(renorm_adj, features, alpha, K)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train).to(device)\n",
    "    idx_val = torch.LongTensor(idx_val).to(device)\n",
    "    idx_test = torch.LongTensor(idx_test).to(device)\n",
    "\n",
    "    features = torch.from_numpy(features).to(device)\n",
    "    labels = torch.LongTensor(labels).to(device)\n",
    "\n",
    "    return features, labels, idx_train, idx_val, idx_test, n_feat, n_class, n_vertex\n",
    "\n",
    "def prepare_model(n_feat, n_class, enable_bias, early_stopping_patience, learning_rate, \\\n",
    "                weight_decay_rate, model_save_path, opt):\n",
    "    model = models.SSGC(n_feat, n_class, enable_bias).to(device)\n",
    "    loss = nn.NLLLoss()\n",
    "    early_stopping = earlystopping.EarlyStopping(patience=early_stopping_patience, path=model_save_path, verbose=False)\n",
    "\n",
    "    if opt == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_rate)\n",
    "    elif opt == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_rate)\n",
    "    else:\n",
    "        raise ValueError(f'ERROR: optimizer {opt} is undefined.')\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.95)\n",
    "\n",
    "    return model, loss, early_stopping, optimizer, scheduler\n",
    "\n",
    "def train(epochs, model, optimizer, scheduler, early_stopping, features, labels, loss, idx_train, idx_val, dataset_name):\n",
    "    train_time_list = []\n",
    "    for epoch in range(epochs):\n",
    "        train_epoch_begin_time = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features)\n",
    "        loss_train = loss(output[idx_train], labels[idx_train])\n",
    "        acc_train = utility.calc_accuracy(output[idx_train], labels[idx_train])\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        train_epoch_end_time = time.time()\n",
    "        train_epoch_time_duration = train_epoch_end_time - train_epoch_begin_time\n",
    "        train_time_list.append(train_epoch_time_duration)\n",
    "\n",
    "        loss_val, acc_val = val(model, labels, output, loss, idx_val)\n",
    "        #print('Epoch: {:03d} | Learning rate: {:.8f} | Train loss: {:.6f} | Train acc: {:.6f} | Val loss: {:.6f} | Val acc: {:.6f} | Training duration: {:.6f}'.\\\n",
    "        #    format(epoch+1, optimizer.param_groups[0]['lr'], loss_train.item(), acc_train.item(), loss_val.item(), acc_val.item(), train_epoch_time_duration))\n",
    "        #nni.report_intermediate_result(acc_val.item())\n",
    "\n",
    "        early_stopping(loss_val, model)\n",
    "        if early_stopping.early_stop:\n",
    "            #print('Early stopping.')\n",
    "            break\n",
    "    \n",
    "    mean_train_epoch_time_duration = np.mean(train_time_list)\n",
    "    #print('\\nTraining finished.\\n')\n",
    "\n",
    "    return mean_train_epoch_time_duration\n",
    "\n",
    "def val(model, labels, output, loss, idx_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_val = loss(output[idx_val], labels[idx_val])\n",
    "        acc_val = utility.calc_accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "    return loss_val, acc_val\n",
    "\n",
    "def test(model, model_save_path, features, labels, loss, idx_test, model_name, dataset_name, mean_train_epoch_time_duration):\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features)\n",
    "        loss_test = loss(output[idx_test], labels[idx_test])\n",
    "        acc_test = utility.calc_accuracy(output[idx_test], labels[idx_test])\n",
    "        #nmi_test = utility.calc_nmi(output[idx_test], labels[idx_test])\n",
    "        #f1_test = utility.calc_f1(output[idx_test], labels[idx_test])\n",
    "        #print('Model: {} | Dataset: {} | Test loss: {:.6f} | Test acc: {:.6f} | Training duration: {:.6f}'.\\\n",
    "        #    format(model_name, dataset_name, loss_test.item(), acc_test.item(), mean_train_epoch_time_duration))\n",
    "        #nni.report_final_result(acc_test.item())\n",
    "        return acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8681f1",
   "metadata": {},
   "source": [
    "## Small/Medium Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70ffd8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: cora , average accuracy: tensor(0.7978, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/myprojectdir/SSGC_new/SSGC/script/dataloader.py:8: RuntimeWarning: divide by zero encountered in power\n",
      "  row_sum_inv = np.power(np.sum(features, axis=1), -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: citeseer , average accuracy: tensor(0.6790, dtype=torch.float64)\n",
      "dataset: pubmed , average accuracy: tensor(0.7857, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"cora\", \"citeseer\", \"pubmed\"]\n",
    "for dataset in datasets:\n",
    "    average_acc = 0\n",
    "    for _ in range(10):\n",
    "        device, dataset_name, data_path, graph_path, learning_rate, weight_decay_rate, model_name, model_save_path, renorm_adj_type, alpha, K, enable_bias, epochs, opt, early_stopping_patience = get_parameters(dataset)\n",
    "\n",
    "        features, labels, idx_train, idx_val, idx_test, n_feat, n_class, n_vertex = process_data(device, dataset_name, data_path, graph_path, renorm_adj_type, alpha, K)\n",
    "\n",
    "        model, loss, early_stopping, optimizer, scheduler = prepare_model(n_feat, n_class, enable_bias, early_stopping_patience, learning_rate, weight_decay_rate, model_save_path, opt)\n",
    "\n",
    "        mean_train_epoch_time_duration = train(epochs, model, optimizer, scheduler, early_stopping, features, labels, loss, idx_train, idx_val, dataset_name)\n",
    "\n",
    "        average_acc += test(model, model_save_path, features, labels, loss, idx_test, model_name, dataset_name, mean_train_epoch_time_duration)\n",
    "    average_acc /= 10\n",
    "    print(\"dataset:\", dataset, \", average accuracy:\", average_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab9017",
   "metadata": {},
   "source": [
    "## Alpha and K Value Comparison on Cora, Citeseer, Pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e1b7781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: cora , average accuracy: tensor(0.7930, dtype=torch.float64) alpha: 0.0\n",
      "dataset: cora , average accuracy: tensor(0.7950, dtype=torch.float64) alpha: 0.05\n",
      "dataset: cora , average accuracy: tensor(0.8050, dtype=torch.float64) alpha: 0.1\n",
      "dataset: cora , average accuracy: tensor(0.8110, dtype=torch.float64) alpha: 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kali/myprojectdir/SSGC_new/SSGC/script/dataloader.py:8: RuntimeWarning: divide by zero encountered in power\n",
      "  row_sum_inv = np.power(np.sum(features, axis=1), -1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping counter: 1 out of 10\n",
      "Early stopping counter: 2 out of 10\n",
      "Early stopping counter: 3 out of 10\n",
      "Early stopping counter: 4 out of 10\n",
      "Early stopping counter: 5 out of 10\n",
      "Early stopping counter: 6 out of 10\n",
      "Early stopping counter: 7 out of 10\n",
      "Early stopping counter: 8 out of 10\n",
      "Early stopping counter: 9 out of 10\n",
      "Early stopping counter: 10 out of 10\n",
      "dataset: citeseer , average accuracy: tensor(0.6770, dtype=torch.float64) alpha: 0.0\n",
      "Early stopping counter: 1 out of 10\n",
      "Early stopping counter: 2 out of 10\n",
      "Early stopping counter: 3 out of 10\n",
      "Early stopping counter: 4 out of 10\n",
      "Early stopping counter: 5 out of 10\n",
      "Early stopping counter: 6 out of 10\n",
      "Early stopping counter: 7 out of 10\n",
      "Early stopping counter: 8 out of 10\n",
      "Early stopping counter: 9 out of 10\n",
      "Early stopping counter: 10 out of 10\n",
      "dataset: citeseer , average accuracy: tensor(0.6830, dtype=torch.float64) alpha: 0.05\n",
      "Early stopping counter: 1 out of 10\n",
      "Early stopping counter: 1 out of 10\n",
      "Early stopping counter: 2 out of 10\n",
      "Early stopping counter: 3 out of 10\n",
      "Early stopping counter: 4 out of 10\n",
      "Early stopping counter: 5 out of 10\n",
      "Early stopping counter: 6 out of 10\n",
      "Early stopping counter: 7 out of 10\n",
      "Early stopping counter: 1 out of 10\n",
      "Early stopping counter: 2 out of 10\n",
      "Early stopping counter: 3 out of 10\n",
      "Early stopping counter: 4 out of 10\n",
      "Early stopping counter: 5 out of 10\n",
      "Early stopping counter: 6 out of 10\n",
      "Early stopping counter: 7 out of 10\n",
      "Early stopping counter: 8 out of 10\n",
      "Early stopping counter: 9 out of 10\n",
      "Early stopping counter: 10 out of 10\n",
      "dataset: citeseer , average accuracy: tensor(0.6770, dtype=torch.float64) alpha: 0.1\n",
      "Early stopping counter: 1 out of 10\n",
      "Early stopping counter: 2 out of 10\n",
      "Early stopping counter: 3 out of 10\n",
      "Early stopping counter: 4 out of 10\n",
      "Early stopping counter: 5 out of 10\n",
      "Early stopping counter: 6 out of 10\n",
      "Early stopping counter: 7 out of 10\n",
      "Early stopping counter: 8 out of 10\n",
      "Early stopping counter: 9 out of 10\n",
      "Early stopping counter: 10 out of 10\n",
      "dataset: citeseer , average accuracy: tensor(0.6790, dtype=torch.float64) alpha: 0.15\n",
      "dataset: pubmed , average accuracy: tensor(0.7800, dtype=torch.float64) alpha: 0.0\n",
      "Early stopping counter: 1 out of 10\n",
      "Early stopping counter: 2 out of 10\n",
      "Early stopping counter: 3 out of 10\n",
      "Early stopping counter: 4 out of 10\n",
      "Early stopping counter: 5 out of 10\n",
      "dataset: pubmed , average accuracy: tensor(0.7860, dtype=torch.float64) alpha: 0.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a13e4c3a083e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenorm_adj_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_patience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vertex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenorm_adj_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_patience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1d9d4d65ff7d>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(device, dataset_name, data_path, graph_path, renorm_adj_type, alpha, K)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cora'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'citeseer'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pubmed'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_citation_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cornell'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'texas'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'washington'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'wisconsin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_webkb_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myprojectdir/SSGC_new/SSGC/script/dataloader.py\u001b[0m in \u001b[0;36mload_citation_data\u001b[0;34m(dataset_name, dataset_path)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx_reorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjacency_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict_of_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myprojectdir/SSGC_new/SSGC/script/dataloader.py\u001b[0m in \u001b[0;36mnorm_feat\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrow_sum_inv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_sum_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdeg_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_sum_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnorm_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeg_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mnorm_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datasets = [\"cora\", \"citeseer\", \"pubmed\"]\n",
    "alpha_ = [0.0, 0.05, 0.1, 0.15]\n",
    "K_ = [2, 4, 8, 16, 32, 64]\n",
    "alpha_standard = 0.05\n",
    "K_standard = 16\n",
    "for dataset in datasets:\n",
    "    K = K_standard\n",
    "    for alpha in alpha_:\n",
    "        device, dataset_name, data_path, graph_path, learning_rate, weight_decay_rate, model_name, model_save_path, renorm_adj_type, _, __, enable_bias, epochs, opt, early_stopping_patience = get_parameters(dataset)\n",
    "\n",
    "        features, labels, idx_train, idx_val, idx_test, n_feat, n_class, n_vertex = process_data(device, dataset_name, data_path, graph_path, renorm_adj_type, alpha, K)\n",
    "\n",
    "        model, loss, early_stopping, optimizer, scheduler = prepare_model(n_feat, n_class, enable_bias, early_stopping_patience, learning_rate, weight_decay_rate, model_save_path, opt)\n",
    "\n",
    "        mean_train_epoch_time_duration = train(epochs, model, optimizer, scheduler, early_stopping, features, labels, loss, idx_train, idx_val, dataset_name)\n",
    "\n",
    "        acc = test(model, model_save_path, features, labels, loss, idx_test, model_name, dataset_name, mean_train_epoch_time_duration)\n",
    "\n",
    "        print(\"dataset:\", dataset, \", average accuracy:\", acc, \"alpha:\", alpha)\n",
    "for dataset in datasets:\n",
    "    alpha = alpha_standard\n",
    "    for K in K_:\n",
    "        device, dataset_name, data_path, graph_path, learning_rate, weight_decay_rate, model_name, model_save_path, renorm_adj_type, _, __, enable_bias, epochs, opt, early_stopping_patience = get_parameters(dataset)\n",
    "\n",
    "        features, labels, idx_train, idx_val, idx_test, n_feat, n_class, n_vertex = process_data(device, dataset_name, data_path, graph_path, renorm_adj_type, alpha, K)\n",
    "\n",
    "        model, loss, early_stopping, optimizer, scheduler = prepare_model(n_feat, n_class, enable_bias, early_stopping_patience, learning_rate, weight_decay_rate, model_save_path, opt)\n",
    "\n",
    "        mean_train_epoch_time_duration = train(epochs, model, optimizer, scheduler, early_stopping, features, labels, loss, idx_train, idx_val, dataset_name)\n",
    "\n",
    "        acc = test(model, model_save_path, features, labels, loss, idx_test, model_name, dataset_name, mean_train_epoch_time_duration)\n",
    "\n",
    "        print(\"dataset:\", dataset, \", average accuracy:\", acc, \"K:\", K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55498fa7",
   "metadata": {},
   "source": [
    "## Large OGB Datasets (SSGC) / Community Prediction (Reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "047a1cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csc.csc_matrix'>\n",
      "Epoch: 001 | Learning rate: 0.20000000 | Train loss: 3.976514 | Train acc: 0.034022 | Val loss: 3.780668 | Val acc: 0.024900 | Training duration: 0.316380\n",
      "[2022-01-23 23:06:15] INFO (nni/MainThread) Intermediate result: 0.024900164435048156  (Index 100)\n",
      "Epoch: 002 | Learning rate: 0.20000000 | Train loss: 3.785760 | Train acc: 0.205540 | Val loss: 3.375981 | Val acc: 0.181147 | Training duration: 0.405493\n",
      "[2022-01-23 23:06:15] INFO (nni/MainThread) Intermediate result: 0.1811470183563207  (Index 101)\n",
      "Epoch: 003 | Learning rate: 0.20000000 | Train loss: 4.677960 | Train acc: 0.270890 | Val loss: 3.901505 | Val acc: 0.260445 | Training duration: 0.379934\n",
      "[2022-01-23 23:06:16] INFO (nni/MainThread) Intermediate result: 0.26044498137521394  (Index 102)\n",
      "Early stopping counter: 1 out of 1000\n",
      "Epoch: 004 | Learning rate: 0.20000000 | Train loss: 6.538178 | Train acc: 0.246687 | Val loss: 5.097915 | Val acc: 0.254908 | Training duration: 0.380181\n",
      "[2022-01-23 23:06:16] INFO (nni/MainThread) Intermediate result: 0.25490788281485954  (Index 103)\n",
      "Early stopping counter: 2 out of 1000\n",
      "Epoch: 005 | Learning rate: 0.20000000 | Train loss: 7.556878 | Train acc: 0.263984 | Val loss: 5.569923 | Val acc: 0.282157 | Training duration: 0.402255\n",
      "[2022-01-23 23:06:17] INFO (nni/MainThread) Intermediate result: 0.2821571193664217  (Index 104)\n",
      "Early stopping counter: 3 out of 1000\n",
      "Epoch: 006 | Learning rate: 0.20000000 | Train loss: 6.950157 | Train acc: 0.252977 | Val loss: 5.156458 | Val acc: 0.272895 | Training duration: 0.405273\n",
      "[2022-01-23 23:06:17] INFO (nni/MainThread) Intermediate result: 0.272895063592738  (Index 105)\n",
      "Early stopping counter: 4 out of 1000\n",
      "Epoch: 007 | Learning rate: 0.20000000 | Train loss: 5.966346 | Train acc: 0.278895 | Val loss: 4.645785 | Val acc: 0.309440 | Training duration: 0.443257\n",
      "[2022-01-23 23:06:17] INFO (nni/MainThread) Intermediate result: 0.3094399140910769  (Index 106)\n",
      "Early stopping counter: 5 out of 1000\n",
      "Epoch: 008 | Learning rate: 0.20000000 | Train loss: 6.301920 | Train acc: 0.265260 | Val loss: 5.174120 | Val acc: 0.236417 | Training duration: 0.389196\n",
      "[2022-01-23 23:06:18] INFO (nni/MainThread) Intermediate result: 0.23641732944058524  (Index 107)\n",
      "Early stopping counter: 6 out of 1000\n",
      "Epoch: 009 | Learning rate: 0.20000000 | Train loss: 6.445467 | Train acc: 0.281270 | Val loss: 5.306715 | Val acc: 0.240948 | Training duration: 0.371009\n",
      "[2022-01-23 23:06:18] INFO (nni/MainThread) Intermediate result: 0.24094768280814793  (Index 108)\n",
      "Early stopping counter: 7 out of 1000\n",
      "Epoch: 010 | Learning rate: 0.20000000 | Train loss: 6.017158 | Train acc: 0.281105 | Val loss: 5.177938 | Val acc: 0.235343 | Training duration: 0.392305\n",
      "[2022-01-23 23:06:19] INFO (nni/MainThread) Intermediate result: 0.23534346790160743  (Index 109)\n",
      "Early stopping counter: 8 out of 1000\n",
      "Epoch: 011 | Learning rate: 0.20000000 | Train loss: 5.844040 | Train acc: 0.276278 | Val loss: 5.178796 | Val acc: 0.225075 | Training duration: 0.363112\n",
      "[2022-01-23 23:06:19] INFO (nni/MainThread) Intermediate result: 0.22507466693513206  (Index 110)\n",
      "Early stopping counter: 9 out of 1000\n",
      "Epoch: 012 | Learning rate: 0.20000000 | Train loss: 5.830582 | Train acc: 0.257903 | Val loss: 5.120420 | Val acc: 0.205544 | Training duration: 0.398357\n",
      "[2022-01-23 23:06:20] INFO (nni/MainThread) Intermediate result: 0.20554381019497298  (Index 111)\n",
      "Early stopping counter: 10 out of 1000\n",
      "Epoch: 013 | Learning rate: 0.20000000 | Train loss: 5.640651 | Train acc: 0.281512 | Val loss: 4.843211 | Val acc: 0.259103 | Training duration: 0.412823\n",
      "[2022-01-23 23:06:20] INFO (nni/MainThread) Intermediate result: 0.25910265445149167  (Index 112)\n",
      "Early stopping counter: 11 out of 1000\n",
      "Epoch: 014 | Learning rate: 0.20000000 | Train loss: 5.059994 | Train acc: 0.308541 | Val loss: 4.441989 | Val acc: 0.302896 | Training duration: 0.374531\n",
      "[2022-01-23 23:06:20] INFO (nni/MainThread) Intermediate result: 0.3028960703379308  (Index 113)\n",
      "Early stopping counter: 12 out of 1000\n",
      "Epoch: 015 | Learning rate: 0.20000000 | Train loss: 4.528290 | Train acc: 0.331446 | Val loss: 4.050028 | Val acc: 0.344508 | Training duration: 0.381396\n",
      "[2022-01-23 23:06:21] INFO (nni/MainThread) Intermediate result: 0.34450820497332124  (Index 114)\n",
      "Early stopping counter: 13 out of 1000\n",
      "Epoch: 016 | Learning rate: 0.20000000 | Train loss: 4.327375 | Train acc: 0.327289 | Val loss: 3.946816 | Val acc: 0.311823 | Training duration: 0.386271\n",
      "[2022-01-23 23:06:21] INFO (nni/MainThread) Intermediate result: 0.3118225443806839  (Index 115)\n",
      "Early stopping counter: 14 out of 1000\n",
      "Epoch: 017 | Learning rate: 0.20000000 | Train loss: 4.148248 | Train acc: 0.340408 | Val loss: 3.804033 | Val acc: 0.316621 | Training duration: 0.378410\n",
      "[2022-01-23 23:06:22] INFO (nni/MainThread) Intermediate result: 0.31662136313299105  (Index 116)\n",
      "Early stopping counter: 15 out of 1000\n",
      "Epoch: 018 | Learning rate: 0.20000000 | Train loss: 4.042850 | Train acc: 0.339242 | Val loss: 3.797901 | Val acc: 0.315950 | Training duration: 0.376631\n",
      "[2022-01-23 23:06:22] INFO (nni/MainThread) Intermediate result: 0.3159501996711299  (Index 117)\n",
      "Early stopping counter: 16 out of 1000\n",
      "Epoch: 019 | Learning rate: 0.20000000 | Train loss: 4.227863 | Train acc: 0.347269 | Val loss: 3.618133 | Val acc: 0.344374 | Training duration: 0.377955\n",
      "[2022-01-23 23:06:22] INFO (nni/MainThread) Intermediate result: 0.344373972280949  (Index 118)\n",
      "Early stopping counter: 17 out of 1000\n",
      "Epoch: 020 | Learning rate: 0.20000000 | Train loss: 4.157564 | Train acc: 0.354494 | Val loss: 3.543172 | Val acc: 0.356858 | Training duration: 0.336004\n",
      "[2022-01-23 23:06:23] INFO (nni/MainThread) Intermediate result: 0.3568576126715662  (Index 119)\n",
      "Early stopping counter: 18 out of 1000\n",
      "Epoch: 021 | Learning rate: 0.20000000 | Train loss: 3.835283 | Train acc: 0.355956 | Val loss: 3.489465 | Val acc: 0.359542 | Training duration: 0.386991\n",
      "[2022-01-23 23:06:23] INFO (nni/MainThread) Intermediate result: 0.3595422665190107  (Index 120)\n",
      "Early stopping counter: 19 out of 1000\n",
      "Epoch: 022 | Learning rate: 0.20000000 | Train loss: 3.843673 | Train acc: 0.348754 | Val loss: 3.455441 | Val acc: 0.354005 | Training duration: 0.369314\n",
      "[2022-01-23 23:06:24] INFO (nni/MainThread) Intermediate result: 0.35400516795865633  (Index 121)\n",
      "Early stopping counter: 20 out of 1000\n",
      "Epoch: 023 | Learning rate: 0.20000000 | Train loss: 3.632022 | Train acc: 0.358727 | Val loss: 3.317856 | Val acc: 0.364542 | Training duration: 0.367668\n",
      "[2022-01-23 23:06:24] INFO (nni/MainThread) Intermediate result: 0.36454243430987615  (Index 122)\n",
      "Epoch: 024 | Learning rate: 0.20000000 | Train loss: 3.499584 | Train acc: 0.359893 | Val loss: 3.213539 | Val acc: 0.366388 | Training duration: 0.330956\n",
      "[2022-01-23 23:06:24] INFO (nni/MainThread) Intermediate result: 0.3663881338299943  (Index 123)\n",
      "Epoch: 025 | Learning rate: 0.20000000 | Train loss: 3.652115 | Train acc: 0.354549 | Val loss: 3.162368 | Val acc: 0.358804 | Training duration: 0.365180\n",
      "[2022-01-23 23:06:25] INFO (nni/MainThread) Intermediate result: 0.3588039867109635  (Index 124)\n",
      "Epoch: 026 | Learning rate: 0.20000000 | Train loss: 3.498703 | Train acc: 0.365611 | Val loss: 3.150017 | Val acc: 0.360281 | Training duration: 0.396660\n",
      "[2022-01-23 23:06:25] INFO (nni/MainThread) Intermediate result: 0.36028054632705797  (Index 125)\n",
      "Epoch: 027 | Learning rate: 0.20000000 | Train loss: 3.511335 | Train acc: 0.361520 | Val loss: 3.281646 | Val acc: 0.341656 | Training duration: 0.383837\n",
      "[2022-01-23 23:06:26] INFO (nni/MainThread) Intermediate result: 0.34165576026041145  (Index 126)\n",
      "Early stopping counter: 1 out of 1000\n",
      "Epoch: 028 | Learning rate: 0.20000000 | Train loss: 3.478497 | Train acc: 0.366688 | Val loss: 3.312132 | Val acc: 0.353904 | Training duration: 0.405178\n",
      "[2022-01-23 23:06:26] INFO (nni/MainThread) Intermediate result: 0.3539044934393772  (Index 127)\n",
      "Early stopping counter: 2 out of 1000\n",
      "Epoch: 029 | Learning rate: 0.20000000 | Train loss: 3.431046 | Train acc: 0.370559 | Val loss: 3.231875 | Val acc: 0.363536 | Training duration: 0.407575\n",
      "[2022-01-23 23:06:26] INFO (nni/MainThread) Intermediate result: 0.36353568911708445  (Index 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping counter: 3 out of 1000\n",
      "Epoch: 030 | Learning rate: 0.20000000 | Train loss: 3.542371 | Train acc: 0.378718 | Val loss: 3.126051 | Val acc: 0.375080 | Training duration: 0.377053\n",
      "[2022-01-23 23:06:27] INFO (nni/MainThread) Intermediate result: 0.375079700661096  (Index 129)\n",
      "Epoch: 031 | Learning rate: 0.20000000 | Train loss: 3.522965 | Train acc: 0.374726 | Val loss: 3.051761 | Val acc: 0.349072 | Training duration: 0.338187\n",
      "[2022-01-23 23:06:27] INFO (nni/MainThread) Intermediate result: 0.34907211651397696  (Index 130)\n",
      "Epoch: 032 | Learning rate: 0.20000000 | Train loss: 3.559358 | Train acc: 0.370746 | Val loss: 3.114008 | Val acc: 0.335213 | Training duration: 0.374455\n",
      "[2022-01-23 23:06:28] INFO (nni/MainThread) Intermediate result: 0.3352125910265445  (Index 131)\n",
      "Early stopping counter: 1 out of 1000\n",
      "Epoch: 033 | Learning rate: 0.20000000 | Train loss: 3.368493 | Train acc: 0.383754 | Val loss: 3.157601 | Val acc: 0.348132 | Training duration: 0.332783\n",
      "[2022-01-23 23:06:28] INFO (nni/MainThread) Intermediate result: 0.34813248766737137  (Index 132)\n",
      "Early stopping counter: 2 out of 1000\n",
      "Epoch: 034 | Learning rate: 0.20000000 | Train loss: 3.529522 | Train acc: 0.403371 | Val loss: 3.137590 | Val acc: 0.392698 | Training duration: 0.353757\n",
      "[2022-01-23 23:06:28] INFO (nni/MainThread) Intermediate result: 0.39269774153495085  (Index 133)\n",
      "Early stopping counter: 3 out of 1000\n",
      "Epoch: 035 | Learning rate: 0.20000000 | Train loss: 3.643426 | Train acc: 0.408562 | Val loss: 3.211260 | Val acc: 0.406725 | Training duration: 0.363810\n",
      "[2022-01-23 23:06:29] INFO (nni/MainThread) Intermediate result: 0.4067250578878486  (Index 134)\n",
      "Early stopping counter: 4 out of 1000\n",
      "Epoch: 036 | Learning rate: 0.20000000 | Train loss: 3.693430 | Train acc: 0.415522 | Val loss: 3.282439 | Val acc: 0.415954 | Training duration: 0.409995\n",
      "[2022-01-23 23:06:29] INFO (nni/MainThread) Intermediate result: 0.4159535554884392  (Index 135)\n",
      "Early stopping counter: 5 out of 1000\n",
      "Epoch: 037 | Learning rate: 0.20000000 | Train loss: 3.712579 | Train acc: 0.414170 | Val loss: 3.336504 | Val acc: 0.417967 | Training duration: 0.347056\n",
      "[2022-01-23 23:06:30] INFO (nni/MainThread) Intermediate result: 0.4179670458740226  (Index 136)\n",
      "Early stopping counter: 6 out of 1000\n",
      "Epoch: 038 | Learning rate: 0.20000000 | Train loss: 3.724683 | Train acc: 0.412740 | Val loss: 3.356464 | Val acc: 0.412195 | Training duration: 0.368489\n",
      "[2022-01-23 23:06:30] INFO (nni/MainThread) Intermediate result: 0.41219504010201685  (Index 137)\n",
      "Early stopping counter: 7 out of 1000\n",
      "Epoch: 039 | Learning rate: 0.20000000 | Train loss: 3.732896 | Train acc: 0.410068 | Val loss: 3.336409 | Val acc: 0.403705 | Training duration: 0.376809\n",
      "[2022-01-23 23:06:31] INFO (nni/MainThread) Intermediate result: 0.40370482230947347  (Index 138)\n",
      "Early stopping counter: 8 out of 1000\n",
      "Epoch: 040 | Learning rate: 0.20000000 | Train loss: 3.513609 | Train acc: 0.414928 | Val loss: 3.231619 | Val acc: 0.409410 | Training duration: 0.391045\n",
      "[2022-01-23 23:06:31] INFO (nni/MainThread) Intermediate result: 0.4094097117352931  (Index 139)\n",
      "Early stopping counter: 9 out of 1000\n",
      "Epoch: 041 | Learning rate: 0.20000000 | Train loss: 3.736033 | Train acc: 0.424517 | Val loss: 3.191229 | Val acc: 0.415752 | Training duration: 0.383861\n",
      "[2022-01-23 23:06:31] INFO (nni/MainThread) Intermediate result: 0.41575220644988087  (Index 140)\n",
      "Early stopping counter: 10 out of 1000\n",
      "Epoch: 042 | Learning rate: 0.20000000 | Train loss: 3.601482 | Train acc: 0.433875 | Val loss: 3.105038 | Val acc: 0.421826 | Training duration: 0.388383\n",
      "[2022-01-23 23:06:32] INFO (nni/MainThread) Intermediate result: 0.42182623577972417  (Index 141)\n",
      "Early stopping counter: 11 out of 1000\n",
      "Epoch: 043 | Learning rate: 0.20000000 | Train loss: 3.521606 | Train acc: 0.418854 | Val loss: 3.144212 | Val acc: 0.391423 | Training duration: 0.399157\n",
      "[2022-01-23 23:06:32] INFO (nni/MainThread) Intermediate result: 0.3914225309574147  (Index 142)\n",
      "Early stopping counter: 12 out of 1000\n",
      "Epoch: 044 | Learning rate: 0.20000000 | Train loss: 3.284715 | Train acc: 0.421713 | Val loss: 2.967135 | Val acc: 0.394476 | Training duration: 0.370993\n",
      "[2022-01-23 23:06:33] INFO (nni/MainThread) Intermediate result: 0.39447632470888283  (Index 143)\n",
      "Epoch: 045 | Learning rate: 0.20000000 | Train loss: 3.204782 | Train acc: 0.424154 | Val loss: 2.953116 | Val acc: 0.395617 | Training duration: 0.338709\n",
      "[2022-01-23 23:06:33] INFO (nni/MainThread) Intermediate result: 0.39561730259404676  (Index 144)\n",
      "Epoch: 046 | Learning rate: 0.20000000 | Train loss: 3.291947 | Train acc: 0.414532 | Val loss: 3.000520 | Val acc: 0.371791 | Training duration: 0.337520\n",
      "[2022-01-23 23:06:33] INFO (nni/MainThread) Intermediate result: 0.37179099969797647  (Index 145)\n",
      "Early stopping counter: 1 out of 1000\n",
      "Epoch: 047 | Learning rate: 0.20000000 | Train loss: 3.489831 | Train acc: 0.413587 | Val loss: 3.126151 | Val acc: 0.364710 | Training duration: 0.365074\n",
      "[2022-01-23 23:06:34] INFO (nni/MainThread) Intermediate result: 0.36471022517534146  (Index 146)\n",
      "Early stopping counter: 2 out of 1000\n",
      "Epoch: 048 | Learning rate: 0.20000000 | Train loss: 3.348508 | Train acc: 0.440154 | Val loss: 2.930252 | Val acc: 0.409779 | Training duration: 0.389188\n",
      "[2022-01-23 23:06:34] INFO (nni/MainThread) Intermediate result: 0.40977885163931677  (Index 147)\n",
      "Epoch: 049 | Learning rate: 0.20000000 | Train loss: 3.300161 | Train acc: 0.436767 | Val loss: 2.905973 | Val acc: 0.407900 | Training duration: 0.362010\n",
      "[2022-01-23 23:06:35] INFO (nni/MainThread) Intermediate result: 0.4078995939461056  (Index 148)\n",
      "Epoch: 050 | Learning rate: 0.20000000 | Train loss: 3.222997 | Train acc: 0.437800 | Val loss: 2.887818 | Val acc: 0.408034 | Training duration: 0.341246\n",
      "[2022-01-23 23:06:35] INFO (nni/MainThread) Intermediate result: 0.4080338266384778  (Index 149)\n",
      "Epoch: 051 | Learning rate: 0.20000000 | Train loss: 3.171019 | Train acc: 0.444695 | Val loss: 2.862057 | Val acc: 0.413705 | Training duration: 0.375397\n",
      "[2022-01-23 23:06:35] INFO (nni/MainThread) Intermediate result: 0.4137051578912044  (Index 150)\n",
      "Epoch: 052 | Learning rate: 0.20000000 | Train loss: 3.104644 | Train acc: 0.443430 | Val loss: 2.814221 | Val acc: 0.409074 | Training duration: 0.370038\n",
      "[2022-01-23 23:06:36] INFO (nni/MainThread) Intermediate result: 0.40907413000436255  (Index 151)\n",
      "Epoch: 053 | Learning rate: 0.20000000 | Train loss: 3.051489 | Train acc: 0.440307 | Val loss: 2.758533 | Val acc: 0.405114 | Training duration: 0.338231\n",
      "[2022-01-23 23:06:36] INFO (nni/MainThread) Intermediate result: 0.40511426557938185  (Index 152)\n",
      "Epoch: 054 | Learning rate: 0.20000000 | Train loss: 3.092068 | Train acc: 0.421944 | Val loss: 2.765639 | Val acc: 0.378368 | Training duration: 0.344017\n",
      "[2022-01-23 23:06:37] INFO (nni/MainThread) Intermediate result: 0.3783684016242156  (Index 153)\n",
      "Early stopping counter: 1 out of 1000\n",
      "Epoch: 055 | Learning rate: 0.20000000 | Train loss: 3.102058 | Train acc: 0.418040 | Val loss: 2.734414 | Val acc: 0.374442 | Training duration: 0.361380\n",
      "[2022-01-23 23:06:37] INFO (nni/MainThread) Intermediate result: 0.3744420953723279  (Index 154)\n",
      "Epoch: 056 | Learning rate: 0.20000000 | Train loss: 2.800737 | Train acc: 0.430675 | Val loss: 2.630153 | Val acc: 0.394845 | Training duration: 0.397203\n",
      "[2022-01-23 23:06:37] INFO (nni/MainThread) Intermediate result: 0.3948454646129065  (Index 155)\n",
      "Epoch: 057 | Learning rate: 0.20000000 | Train loss: 2.834604 | Train acc: 0.444233 | Val loss: 2.586537 | Val acc: 0.418101 | Training duration: 0.336713\n",
      "[2022-01-23 23:06:38] INFO (nni/MainThread) Intermediate result: 0.41810127856639484  (Index 156)\n",
      "Epoch: 058 | Learning rate: 0.20000000 | Train loss: 2.834676 | Train acc: 0.431181 | Val loss: 2.611169 | Val acc: 0.390449 | Training duration: 0.358389\n",
      "[2022-01-23 23:06:38] INFO (nni/MainThread) Intermediate result: 0.390449343937716  (Index 157)\n",
      "Early stopping counter: 1 out of 1000\n",
      "Epoch: 059 | Learning rate: 0.20000000 | Train loss: 2.797851 | Train acc: 0.440681 | Val loss: 2.625536 | Val acc: 0.405148 | Training duration: 0.403954\n",
      "[2022-01-23 23:06:39] INFO (nni/MainThread) Intermediate result: 0.40514782375247493  (Index 158)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping counter: 2 out of 1000\n",
      "Epoch: 060 | Learning rate: 0.20000000 | Train loss: 2.995047 | Train acc: 0.445773 | Val loss: 2.689713 | Val acc: 0.415115 | Training duration: 0.390875\n",
      "[2022-01-23 23:06:39] INFO (nni/MainThread) Intermediate result: 0.41511460116111276  (Index 159)\n",
      "Early stopping counter: 3 out of 1000\n",
      "Epoch: 061 | Learning rate: 0.20000000 | Train loss: 2.900319 | Train acc: 0.482181 | Val loss: 2.668169 | Val acc: 0.475821 | Training duration: 0.362269\n",
      "[2022-01-23 23:06:39] INFO (nni/MainThread) Intermediate result: 0.47582133628645257  (Index 160)\n",
      "Early stopping counter: 4 out of 1000\n",
      "Epoch: 062 | Learning rate: 0.20000000 | Train loss: 2.790653 | Train acc: 0.504778 | Val loss: 2.640281 | Val acc: 0.524481 | Training duration: 0.376529\n",
      "[2022-01-23 23:06:40] INFO (nni/MainThread) Intermediate result: 0.524480687271385  (Index 161)\n",
      "Early stopping counter: 5 out of 1000\n",
      "Epoch: 063 | Learning rate: 0.20000000 | Train loss: 2.829446 | Train acc: 0.494496 | Val loss: 2.686730 | Val acc: 0.493406 | Training duration: 0.368868\n",
      "[2022-01-23 23:06:40] INFO (nni/MainThread) Intermediate result: 0.49340581898721436  (Index 162)\n",
      "Early stopping counter: 6 out of 1000\n",
      "Epoch: 064 | Learning rate: 0.20000000 | Train loss: 2.780256 | Train acc: 0.486502 | Val loss: 2.664821 | Val acc: 0.461593 | Training duration: 0.381210\n",
      "[2022-01-23 23:06:41] INFO (nni/MainThread) Intermediate result: 0.4615926708949965  (Index 163)\n",
      "Early stopping counter: 7 out of 1000\n",
      "Epoch: 065 | Learning rate: 0.20000000 | Train loss: 2.739136 | Train acc: 0.480421 | Val loss: 2.684455 | Val acc: 0.452666 | Training duration: 0.374195\n",
      "[2022-01-23 23:06:41] INFO (nni/MainThread) Intermediate result: 0.45266619685224335  (Index 164)\n",
      "Early stopping counter: 8 out of 1000\n",
      "Epoch: 066 | Learning rate: 0.20000000 | Train loss: 2.864088 | Train acc: 0.474055 | Val loss: 2.744819 | Val acc: 0.429310 | Training duration: 0.389983\n",
      "[2022-01-23 23:06:42] INFO (nni/MainThread) Intermediate result: 0.4293097083794758  (Index 165)\n",
      "Early stopping counter: 9 out of 1000\n",
      "Epoch: 067 | Learning rate: 0.20000000 | Train loss: 2.811532 | Train acc: 0.473516 | Val loss: 2.777516 | Val acc: 0.421021 | Training duration: 0.380178\n",
      "[2022-01-23 23:06:42] INFO (nni/MainThread) Intermediate result: 0.4210208396254908  (Index 166)\n",
      "Early stopping counter: 10 out of 1000\n",
      "Epoch: 068 | Learning rate: 0.20000000 | Train loss: 2.859877 | Train acc: 0.473736 | Val loss: 2.820161 | Val acc: 0.437901 | Training duration: 0.363471\n",
      "[2022-01-23 23:06:42] INFO (nni/MainThread) Intermediate result: 0.4379006006912984  (Index 167)\n",
      "Early stopping counter: 11 out of 1000\n",
      "Epoch: 069 | Learning rate: 0.20000000 | Train loss: 2.980843 | Train acc: 0.468139 | Val loss: 2.914952 | Val acc: 0.430719 | Training duration: 0.373534\n",
      "[2022-01-23 23:06:43] INFO (nni/MainThread) Intermediate result: 0.4307191516493842  (Index 168)\n",
      "Early stopping counter: 12 out of 1000\n",
      "Epoch: 070 | Learning rate: 0.20000000 | Train loss: 2.925053 | Train acc: 0.466060 | Val loss: 2.942698 | Val acc: 0.431793 | Training duration: 0.385469\n",
      "[2022-01-23 23:06:43] INFO (nni/MainThread) Intermediate result: 0.43179301318836205  (Index 169)\n",
      "Early stopping counter: 13 out of 1000\n",
      "Epoch: 071 | Learning rate: 0.20000000 | Train loss: 3.007479 | Train acc: 0.469392 | Val loss: 2.941690 | Val acc: 0.437330 | Training duration: 0.393732\n",
      "[2022-01-23 23:06:44] INFO (nni/MainThread) Intermediate result: 0.4373301117487164  (Index 170)\n",
      "Early stopping counter: 14 out of 1000\n",
      "Epoch: 072 | Learning rate: 0.20000000 | Train loss: 2.886129 | Train acc: 0.467754 | Val loss: 2.875675 | Val acc: 0.432296 | Training duration: 0.344393\n",
      "[2022-01-23 23:06:44] INFO (nni/MainThread) Intermediate result: 0.4322963857847579  (Index 171)\n",
      "Early stopping counter: 15 out of 1000\n",
      "Epoch: 073 | Learning rate: 0.20000000 | Train loss: 2.977238 | Train acc: 0.453536 | Val loss: 2.884176 | Val acc: 0.408806 | Training duration: 0.423438\n",
      "[2022-01-23 23:06:44] INFO (nni/MainThread) Intermediate result: 0.4088056646196181  (Index 172)\n",
      "Early stopping counter: 16 out of 1000\n",
      "Epoch: 074 | Learning rate: 0.20000000 | Train loss: 2.961992 | Train acc: 0.450622 | Val loss: 2.829152 | Val acc: 0.401926 | Training duration: 0.410146\n",
      "[2022-01-23 23:06:45] INFO (nni/MainThread) Intermediate result: 0.4019262391355415  (Index 173)\n",
      "Early stopping counter: 17 out of 1000\n",
      "Epoch: 075 | Learning rate: 0.20000000 | Train loss: 2.762378 | Train acc: 0.460430 | Val loss: 2.681150 | Val acc: 0.411725 | Training duration: 0.357208\n",
      "[2022-01-23 23:06:45] INFO (nni/MainThread) Intermediate result: 0.41172522567871406  (Index 174)\n",
      "Early stopping counter: 18 out of 1000\n",
      "Epoch: 076 | Learning rate: 0.20000000 | Train loss: 2.575633 | Train acc: 0.478860 | Val loss: 2.519095 | Val acc: 0.435317 | Training duration: 0.389507\n",
      "[2022-01-23 23:06:46] INFO (nni/MainThread) Intermediate result: 0.435316621363133  (Index 175)\n",
      "Epoch: 077 | Learning rate: 0.20000000 | Train loss: 2.677944 | Train acc: 0.475528 | Val loss: 2.536907 | Val acc: 0.417967 | Training duration: 0.343096\n",
      "[2022-01-23 23:06:46] INFO (nni/MainThread) Intermediate result: 0.4179670458740226  (Index 176)\n",
      "Early stopping counter: 1 out of 1000\n",
      "Epoch: 078 | Learning rate: 0.20000000 | Train loss: 2.740193 | Train acc: 0.469964 | Val loss: 2.556819 | Val acc: 0.413168 | Training duration: 0.371569\n",
      "[2022-01-23 23:06:47] INFO (nni/MainThread) Intermediate result: 0.41316822712171547  (Index 177)\n",
      "Early stopping counter: 2 out of 1000\n",
      "Epoch: 079 | Learning rate: 0.20000000 | Train loss: 2.799423 | Train acc: 0.501765 | Val loss: 2.472884 | Val acc: 0.505051 | Training duration: 0.362096\n",
      "[2022-01-23 23:06:47] INFO (nni/MainThread) Intermediate result: 0.5050505050505051  (Index 178)\n",
      "Epoch: 080 | Learning rate: 0.20000000 | Train loss: 2.691899 | Train acc: 0.494496 | Val loss: 2.462585 | Val acc: 0.497097 | Training duration: 0.338665\n",
      "[2022-01-23 23:06:47] INFO (nni/MainThread) Intermediate result: 0.4970972180274506  (Index 179)\n",
      "Epoch: 081 | Learning rate: 0.20000000 | Train loss: 2.736206 | Train acc: 0.487679 | Val loss: 2.485899 | Val acc: 0.478103 | Training duration: 0.361394\n",
      "[2022-01-23 23:06:48] INFO (nni/MainThread) Intermediate result: 0.4781032920567804  (Index 180)\n",
      "Early stopping counter: 1 out of 1000\n",
      "Epoch: 082 | Learning rate: 0.20000000 | Train loss: 2.861322 | Train acc: 0.484149 | Val loss: 2.501671 | Val acc: 0.473774 | Training duration: 0.363494\n",
      "[2022-01-23 23:06:48] INFO (nni/MainThread) Intermediate result: 0.4737742877277761  (Index 181)\n",
      "Early stopping counter: 2 out of 1000\n",
      "Epoch: 083 | Learning rate: 0.20000000 | Train loss: 2.891246 | Train acc: 0.483280 | Val loss: 2.512576 | Val acc: 0.464982 | Training duration: 0.403561\n",
      "[2022-01-23 23:06:49] INFO (nni/MainThread) Intermediate result: 0.4649820463773952  (Index 182)\n",
      "Early stopping counter: 3 out of 1000\n",
      "Epoch: 084 | Learning rate: 0.20000000 | Train loss: 2.756338 | Train acc: 0.474308 | Val loss: 2.526059 | Val acc: 0.436122 | Training duration: 0.378741\n",
      "[2022-01-23 23:06:49] INFO (nni/MainThread) Intermediate result: 0.43612201751736634  (Index 183)\n",
      "Early stopping counter: 4 out of 1000\n",
      "Epoch: 085 | Learning rate: 0.20000000 | Train loss: 3.004157 | Train acc: 0.453184 | Val loss: 2.666692 | Val acc: 0.395181 | Training duration: 0.386748\n",
      "[2022-01-23 23:06:49] INFO (nni/MainThread) Intermediate result: 0.39518104634383705  (Index 184)\n",
      "Early stopping counter: 5 out of 1000\n",
      "Epoch: 086 | Learning rate: 0.20000000 | Train loss: 3.005780 | Train acc: 0.452799 | Val loss: 2.715904 | Val acc: 0.395215 | Training duration: 0.390357\n",
      "[2022-01-23 23:06:50] INFO (nni/MainThread) Intermediate result: 0.3952146045169301  (Index 185)\n",
      "Early stopping counter: 6 out of 1000\n",
      "Epoch: 087 | Learning rate: 0.20000000 | Train loss: 2.832813 | Train acc: 0.468249 | Val loss: 2.617232 | Val acc: 0.416188 | Training duration: 0.373920\n",
      "[2022-01-23 23:06:50] INFO (nni/MainThread) Intermediate result: 0.41618846270009063  (Index 186)\n",
      "Early stopping counter: 7 out of 1000\n",
      "Epoch: 088 | Learning rate: 0.20000000 | Train loss: 2.870846 | Train acc: 0.483599 | Val loss: 2.578049 | Val acc: 0.442095 | Training duration: 0.382609\n",
      "[2022-01-23 23:06:51] INFO (nni/MainThread) Intermediate result: 0.44209537232793045  (Index 187)\n",
      "Early stopping counter: 8 out of 1000\n",
      "Epoch: 089 | Learning rate: 0.20000000 | Train loss: 2.926027 | Train acc: 0.479157 | Val loss: 2.620641 | Val acc: 0.422900 | Training duration: 0.392445\n",
      "[2022-01-23 23:06:51] INFO (nni/MainThread) Intermediate result: 0.422900097318702  (Index 188)\n",
      "Early stopping counter: 9 out of 1000\n",
      "Epoch: 090 | Learning rate: 0.20000000 | Train loss: 3.191932 | Train acc: 0.462135 | Val loss: 2.721623 | Val acc: 0.402665 | Training duration: 0.385328\n",
      "[2022-01-23 23:06:52] INFO (nni/MainThread) Intermediate result: 0.40266451894358873  (Index 189)\n",
      "Early stopping counter: 10 out of 1000\n",
      "Epoch: 091 | Learning rate: 0.20000000 | Train loss: 3.321609 | Train acc: 0.471075 | Val loss: 2.761157 | Val acc: 0.453539 | Training duration: 0.341713\n",
      "[2022-01-23 23:06:52] INFO (nni/MainThread) Intermediate result: 0.4535387093526628  (Index 190)\n",
      "Early stopping counter: 11 out of 1000\n",
      "Epoch: 092 | Learning rate: 0.20000000 | Train loss: 3.194749 | Train acc: 0.465522 | Val loss: 2.776739 | Val acc: 0.453874 | Training duration: 0.394048\n",
      "[2022-01-23 23:06:52] INFO (nni/MainThread) Intermediate result: 0.4538742910835934  (Index 191)\n",
      "Early stopping counter: 12 out of 1000\n",
      "Epoch: 093 | Learning rate: 0.20000000 | Train loss: 3.132172 | Train acc: 0.459892 | Val loss: 2.805984 | Val acc: 0.439914 | Training duration: 0.405483\n",
      "[2022-01-23 23:06:53] INFO (nni/MainThread) Intermediate result: 0.4399140910768818  (Index 192)\n",
      "Early stopping counter: 13 out of 1000\n",
      "Epoch: 094 | Learning rate: 0.20000000 | Train loss: 3.193479 | Train acc: 0.459386 | Val loss: 2.838377 | Val acc: 0.425283 | Training duration: 0.385006\n",
      "[2022-01-23 23:06:53] INFO (nni/MainThread) Intermediate result: 0.425282727608309  (Index 193)\n",
      "Early stopping counter: 14 out of 1000\n",
      "Epoch: 095 | Learning rate: 0.20000000 | Train loss: 3.208745 | Train acc: 0.459573 | Val loss: 2.803435 | Val acc: 0.417766 | Training duration: 0.369939\n",
      "[2022-01-23 23:06:54] INFO (nni/MainThread) Intermediate result: 0.4177656968354643  (Index 194)\n",
      "Early stopping counter: 15 out of 1000\n",
      "Epoch: 096 | Learning rate: 0.20000000 | Train loss: 2.994646 | Train acc: 0.472273 | Val loss: 2.722777 | Val acc: 0.439109 | Training duration: 0.369384\n",
      "[2022-01-23 23:06:54] INFO (nni/MainThread) Intermediate result: 0.4391086949226484  (Index 195)\n",
      "Early stopping counter: 16 out of 1000\n",
      "Epoch: 097 | Learning rate: 0.20000000 | Train loss: 3.176188 | Train acc: 0.479245 | Val loss: 2.756164 | Val acc: 0.449914 | Training duration: 0.364531\n",
      "[2022-01-23 23:06:54] INFO (nni/MainThread) Intermediate result: 0.4499144266586127  (Index 196)\n",
      "Early stopping counter: 17 out of 1000\n",
      "Epoch: 098 | Learning rate: 0.20000000 | Train loss: 3.156245 | Train acc: 0.483434 | Val loss: 2.743678 | Val acc: 0.449545 | Training duration: 0.369016\n",
      "[2022-01-23 23:06:55] INFO (nni/MainThread) Intermediate result: 0.4495452867545891  (Index 197)\n",
      "Early stopping counter: 18 out of 1000\n",
      "Epoch: 099 | Learning rate: 0.20000000 | Train loss: 2.942787 | Train acc: 0.479520 | Val loss: 2.698902 | Val acc: 0.433202 | Training duration: 0.366288\n",
      "[2022-01-23 23:06:55] INFO (nni/MainThread) Intermediate result: 0.43320245645827044  (Index 198)\n",
      "Early stopping counter: 19 out of 1000\n",
      "Epoch: 100 | Learning rate: 0.20000000 | Train loss: 3.001241 | Train acc: 0.466742 | Val loss: 2.747477 | Val acc: 0.407228 | Training duration: 0.399782\n",
      "[2022-01-23 23:06:56] INFO (nni/MainThread) Intermediate result: 0.40722843048424445  (Index 199)\n",
      "Early stopping counter: 20 out of 1000\n",
      "dataset: arxiv acc: tensor(0.4515, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "import scipy.sparse\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "from script import dataloader\n",
    "import numpy as np\n",
    "import torch\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "dataset_name = [\"arxiv\", \"mag\", \"products\", \"reddit\"]\n",
    "renorm_adj_type = \"sym\"\n",
    "learning_rate = 0.2\n",
    "weight_decay_rate = 0.0000005\n",
    "alpha = 0.05\n",
    "K = 16\n",
    "enable_bias = True\n",
    "epochs = 15\n",
    "opt = \"Adam\"\n",
    "early_stopping_patience = 1000\n",
    "model_name = \"ssgc\"\n",
    "data_path = './data/'\n",
    "graph_path = None\n",
    "\n",
    "#device, dataset_name, data_path, graph_path, learning_rate, weight_decay_rate, model_name, model_save_path, renorm_adj_type, alpha, K, enable_bias, epochs, opt, early_stopping_patience = get_parameters('citeseer')\n",
    "for ds in dataset_name:\n",
    "    model_save_path = './model/save/' + ds + '/ssgc_sym_' + str(alpha) + \\\n",
    "                    '_alpha_' + str(K) + '_power' + '.pth'\n",
    "    #for i in range(10): Those datasets are way too big we dont have the computation power to use 10 runs\n",
    "    features, labels, idx_train, idx_val, idx_test, n_feat, n_class, n_vertex = process_data(device, ds, data_path, graph_path, renorm_adj_type, alpha, K)\n",
    "\n",
    "    model, loss, early_stopping, optimizer, scheduler = prepare_model(n_feat, n_class, enable_bias, early_stopping_patience, learning_rate, weight_decay_rate, model_save_path, opt)\n",
    "\n",
    "    mean_train_epoch_time_duration = train(epochs, model, optimizer, scheduler, early_stopping, features, labels, loss, idx_train, idx_val, dataset_name)\n",
    "\n",
    "    average_acc = test(model, model_save_path, features, labels, loss, idx_test, model_name, dataset_name, mean_train_epoch_time_duration)\n",
    "    print(\"dataset:\", ds, \"acc:\", average_acc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83a311",
   "metadata": {},
   "source": [
    "## Large OGB ARXIV (SSGC + MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030020d4",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a3f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.utils import dropout_adj\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "from typing import Optional\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "#from logger import Logger\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "\n",
    "\n",
    "class SGConv(MessagePassing):\n",
    "    r\"\"\"The simple graph convolutional operator from the `\"Simplifying Graph\n",
    "    Convolutional Networks\" <https://arxiv.org/abs/1902.07153>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = {\\left(\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X} \\mathbf{\\Theta},\n",
    "\n",
    "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
    "    adjacency matrix with inserted self-loops and\n",
    "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        K (int, optional): Number of hops :math:`K`. (default: :obj:`1`)\n",
    "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
    "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
    "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X}` on\n",
    "            first execution, and will use the cached version for further\n",
    "            executions.\n",
    "            This parameter should only be set to :obj:`True` in transductive\n",
    "            learning scenarios. (default: :obj:`False`)\n",
    "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
    "            self-loops to the input graph. (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "\n",
    "    _cached_x: Optional[Tensor]\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, K: int = 1,\n",
    "                 cached: bool = False, add_self_loops: bool = True,\n",
    "                 bias: bool = True, dropout: float = 0.05, **kwargs):\n",
    "        super(SGConv, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self._cached_x = None\n",
    "\n",
    "        self.lin = Linear(in_channels, out_channels, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #self.lin.reset_parameters()\n",
    "        self._cached_x = None\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_weight: OptTensor = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        cache = self._cached_x\n",
    "        if cache is None:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
    "                    self.add_self_loops, dtype=x.dtype)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                edge_index = gcn_norm(  # yapf: disable\n",
    "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
    "                    self.add_self_loops, dtype=x.dtype)\n",
    "\n",
    "            x_set = []\n",
    "            alpha = 0.05\n",
    "            output = alpha * x\n",
    "            #temp_edge_index, edge_weight = dropout_adj(edge_index, 0.5)\n",
    "            for k in range(self.K):\n",
    "                x = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
    "                                   size=None)\n",
    "                # x_set.append(x)\n",
    "                output = output + (1. / self.K) * x\n",
    "            # x = torch.stack(x_set,2)\n",
    "            # alpha = 0.05\n",
    "            # x = (1-alpha)*torch.mean(x,2).squeeze() + alpha*x_ori\n",
    "            x = output\n",
    "            if self.cached:\n",
    "                self._cached_x = x\n",
    "        else:\n",
    "            x = cache\n",
    "\n",
    "        return x#self.lin(x)\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
    "        return edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, K={})'.format(self.__class__.__name__,\n",
    "                                         self.in_channels, self.out_channels,\n",
    "                                         self.K)\n",
    "\n",
    "\n",
    "class SGC(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden, out_channels, num_layers, dropout):\n",
    "        super(SGC, self).__init__()\n",
    "        self.conv1 = SGConv(\n",
    "            in_channels, hidden, K=num_layers, cached=True)\n",
    "        self.lin = torch.nn.Linear(hidden, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "        #x = self.conv1(x, edge_index)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.conv1(x, edge_index)#F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.adj_t)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-arxiv',\n",
    "                                     transform=T.ToSparseTensor())\n",
    "\n",
    "    data = dataset[0]\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    data = data.to(device)\n",
    "\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train'].to(device)\n",
    "\n",
    "    model = SGC(data.num_features, 256, dataset.num_classes, 10, 0.5).to(device)\n",
    "    \n",
    "    features = model(data.x, data.adj_t)\n",
    "    torch.save(features,'embedding.pt')\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948bd125",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96755de8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 01, Loss: 3.7619, Train: 12.49%, Valid: 23.52%, Test: 21.88%\n",
      "Run: 01, Epoch: 02, Loss: 2.7462, Train: 34.00%, Valid: 34.02%, Test: 30.28%\n",
      "Run: 01, Epoch: 03, Loss: 2.3291, Train: 31.60%, Valid: 32.18%, Test: 29.18%\n",
      "Run: 01, Epoch: 04, Loss: 2.1686, Train: 22.62%, Valid: 15.97%, Test: 13.49%\n",
      "Run: 01, Epoch: 05, Loss: 2.0158, Train: 25.81%, Valid: 16.46%, Test: 13.70%\n",
      "Run: 01, Epoch: 06, Loss: 1.8935, Train: 35.12%, Valid: 31.40%, Test: 31.58%\n",
      "Run: 01, Epoch: 07, Loss: 1.7974, Train: 34.11%, Valid: 28.79%, Test: 29.74%\n",
      "Run: 01, Epoch: 08, Loss: 1.7222, Train: 33.07%, Valid: 24.31%, Test: 24.93%\n",
      "Run: 01, Epoch: 09, Loss: 1.6544, Train: 36.09%, Valid: 28.05%, Test: 29.06%\n",
      "Run: 01, Epoch: 10, Loss: 1.6055, Train: 39.27%, Valid: 32.02%, Test: 34.14%\n",
      "Run: 01, Epoch: 11, Loss: 1.5499, Train: 41.07%, Valid: 34.07%, Test: 36.79%\n",
      "Run: 01, Epoch: 12, Loss: 1.5198, Train: 42.28%, Valid: 35.73%, Test: 38.53%\n",
      "Run: 01, Epoch: 13, Loss: 1.4785, Train: 42.45%, Valid: 34.98%, Test: 37.92%\n",
      "Run: 01, Epoch: 14, Loss: 1.4522, Train: 42.16%, Valid: 34.01%, Test: 37.08%\n",
      "Run: 01, Epoch: 15, Loss: 1.4195, Train: 43.11%, Valid: 35.32%, Test: 38.32%\n",
      "Run: 01, Epoch: 16, Loss: 1.3993, Train: 44.98%, Valid: 37.55%, Test: 40.09%\n",
      "Run: 01, Epoch: 17, Loss: 1.3760, Train: 45.89%, Valid: 38.23%, Test: 40.46%\n",
      "Run: 01, Epoch: 18, Loss: 1.3579, Train: 45.63%, Valid: 37.85%, Test: 40.10%\n",
      "Run: 01, Epoch: 19, Loss: 1.3416, Train: 46.14%, Valid: 39.39%, Test: 41.93%\n",
      "Run: 01, Epoch: 20, Loss: 1.3213, Train: 46.76%, Valid: 40.11%, Test: 42.59%\n",
      "Run: 01, Epoch: 21, Loss: 1.3102, Train: 47.28%, Valid: 39.60%, Test: 42.01%\n",
      "Run: 01, Epoch: 22, Loss: 1.2952, Train: 48.30%, Valid: 40.59%, Test: 42.73%\n",
      "Run: 01, Epoch: 23, Loss: 1.2842, Train: 49.37%, Valid: 42.29%, Test: 44.31%\n",
      "Run: 01, Epoch: 24, Loss: 1.2652, Train: 50.63%, Valid: 43.81%, Test: 45.62%\n",
      "Run: 01, Epoch: 25, Loss: 1.2576, Train: 52.16%, Valid: 45.20%, Test: 46.60%\n",
      "Run: 01, Epoch: 26, Loss: 1.2431, Train: 54.32%, Valid: 48.01%, Test: 49.15%\n",
      "Run: 01, Epoch: 27, Loss: 1.2341, Train: 56.11%, Valid: 51.00%, Test: 52.14%\n",
      "Run: 01, Epoch: 28, Loss: 1.2262, Train: 56.55%, Valid: 51.44%, Test: 52.86%\n",
      "Run: 01, Epoch: 29, Loss: 1.2162, Train: 57.43%, Valid: 52.88%, Test: 54.49%\n",
      "Run: 01, Epoch: 30, Loss: 1.2112, Train: 59.28%, Valid: 56.09%, Test: 57.73%\n",
      "Run: 01, Epoch: 31, Loss: 1.1980, Train: 60.71%, Valid: 58.32%, Test: 59.61%\n",
      "Run: 01, Epoch: 32, Loss: 1.1927, Train: 61.40%, Valid: 59.28%, Test: 60.30%\n",
      "Run: 01, Epoch: 33, Loss: 1.1832, Train: 62.37%, Valid: 60.61%, Test: 61.48%\n",
      "Run: 01, Epoch: 34, Loss: 1.1731, Train: 63.74%, Valid: 62.62%, Test: 63.67%\n",
      "Run: 01, Epoch: 35, Loss: 1.1683, Train: 64.00%, Valid: 62.98%, Test: 63.96%\n",
      "Run: 01, Epoch: 36, Loss: 1.1618, Train: 64.18%, Valid: 62.83%, Test: 63.76%\n",
      "Run: 01, Epoch: 37, Loss: 1.1554, Train: 65.05%, Valid: 63.97%, Test: 64.68%\n",
      "Run: 01, Epoch: 38, Loss: 1.1483, Train: 66.22%, Valid: 65.58%, Test: 66.09%\n",
      "Run: 01, Epoch: 39, Loss: 1.1426, Train: 66.89%, Valid: 66.43%, Test: 66.72%\n",
      "Run: 01, Epoch: 40, Loss: 1.1369, Train: 67.31%, Valid: 66.94%, Test: 67.16%\n",
      "Run: 01, Epoch: 41, Loss: 1.1316, Train: 67.69%, Valid: 67.46%, Test: 67.65%\n",
      "Run: 01, Epoch: 42, Loss: 1.1251, Train: 68.13%, Valid: 67.73%, Test: 67.86%\n",
      "Run: 01, Epoch: 43, Loss: 1.1211, Train: 68.50%, Valid: 68.29%, Test: 68.08%\n",
      "Run: 01, Epoch: 44, Loss: 1.1156, Train: 68.61%, Valid: 68.51%, Test: 68.25%\n",
      "Run: 01, Epoch: 45, Loss: 1.1092, Train: 68.86%, Valid: 68.63%, Test: 68.61%\n",
      "Run: 01, Epoch: 46, Loss: 1.1079, Train: 69.14%, Valid: 68.83%, Test: 68.59%\n",
      "Run: 01, Epoch: 47, Loss: 1.1041, Train: 69.39%, Valid: 68.94%, Test: 68.66%\n",
      "Run: 01, Epoch: 48, Loss: 1.1001, Train: 69.68%, Valid: 69.45%, Test: 69.13%\n",
      "Run: 01, Epoch: 49, Loss: 1.0945, Train: 69.92%, Valid: 69.53%, Test: 69.30%\n",
      "Run: 01, Epoch: 50, Loss: 1.0898, Train: 70.14%, Valid: 69.60%, Test: 69.03%\n",
      "Run: 01, Epoch: 51, Loss: 1.0871, Train: 70.01%, Valid: 69.50%, Test: 68.89%\n",
      "Run: 01, Epoch: 52, Loss: 1.0853, Train: 70.10%, Valid: 69.80%, Test: 69.46%\n",
      "Run: 01, Epoch: 53, Loss: 1.0814, Train: 70.39%, Valid: 70.27%, Test: 69.85%\n",
      "Run: 01, Epoch: 54, Loss: 1.0799, Train: 70.66%, Valid: 70.31%, Test: 69.76%\n",
      "Run: 01, Epoch: 55, Loss: 1.0740, Train: 70.58%, Valid: 70.03%, Test: 69.71%\n",
      "Run: 01, Epoch: 56, Loss: 1.0721, Train: 70.63%, Valid: 70.14%, Test: 69.85%\n",
      "Run: 01, Epoch: 57, Loss: 1.0687, Train: 70.95%, Valid: 70.41%, Test: 69.86%\n",
      "Run: 01, Epoch: 58, Loss: 1.0643, Train: 71.04%, Valid: 70.37%, Test: 69.65%\n",
      "Run: 01, Epoch: 59, Loss: 1.0626, Train: 71.05%, Valid: 70.53%, Test: 70.07%\n",
      "Run: 01, Epoch: 60, Loss: 1.0586, Train: 71.12%, Valid: 70.58%, Test: 70.01%\n",
      "Run: 01, Epoch: 61, Loss: 1.0590, Train: 71.25%, Valid: 70.51%, Test: 69.53%\n",
      "Run: 01, Epoch: 62, Loss: 1.0532, Train: 71.43%, Valid: 70.68%, Test: 69.96%\n",
      "Run: 01, Epoch: 63, Loss: 1.0539, Train: 71.38%, Valid: 70.94%, Test: 70.33%\n",
      "Run: 01, Epoch: 64, Loss: 1.0495, Train: 71.45%, Valid: 70.97%, Test: 70.31%\n",
      "Run: 01, Epoch: 65, Loss: 1.0484, Train: 71.74%, Valid: 70.85%, Test: 70.05%\n",
      "Run: 01, Epoch: 66, Loss: 1.0442, Train: 71.50%, Valid: 70.85%, Test: 70.24%\n",
      "Run: 01, Epoch: 67, Loss: 1.0426, Train: 71.72%, Valid: 70.97%, Test: 70.15%\n",
      "Run: 01, Epoch: 68, Loss: 1.0447, Train: 71.78%, Valid: 71.00%, Test: 70.23%\n",
      "Run: 01, Epoch: 69, Loss: 1.0360, Train: 71.90%, Valid: 71.13%, Test: 70.31%\n",
      "Run: 01, Epoch: 70, Loss: 1.0344, Train: 71.92%, Valid: 71.09%, Test: 70.26%\n",
      "Run: 01, Epoch: 71, Loss: 1.0314, Train: 71.98%, Valid: 71.02%, Test: 70.08%\n",
      "Run: 01, Epoch: 72, Loss: 1.0338, Train: 72.02%, Valid: 71.00%, Test: 70.22%\n",
      "Run: 01, Epoch: 73, Loss: 1.0280, Train: 72.01%, Valid: 70.99%, Test: 69.99%\n",
      "Run: 01, Epoch: 74, Loss: 1.0306, Train: 71.93%, Valid: 71.09%, Test: 69.88%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-44b5d89fdc56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m#logger.print_statistics()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-44b5d89fdc56>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m#logger.add_result(run, result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-44b5d89fdc56>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, x, y_true, train_idx, optimizer)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-44b5d89fdc56>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2280\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2282\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2283\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2284\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "#from logger import Logger\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "def train(model, x, y_true, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x[train_idx])\n",
    "    loss = F.nll_loss(out, y_true.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, x, y_true, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(x)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-arxiv')\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    data = dataset[0]\n",
    "\n",
    "    x = data.x\n",
    "    embedding = torch.load('embedding.pt', map_location='cpu')\n",
    "    #x = torch.cat([x, embedding], dim=-1)\n",
    "    x = embedding\n",
    "    x = x.to(device)\n",
    "\n",
    "    y_true = data.y.to(device)\n",
    "    train_idx = split_idx['train'].to(device)\n",
    "\n",
    "    model = MLP(x.size(-1), 256, dataset.num_classes,\n",
    "                3, 0.5).to(device)\n",
    "\n",
    "    evaluator = Evaluator(name='ogbn-arxiv')\n",
    "    #logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(10):\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        for epoch in range(1, 1 + 500):\n",
    "            loss = train(model, x, y_true, train_idx, optimizer)\n",
    "            result = test(model, x, y_true, split_idx, evaluator)\n",
    "            #logger.add_result(run, result)\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}%, '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        #logger.print_statistics(run)\n",
    "    #logger.print_statistics()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194e7dd0",
   "metadata": {},
   "source": [
    "# Large OGB MAG (SSGC + MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a0cce",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a531334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "from typing import Optional\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "#from logger import Logger\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "\n",
    "\n",
    "class SGConv(MessagePassing):\n",
    "    r\"\"\"The simple graph convolutional operator from the `\"Simplifying Graph\n",
    "    Convolutional Networks\" <https://arxiv.org/abs/1902.07153>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = {\\left(\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X} \\mathbf{\\Theta},\n",
    "\n",
    "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
    "    adjacency matrix with inserted self-loops and\n",
    "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        K (int, optional): Number of hops :math:`K`. (default: :obj:`1`)\n",
    "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
    "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
    "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X}` on\n",
    "            first execution, and will use the cached version for further\n",
    "            executions.\n",
    "            This parameter should only be set to :obj:`True` in transductive\n",
    "            learning scenarios. (default: :obj:`False`)\n",
    "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
    "            self-loops to the input graph. (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "\n",
    "    _cached_x: Optional[Tensor]\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, K: int = 1,\n",
    "                 cached: bool = False, add_self_loops: bool = True,\n",
    "                 bias: bool = True, dropout: float = 0.05, **kwargs):\n",
    "        super(SGConv, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self._cached_x = None\n",
    "\n",
    "        #self.lin = Linear(in_channels, out_channels, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #self.lin.reset_parameters()\n",
    "        self._cached_x = None\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_weight: OptTensor = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        cache = self._cached_x\n",
    "        if cache is None:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
    "                    self.add_self_loops, dtype=x.dtype)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                edge_index = gcn_norm(  # yapf: disable\n",
    "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
    "                    self.add_self_loops, dtype=x.dtype)\n",
    "\n",
    "            x = F.normalize(x,dim=1, p=2)\n",
    "            alpha = 0.05\n",
    "            output = alpha*x\n",
    "            for k in range(self.K):\n",
    "                x = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
    "                                   size=None)\n",
    "                #x_set.append(x)\n",
    "                output = output + (1./self.K)*x\n",
    "            #x = torch.stack(x_set,2)\n",
    "            #alpha = 0.05\n",
    "            #x = (1-alpha)*torch.mean(x,2).squeeze() + alpha*x_ori\n",
    "            x = output\n",
    "            if self.cached:\n",
    "                self._cached_x = x\n",
    "        else:\n",
    "            x = cache\n",
    "\n",
    "        return x#self.lin(x)\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
    "        return edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, K={})'.format(self.__class__.__name__,\n",
    "                                         self.in_channels, self.out_channels,\n",
    "                                         self.K)\n",
    "\n",
    "\n",
    "class SGC(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers):\n",
    "        super(SGC, self).__init__()\n",
    "        self.conv1 = SGConv(\n",
    "            in_channels, out_channels, K=num_layers, cached=True)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "        #x = self.conv1(x, edge_index)\n",
    "        return self.conv1(x, edge_index)#F.log_softmax(x, dim=1)\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            GCNConv(in_channels, hidden_channels, normalize=False))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, normalize=False))\n",
    "        self.convs.append(\n",
    "            GCNConv(hidden_channels, out_channels, normalize=False))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.adj_t)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']['paper']],\n",
    "        'y_pred': y_pred[split_idx['train']['paper']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']['paper']],\n",
    "        'y_pred': y_pred[split_idx['valid']['paper']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']['paper']],\n",
    "        'y_pred': y_pred[split_idx['test']['paper']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-mag')\n",
    "    rel_data = dataset[0]\n",
    "\n",
    "    # We are only interested in paper <-> paper relations.\n",
    "    data = Data(\n",
    "        x=rel_data.x_dict['paper'],\n",
    "        edge_index=rel_data.edge_index_dict[('paper', 'cites', 'paper')],\n",
    "        y=rel_data.y_dict['paper'])\n",
    "\n",
    "    data = T.ToSparseTensor()(data)\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train']['paper'].to(device)\n",
    "\n",
    "    model = SGC(data.num_features,\n",
    "                dataset.num_classes, 16,\n",
    "                ).to(device)\n",
    "\n",
    "    # Pre-compute GCN normalization.\n",
    "    adj_t = data.adj_t.set_diag()\n",
    "    deg = adj_t.sum(dim=1).to(torch.float)\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
    "    data.adj_t = adj_t\n",
    "\n",
    "    data = data.to(device)\n",
    "    features = model(data.x, data.adj_t)\n",
    "    torch.save(features,'embedding.pt')\n",
    "    # evaluator = Evaluator(name='ogbn-mag')\n",
    "    # logger = Logger(args.runs, args)\n",
    "    #\n",
    "    # for run in range(args.runs):\n",
    "    #     model.reset_parameters()\n",
    "    #     optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    #     for epoch in range(1, 1 + args.epochs):\n",
    "    #         loss = train(model, data, train_idx, optimizer)\n",
    "    #         result = test(model, data, split_idx, evaluator)\n",
    "    #         logger.add_result(run, result)\n",
    "    #\n",
    "    #         if epoch % args.log_steps == 0:\n",
    "    #             train_acc, valid_acc, test_acc = result\n",
    "    #             print(f'Run: {run + 1:02d}, '\n",
    "    #                   f'Epoch: {epoch:02d}, '\n",
    "    #                   f'Loss: {loss:.4f}, '\n",
    "    #                   f'Train: {100 * train_acc:.2f}%, '\n",
    "    #                   f'Valid: {100 * valid_acc:.2f}% '\n",
    "    #                   f'Test: {100 * test_acc:.2f}%')\n",
    "    #\n",
    "    #     logger.print_statistics(run)\n",
    "    #logger.print_statistics()\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477508c",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "#from logger import Logger\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "def train(model, x, y_true, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x[train_idx])\n",
    "    loss = F.nll_loss(out, y_true[train_idx].squeeze(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, x, y_true, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(x)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['train']['paper']],\n",
    "        'y_pred': y_pred[split_idx['train']['paper']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['valid']['paper']],\n",
    "        'y_pred': y_pred[split_idx['valid']['paper']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['test']['paper']],\n",
    "        'y_pred': y_pred[split_idx['test']['paper']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-mag')\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    data = dataset[0]\n",
    "\n",
    "    x = data.x_dict['paper']\n",
    "    embedding = torch.load('embedding.pt', map_location='cpu')\n",
    "    x = torch.cat([x, embedding], dim=-1)\n",
    "    x = x.to(device)\n",
    "\n",
    "    y_true = data.y_dict['paper'].to(device)\n",
    "    train_idx = split_idx['train']['paper'].to(device)\n",
    "\n",
    "    model = MLP(x.size(-1), 256, dataset.num_classes,\n",
    "                3, 0.0).to(device)\n",
    "\n",
    "    evaluator = Evaluator(name='ogbn-mag')\n",
    "    #logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(10):\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        for epoch in range(1, 1 + 500):\n",
    "            loss = train(model, x, y_true, train_idx, optimizer)\n",
    "            result = test(model, x, y_true, split_idx, evaluator)\n",
    "            #logger.add_result(run, result)\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}%, '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        #logger.print_statistics(run)\n",
    "    #logger.print_statistics()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcdb36d",
   "metadata": {},
   "source": [
    "# Node Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c952f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: cora acc: 0.6667774495322502  nmi: 0.5156671595176218 f1-score:  0.6304953643534703\n",
      "dataset: citeseer acc: 0.687702722128566  nmi: 0.42589838263216256 f1-score:  0.6442045975917995\n",
      "dataset: pubmed acc: 0.6967440097045325  nmi: 0.31792580915945245 f1-score:  0.6907569086430445\n",
      "dataset: wiki acc: 0.5207093065916596  nmi: 0.49004917952274724 f1-score:  0.4443000509372268\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.cluster import KMeans\n",
    "from NodeClustering.metrics import clustering_metrics\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "def normalize_adj(adj, type='sym'):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    if type == 'sym':\n",
    "        adj = sp.coo_matrix(adj)\n",
    "        rowsum = np.array(adj.sum(1))\n",
    "        # d_inv_sqrt = np.power(rowsum, -0.5)\n",
    "        # d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "        # return adj*d_inv_sqrt*d_inv_sqrt.flatten()\n",
    "        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "        return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
    "    elif type == 'rw':\n",
    "        rowsum = np.array(adj.sum(1))\n",
    "        d_inv = np.power(rowsum, -1.0).flatten()\n",
    "        d_inv[np.isinf(d_inv)] = 0.\n",
    "        d_mat_inv = sp.diags(d_inv)\n",
    "        adj_normalized = d_mat_inv.dot(adj)\n",
    "        return adj_normalized\n",
    "\n",
    "\n",
    "def preprocess_adj(adj, type='sym', loop=True):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    if loop:\n",
    "        adj = adj + sp.eye(adj.shape[0])\n",
    "    adj_normalized = normalize_adj(adj, type=type)\n",
    "    return adj_normalized\n",
    "\n",
    "\n",
    "def to_onehot(prelabel):\n",
    "    k = len(np.unique(prelabel))\n",
    "    label = np.zeros([prelabel.shape[0], k])\n",
    "    label[range(prelabel.shape[0]), prelabel] = 1\n",
    "    label = label.T\n",
    "    return label\n",
    "\n",
    "\n",
    "def square_dist(prelabel, feature):\n",
    "    if sp.issparse(feature):\n",
    "        feature = feature.todense()\n",
    "    feature = np.array(feature)\n",
    "\n",
    "\n",
    "    onehot = to_onehot(prelabel)\n",
    "\n",
    "    m, n = onehot.shape\n",
    "    count = onehot.sum(1).reshape(m, 1)\n",
    "    count[count==0] = 1\n",
    "\n",
    "    mean = onehot.dot(feature)/count\n",
    "    a2 = (onehot.dot(feature*feature)/count).sum(1)\n",
    "    pdist2 = np.array(a2 + a2.T - 2*mean.dot(mean.T))\n",
    "\n",
    "    intra_dist = pdist2.trace()\n",
    "    inter_dist = pdist2.sum() - intra_dist\n",
    "    intra_dist /= m\n",
    "    inter_dist /= m * (m - 1)\n",
    "    return intra_dist\n",
    "\n",
    "def dist(prelabel, feature):\n",
    "    k = len(np.unique(prelabel))\n",
    "    intra_dist = 0\n",
    "\n",
    "    for i in range(k):\n",
    "        Data_i = feature[np.where(prelabel == i)]\n",
    "\n",
    "        Dis = euclidean_distances(Data_i, Data_i)\n",
    "        n_i = Data_i.shape[0]\n",
    "        if n_i == 0 or n_i == 1:\n",
    "            intra_dist = intra_dist\n",
    "        else:\n",
    "            intra_dist = intra_dist + 1 / k * 1 / (n_i * (n_i - 1)) * sum(sum(Dis))\n",
    "\n",
    "\n",
    "    return intra_dist\n",
    "\n",
    "datasets = ['cora', 'citeseer', 'pubmed', 'wiki']\n",
    "for dataset in datasets:\n",
    "    \n",
    "    #for i in range(10): dont have the computational capacities\n",
    "    data = sio.loadmat('./NodeClustering/{}.mat'.format(dataset))\n",
    "    feature = data['fea']\n",
    "    if sp.issparse(feature):\n",
    "        feature = feature.todense()\n",
    "\n",
    "    adj = data['W']\n",
    "    gnd = data['gnd']\n",
    "    gnd = gnd.T\n",
    "    gnd = gnd - 1\n",
    "    gnd = gnd[0, :]\n",
    "    k = len(np.unique(gnd))\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    intra_list = []\n",
    "    intra_list.append(10000)\n",
    "\n",
    "\n",
    "    acc_list = []\n",
    "    nmi_list = []\n",
    "    f1_list = []\n",
    "    stdacc_list = []\n",
    "    stdnmi_list = []\n",
    "    stdf1_list = []\n",
    "    max_iter = 60\n",
    "    rep = 10\n",
    "    t = time.time()\n",
    "    #adj_normalized = preprocess_adj(adj)\n",
    "    #adj_normalized = (sp.eye(adj_normalized.shape[0]) + adj_normalized) / 2\n",
    "    adj_normalized = preprocess_adj(adj,loop=False)\n",
    "    # adj_normalized = (sp.eye(adj_normalized.shape[0]) + adj_normalized + adj_normalized.dot(adj_normalized)) / 3\n",
    "    total_dist = []\n",
    "\n",
    "    tt = 0\n",
    "    alpha = 0.05 #0.05 for pubmed, 0.1 for citeseer\n",
    "    feature_ori = feature.astype('float64')\n",
    "    #emb = feature.astype('float64')\n",
    "    #emb = feature.astype('float64')\n",
    "    emb = np.zeros_like(feature).astype('float64')\n",
    "    oneV = np.ones(feature.shape[0])\n",
    "    den = np.zeros_like(oneV)\n",
    "    while 1:\n",
    "        tt = tt + 1\n",
    "        power = tt\n",
    "        intraD = np.zeros(rep)\n",
    "\n",
    "\n",
    "        ac = np.zeros(rep)\n",
    "        nm = np.zeros(rep)\n",
    "        f1 = np.zeros(rep)\n",
    "\n",
    "\n",
    "\n",
    "        feature = adj_normalized.dot(feature)\n",
    "\n",
    "        emb += feature\n",
    "        emb_norm = emb/tt\n",
    "        u, s, v = sp.linalg.svds(emb_norm, k=16, which='LM')\n",
    "        u = normalize(emb_norm.dot(v.T))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(rep):\n",
    "            kmeans = KMeans(n_clusters=k).fit(u)\n",
    "            predict_labels = kmeans.predict(u)\n",
    "            intraD[i] = square_dist(predict_labels, u)\n",
    "            #intraD[i] = dist(predict_labels, feature)\n",
    "            cm = clustering_metrics(gnd, predict_labels)\n",
    "            ac[i], nm[i], f1[i] = cm.evaluationClusterModelFromLabel()\n",
    "\n",
    "        intramean = np.mean(intraD)\n",
    "        acc_means = np.mean(ac)\n",
    "        acc_stds = np.std(ac)\n",
    "        nmi_means = np.mean(nm)\n",
    "        nmi_stds = np.std(nm)\n",
    "        f1_means = np.mean(f1)\n",
    "        f1_stds = np.std(f1)\n",
    "\n",
    "        intra_list.append(intramean)\n",
    "        acc_list.append(acc_means)\n",
    "        stdacc_list.append(acc_stds)\n",
    "        nmi_list.append(nmi_means)\n",
    "        stdnmi_list.append(nmi_stds)\n",
    "        f1_list.append(f1_means)\n",
    "        stdf1_list.append(f1_stds)\n",
    "        #print('power: {}'.format(power),\n",
    "        #      'intra_dist: {}'.format(intramean),\n",
    "        #      'acc_mean: {}'.format(acc_means),\n",
    "        #      'acc_std: {}'.format(acc_stds),\n",
    "        #      'nmi_mean: {}'.format(nmi_means),\n",
    "        #      'nmi_std: {}'.format(nmi_stds),\n",
    "        #      'f1_mean: {}'.format(f1_means),\n",
    "        #      'f1_std: {}'.format(f1_stds))\n",
    "\n",
    "        if intra_list[tt] > intra_list[tt - 1] or tt > max_iter:\n",
    "            #print('bestpower: {}'.format(tt - 1))\n",
    "            t = time.time() - t\n",
    "            #print(t)\n",
    "            break\n",
    "    print(\"dataset:\", dataset, \"acc:\", str(sum(acc_list)/len(acc_list)), \" nmi:\", str(sum(nmi_list)/len(nmi_list)), \"f1-score: \", str(sum(f1_list)/len(f1_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44893fd",
   "metadata": {},
   "source": [
    "# Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad526ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 21.1 GiB for an array with shape (53210, 53210) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-640cfc4605e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m#if not args.preprocessed:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0madj_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_to_torch_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mfeat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgc_precompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_dense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myprojectdir/SSGC_new/SSGC/DocumentClassification/utils.py\u001b[0m in \u001b[0;36msparse_to_torch_dense\u001b[0;34m(sparse, device)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msparse_to_torch_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mtorch_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mtodense\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \"\"\"\n\u001b[0;32m--> 864\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 21.1 GiB for an array with shape (53210, 53210) and data type float64"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tabulate\n",
    "from functools import partial\n",
    "from DocumentClassification.utils import *\n",
    "from DocumentClassification.models import SGC\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#if args.tuned:\n",
    "#    with open(\"tuned_result/{}.SGC.tuning.txt\".format(args.dataset), \"r\") as f:\n",
    "#        args.weight_decay = float(f.read())\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "#set_seed(args.seed, args.cuda)\n",
    "\n",
    "def train_linear(model, feat_dict, weight_decay, binary=False):\n",
    "    if not binary:\n",
    "        act = partial(F.log_softmax, dim=1)\n",
    "        criterion = F.nll_loss\n",
    "    else:\n",
    "        act = torch.sigmoid\n",
    "        criterion = F.binary_cross_entropy\n",
    "    optimizer = optim.LBFGS(model.parameters())\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0\n",
    "    plateau = 0\n",
    "    start = time.perf_counter()\n",
    "    for epoch in range(3):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            #output = model(feat_dict[\"train\"].cuda()).squeeze()\n",
    "            output = model(feat_dict[\"train\"]).squeeze()\n",
    "            l2_reg = 0.5*weight_decay*(model.W.weight**2).sum()\n",
    "            #loss = criterion(act(output), label_dict[\"train\"].cuda())+l2_reg\n",
    "            loss = criterion(act(output), label_dict[\"train\"])+l2_reg\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    train_time = time.perf_counter()-start\n",
    "    val_res = eval_linear(model, feat_dict[\"val\"],\n",
    "                          label_dict[\"val\"].cuda(), binary)\n",
    "    # val_res = eval_linear(model, feat_dict[\"val\"],\n",
    "    #                       label_dict[\"val\"].cuda(), binary)\n",
    "    return val_res['accuracy'], model, train_time\n",
    "\n",
    "def eval_linear(model, features, label, binary=False):\n",
    "    model.eval()\n",
    "    if not binary:\n",
    "        act = partial(F.log_softmax, dim=1)\n",
    "        criterion = F.nll_loss\n",
    "    else:\n",
    "        act = torch.sigmoid\n",
    "        criterion = F.binary_cross_entropy\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(features).squeeze()\n",
    "        loss = criterion(act(output), label)\n",
    "        if not binary: predict_class = output.max(1)[1]\n",
    "        else: predict_class = act(output).gt(0.5).float()\n",
    "        correct = torch.eq(predict_class, label).long().sum().item()\n",
    "        acc = correct/predict_class.size(0)\n",
    "\n",
    "    return {\n",
    "        'loss': loss.item(),\n",
    "        'accuracy': acc\n",
    "    }\n",
    "\n",
    "datasets = ['20ng', 'r8', 'r52', 'ohsumed', 'mr']\n",
    "for dataset in datasets:\n",
    "    sp_adj, index_dict, label_dict = load_corpus('20ng')\n",
    "    for k, v in label_dict.items():\n",
    "        if dataset == \"mr\":\n",
    "            label_dict[k] = torch.Tensor(v).to(device)\n",
    "        else:\n",
    "            label_dict[k] = torch.LongTensor(v).to(device)\n",
    "    features = torch.arange(sp_adj.shape[0]).to(device)\n",
    "\n",
    "    adj = sparse_to_torch_sparse(sp_adj, device=\"cpu\")\n",
    "\n",
    "    if dataset == \"mr\": nclass = 1\n",
    "    else: nclass = label_dict[\"train\"].max().item()+1\n",
    "    #if not args.preprocessed:\n",
    "    adj_dense = sparse_to_torch_dense(sp_adj, device='cpu')\n",
    "    feat_dict, precompute_time = sgc_precompute(adj, adj_dense, 5-1, index_dict)\n",
    "    #else:\n",
    "    #    # load the relased degree 2 features\n",
    "    #    with open(os.path.join(\"preprocessed\",\n",
    "    #        \"{}.pkl\".format(args.dataset)), \"rb\") as prep:\n",
    "    #        feat_dict =  pkl.load(prep)\n",
    "    #    precompute_time = 0\n",
    "\n",
    "    model = SGC(nfeat=feat_dict[\"train\"].size(1),\n",
    "                nclass=nclass)\n",
    "    #if args.cuda:\n",
    "    model.cuda()\n",
    "    val_acc, best_model, train_time = train_linear(model, feat_dict, 0, args.dataset==\"mr\")\n",
    "    test_res = eval_linear(best_model, feat_dict[\"test\"].cuda(),\n",
    "                           label_dict[\"test\"].cuda(), dataset==\"mr\")\n",
    "    train_res = eval_linear(best_model, feat_dict[\"train\"].cuda(),\n",
    "                            label_dict[\"train\"].cuda(), dataset==\"mr\")\n",
    "    print(\"Total Time: {:2f}s, Train acc: {:.4f}, Val acc: {:.4f}, Test acc: {:.4f}\".format(precompute_time+train_time, train_res[\"accuracy\"], val_acc, test_res[\"accuracy\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
