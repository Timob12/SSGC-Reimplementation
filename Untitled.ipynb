{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcea5174",
   "metadata": {},
   "source": [
    "# Node Classification\n",
    "Sadly, we do not have the computational power to execute the code for the OGB-mag and OGB-products dataset. The memory that is needed exceeds the 12GB given by google colab. However, the code is fully functional and could be used to check the results if the resources are there. The results we get when applying SSGC on OGB-arxiv do not lead to the results presented in the paper. This could be because there are no hyperparameters given in the paper that should be used on these large datasets. But using the ones given for the small datasets does not lead to the desired result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e1b01",
   "metadata": {},
   "source": [
    "## Small/Medium Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f280ef6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset name: cora\n",
      "Validation Accuracy: 0.8000 Test Accuracy: 0.8110\n",
      "Pre-compute time: 0.4562s, train time: 1.9708s, total: 2.4271s\n",
      "dataset name: citeseer\n",
      "Validation Accuracy: 0.7080 Test Accuracy: 0.6989\n",
      "Pre-compute time: 1.4688s, train time: 4.4152s, total: 5.8840s\n",
      "dataset name: pubmed\n",
      "Validation Accuracy: 0.8020 Test Accuracy: 0.7960\n",
      "Pre-compute time: 1.5775s, train time: 0.9313s, total: 2.5088s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import load_citation, sgc_precompute, set_seed\n",
    "from models import get_model\n",
    "from metrics import accuracy\n",
    "import pickle as pkl\n",
    "#from args import get_citation_args\n",
    "from time import perf_counter\n",
    "\n",
    "# Arguments\n",
    "\n",
    "# setting random seeds\n",
    "#set_seed(seed, cuda)\n",
    "\n",
    "#tuned = False\n",
    "#arg_model = \"SGC\"\n",
    "#dataset = \"cora\"\n",
    "weight_decay = 5e-6\n",
    "#seed = 42\n",
    "#cuda = False\n",
    "normalization = 'NormAdj'\n",
    "hidden = 0\n",
    "dropout = 0\n",
    "degree = 16\n",
    "alpha = 0.05\n",
    "epochs = 1000\n",
    "lr = 0.02\n",
    "\n",
    "def setup(dataset, normalization, degree, alpha):\n",
    "    adj, features, labels, idx_train, idx_val, idx_test = load_citation(dataset, normalization, False)\n",
    "\n",
    "    model = get_model(\"SGC\", features.size(1), labels.max().item()+1, 0, 0, False)\n",
    "\n",
    "    features, precompute_time = sgc_precompute(features, adj, degree, alpha)\n",
    "    \n",
    "    return precompute_time, features, labels, idx_train, idx_val, idx_test, model\n",
    "\n",
    "def train_regression(model,\n",
    "                     train_features, train_labels,\n",
    "                     val_features, val_labels,\n",
    "                     epochs=epochs, weight_decay=weight_decay,\n",
    "                     lr=lr, dropout=dropout):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "    t = perf_counter()\n",
    "    best_acc_val = torch.zeros((1))\n",
    "    best_model = None\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_features)\n",
    "        loss_train = F.cross_entropy(output, train_labels)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output = model(val_features)\n",
    "            acc_val = accuracy(output, val_labels)\n",
    "            if best_acc_val < acc_val:\n",
    "                best_acc_val = acc_val\n",
    "                best_model = model\n",
    "\n",
    "    train_time = perf_counter()-t\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     model.eval()\n",
    "    #     output = model(val_features)\n",
    "    #     acc_val = accuracy(output, val_labels)\n",
    "\n",
    "    return best_model, best_acc_val, train_time\n",
    "\n",
    "def test_regression(model, test_features, test_labels):\n",
    "    model.eval()\n",
    "    return accuracy(model(test_features), test_labels)\n",
    "\n",
    "\n",
    "datasets = [\"cora\", \"citeseer\", \"pubmed\"]\n",
    "for ds in datasets:\n",
    "    acc_test = 0\n",
    "    total_precompute_time = 0\n",
    "    total_train_time = 0\n",
    "    for _ in range(10):\n",
    "        precompute_time, features, labels, idx_train, idx_val, idx_test, model = setup(ds, normalization, degree, alpha)\n",
    "\n",
    "        model, acc_val, train_time = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
    "                    epochs, weight_decay, lr, dropout)\n",
    "        acc_test += test_regression(model, features[idx_test], labels[idx_test])\n",
    "        total_precompute_time += precompute_time\n",
    "        total_train_time += train_time\n",
    "    acc_test /= 10\n",
    "    total_precompute_time /= 10\n",
    "    total_train_time /= 10\n",
    "    print(\"dataset name:\", ds)\n",
    "    print(\"Validation Accuracy: {:.4f} Test Accuracy: {:.4f}\".format(acc_val, acc_test))\n",
    "    print(\"Pre-compute time: {:.4f}s, train time: {:.4f}s, total: {:.4f}s\".format(total_precompute_time, total_train_time, total_precompute_time+total_train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217cb5d4",
   "metadata": {},
   "source": [
    "## Alpha and K Comparison on Cora, Citeseer, and Pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29052d02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: cora , average accuracy: tensor(0.8030, dtype=torch.float64) alpha: 0.0\n",
      "dataset: cora , average accuracy: tensor(0.8110, dtype=torch.float64) alpha: 0.05\n",
      "dataset: cora , average accuracy: tensor(0.8130, dtype=torch.float64) alpha: 0.1\n",
      "dataset: cora , average accuracy: tensor(0.8180, dtype=torch.float64) alpha: 0.15\n",
      "dataset: citeseer , average accuracy: tensor(0.6990, dtype=torch.float64) alpha: 0.0\n",
      "dataset: citeseer , average accuracy: tensor(0.6990, dtype=torch.float64) alpha: 0.05\n",
      "dataset: citeseer , average accuracy: tensor(0.6960, dtype=torch.float64) alpha: 0.1\n",
      "dataset: citeseer , average accuracy: tensor(0.6960, dtype=torch.float64) alpha: 0.15\n",
      "dataset: pubmed , average accuracy: tensor(0.7900, dtype=torch.float64) alpha: 0.0\n",
      "dataset: pubmed , average accuracy: tensor(0.7960, dtype=torch.float64) alpha: 0.05\n",
      "dataset: pubmed , average accuracy: tensor(0.7940, dtype=torch.float64) alpha: 0.1\n",
      "dataset: pubmed , average accuracy: tensor(0.7950, dtype=torch.float64) alpha: 0.15\n",
      "dataset: cora , average accuracy: tensor(0.7800, dtype=torch.float64) K: 2\n",
      "dataset: cora , average accuracy: tensor(0.8050, dtype=torch.float64) K: 4\n",
      "dataset: cora , average accuracy: tensor(0.8080, dtype=torch.float64) K: 8\n",
      "dataset: cora , average accuracy: tensor(0.8110, dtype=torch.float64) K: 16\n",
      "dataset: cora , average accuracy: tensor(0.8130, dtype=torch.float64) K: 32\n",
      "dataset: cora , average accuracy: tensor(0.7980, dtype=torch.float64) K: 64\n",
      "dataset: citeseer , average accuracy: tensor(0.6760, dtype=torch.float64) K: 2\n",
      "dataset: citeseer , average accuracy: tensor(0.6810, dtype=torch.float64) K: 4\n",
      "dataset: citeseer , average accuracy: tensor(0.6910, dtype=torch.float64) K: 8\n",
      "dataset: citeseer , average accuracy: tensor(0.6990, dtype=torch.float64) K: 16\n",
      "dataset: citeseer , average accuracy: tensor(0.7000, dtype=torch.float64) K: 32\n",
      "dataset: citeseer , average accuracy: tensor(0.6930, dtype=torch.float64) K: 64\n",
      "dataset: pubmed , average accuracy: tensor(0.7720, dtype=torch.float64) K: 2\n",
      "dataset: pubmed , average accuracy: tensor(0.7820, dtype=torch.float64) K: 4\n",
      "dataset: pubmed , average accuracy: tensor(0.7900, dtype=torch.float64) K: 8\n",
      "dataset: pubmed , average accuracy: tensor(0.7960, dtype=torch.float64) K: 16\n",
      "dataset: pubmed , average accuracy: tensor(0.7840, dtype=torch.float64) K: 32\n",
      "dataset: pubmed , average accuracy: tensor(0.7640, dtype=torch.float64) K: 64\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"cora\", \"citeseer\", \"pubmed\"]\n",
    "alpha_ = [0.0, 0.05, 0.1, 0.15]\n",
    "K_ = [2, 4, 8, 16, 32, 64]\n",
    "alpha_standard = 0.05\n",
    "K_standard = 16\n",
    "for dataset in datasets:\n",
    "    K = K_standard\n",
    "    for alpha in alpha_:\n",
    "        precompute_time, features, labels, idx_train, idx_val, idx_test, model = setup(dataset, normalization, K_standard, alpha)\n",
    "\n",
    "        model, acc_val, train_time = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
    "                    epochs, weight_decay, lr, dropout)\n",
    "        acc = test_regression(model, features[idx_test], labels[idx_test])\n",
    "\n",
    "        print(\"dataset:\", dataset, \", average accuracy:\", acc, \"alpha:\", alpha)\n",
    "for dataset in datasets:\n",
    "    alpha = alpha_standard\n",
    "    for K in K_:\n",
    "        precompute_time, features, labels, idx_train, idx_val, idx_test, model = setup(dataset, normalization, K, alpha_standard)\n",
    "\n",
    "        model, acc_val, train_time = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
    "                    epochs, weight_decay, lr, dropout)\n",
    "        acc = test_regression(model, features[idx_test], labels[idx_test])\n",
    "\n",
    "        print(\"dataset:\", dataset, \", average accuracy:\", acc, \"K:\", K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e748cd3",
   "metadata": {},
   "source": [
    "## Large OGB-ARXIV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cab2170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4765 Test Accuracy: 0.4338\n",
      "Pre-compute time: 15.0435s, train time: 20.8798s, total: 35.9233s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import load_citation, sgc_precompute, set_seed\n",
    "from models import get_model\n",
    "from metrics import accuracy\n",
    "import pickle as pkl\n",
    "#from args_cora import get_citation_args\n",
    "from time import perf_counter\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import torch_geometric.transforms as T\n",
    "from normalization import fetch_normalization, row_normalize\n",
    "from utils import *\n",
    "\n",
    "# Arguments\n",
    "#args = get_citation_args()\n",
    "\n",
    "#if args.tuned:\n",
    "#    if args.model == \"SGC\":\n",
    "#        with open(\"{}-tuning/{}.txt\".format(args.model, args.dataset), 'rb') as f:\n",
    "#            args.weight_decay = pkl.load(f)['weight_decay']\n",
    "#            print(\"using tuned weight decay: {}\".format(args.weight_decay))\n",
    "#    else:\n",
    "#        raise NotImplemented\n",
    "\n",
    "# setting random seeds\n",
    "#set_seed(args.seed, args.cuda)\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-arxiv', transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "#data.adj_t = data.adj_t.to_symmetric()\n",
    "features = data.x\n",
    "\n",
    "row, col, edge_attr = data.adj_t.t().coo()\n",
    "adj_values = [1]*len(row)\n",
    "adj = csr_matrix((adj_values, (row,col)), shape=(data.x.shape[0],data.x.shape[0]))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "adj, features = preprocess_citation(adj, features, \"AugNormAdj\")\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
    "features = torch.Tensor(features)\n",
    "split_idx = dataset.get_idx_split()\n",
    "idx_train = split_idx['train']\n",
    "idx_val = split_idx['valid']\n",
    "idx_test = split_idx['test']\n",
    "labels = torch.LongTensor([y[0] for y in data.y.tolist()])\n",
    "\n",
    "model = get_model(\"SGC\", features.shape[1], labels.max().item()+1, 0, 0, 0)\n",
    "\n",
    "features, precompute_time = sgc_precompute(features, adj, 16, 0.05)\n",
    "#print(\"{:.4f}s\".format(precompute_time))\n",
    "\n",
    "def train_regression(model,\n",
    "                     train_features, train_labels,\n",
    "                     val_features, val_labels,\n",
    "                     epochs=100, weight_decay=1e-05,\n",
    "                     lr=0.2, dropout=0):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "    t = perf_counter()\n",
    "    best_acc_val = torch.zeros((1))\n",
    "    best_loss_val = 100.\n",
    "    best_model = None\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_features)\n",
    "        loss_train = F.cross_entropy(output, train_labels)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output = model(val_features)\n",
    "            acc_val = accuracy(output, val_labels)\n",
    "            loss_val = F.cross_entropy(output, val_labels)\n",
    "            if best_acc_val < acc_val:\n",
    "                 best_acc_val = acc_val\n",
    "            #     best_model = model\n",
    "            if best_loss_val > loss_val:\n",
    "                best_loss_val = loss_val\n",
    "                best_model = model\n",
    "\n",
    "    train_time = perf_counter()-t\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     model.eval()\n",
    "    #     output = model(val_features)\n",
    "    #     acc_val = accuracy(output, val_labels)\n",
    "\n",
    "    return best_model, best_acc_val, train_time\n",
    "\n",
    "def test_regression(model, test_features, test_labels):\n",
    "    model.eval()\n",
    "    return accuracy(model(test_features), test_labels)\n",
    "\n",
    "acc_test = 0\n",
    "acc_val = 0\n",
    "train_time = 0\n",
    "for _ in range(10):\n",
    "    model, acc_val_buf, train_time_buf = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
    "                        100, 1e-05, 0.02, 0)\n",
    "    acc_test += test_regression(model, features[idx_test], labels[idx_test])\n",
    "    acc_val += acc_val_buf\n",
    "    train_time += train_time_buf\n",
    "acc_test /= 10\n",
    "acc_val /= 10\n",
    "train_time /= 10\n",
    "\n",
    "print(\"Validation Accuracy: {:.4f} Test Accuracy: {:.4f}\".format(acc_val, acc_test))\n",
    "print(\"Pre-compute time: {:.4f}s, train time: {:.4f}s, total: {:.4f}s\".format(precompute_time, train_time, precompute_time+train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b56ab",
   "metadata": {},
   "source": [
    "## Large OGB-MAG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919e26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[736389, 128], y=[736389, 1], adj_t=[736389, 736389, nnz=5416271])\n",
      "torch.Size([736389, 736389])\n",
      "torch.Size([736389, 736389])\n",
      "736389\n",
      "tensor([246, 131, 189,  ..., 266, 289,   1])\n",
      "95.1340s\n",
      "629571\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import load_citation, sgc_precompute, set_seed\n",
    "from models import get_model\n",
    "from metrics import accuracy\n",
    "import pickle as pkl\n",
    "#from args_cora import get_citation_args\n",
    "from time import perf_counter\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import torch_geometric.transforms as T\n",
    "from normalization import fetch_normalization, row_normalize\n",
    "from utils import *\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Arguments\n",
    "#args = get_citation_args()\n",
    "\n",
    "#if args.tuned:\n",
    "#    if args.model == \"SGC\":\n",
    "#        with open(\"{}-tuning/{}.txt\".format(args.model, args.dataset), 'rb') as f:\n",
    "#            args.weight_decay = pkl.load(f)['weight_decay']\n",
    "#            print(\"using tuned weight decay: {}\".format(args.weight_decay))\n",
    "#    else:\n",
    "#        raise NotImplemented\n",
    "\n",
    "# setting random seeds\n",
    "#set_seed(args.seed, args.cuda)\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-mag')\n",
    "data = dataset[0]\n",
    "\n",
    "data = Data(\n",
    "    x=data.x_dict['paper'],\n",
    "    edge_index=data.edge_index_dict[('paper', 'cites', 'paper')],\n",
    "    y=data.y_dict['paper'])\n",
    "features = data.x\n",
    "\n",
    "data = T.ToSparseTensor()(data)\n",
    "#data.adj_t = data.adj_t.to_symmetric()\n",
    "\n",
    "row, col, edge_attr = data.adj_t.t().coo()\n",
    "adj_values = [1]*len(row)\n",
    "adj = csr_matrix((adj_values, (row,col)), shape=(data.x.shape[0],data.x.shape[0]))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "adj, features = preprocess_citation(adj, features, \"AugNormAdj\")\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
    "features = torch.Tensor(features)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "idx_train = split_idx['train']['paper']\n",
    "idx_val = split_idx['valid']['paper']\n",
    "idx_test = split_idx['test']['paper']\n",
    "labels = torch.LongTensor([y[0] for y in data.y.tolist()])\n",
    "\n",
    "model = get_model(\"SGC\", features.shape[1], labels.max().item()+1, 0, 0, 0)\n",
    "\n",
    "features, precompute_time = sgc_precompute(features, adj, 16, 0.05)\n",
    "#print(\"{:.4f}s\".format(precompute_time))\n",
    "\n",
    "def train_regression(model,\n",
    "                     train_features, train_labels,\n",
    "                     val_features, val_labels,\n",
    "                     epochs=100, weight_decay=1e-05,\n",
    "                     lr=0.2, dropout=0):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "    t = perf_counter()\n",
    "    best_acc_val = torch.zeros((1))\n",
    "    best_loss_val = 100.\n",
    "    best_model = None\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_features)\n",
    "        loss_train = F.cross_entropy(output, train_labels)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output = model(val_features)\n",
    "            acc_val = accuracy(output, val_labels)\n",
    "            loss_val = F.cross_entropy(output, val_labels)\n",
    "            if best_acc_val < acc_val:\n",
    "                 best_acc_val = acc_val\n",
    "            #     best_model = model\n",
    "            if best_loss_val > loss_val:\n",
    "                best_loss_val = loss_val\n",
    "                best_model = model\n",
    "\n",
    "    train_time = perf_counter()-t\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     model.eval()\n",
    "    #     output = model(val_features)\n",
    "    #     acc_val = accuracy(output, val_labels)\n",
    "\n",
    "    return best_model, best_acc_val, train_time\n",
    "\n",
    "def test_regression(model, test_features, test_labels):\n",
    "    model.eval()\n",
    "    return accuracy(model(test_features), test_labels)\n",
    "\n",
    "acc_test = 0\n",
    "acc_val = 0\n",
    "train_time = 0\n",
    "for _ in range(10):\n",
    "    model, acc_val_buf, train_time_buf = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
    "                        100, 1e-05, 0.2, 0)\n",
    "    acc_test += test_regression(model, features[idx_test], labels[idx_test])\n",
    "    acc_val += acc_val_buf\n",
    "    train_time += train_time_buf\n",
    "acc_test /= 10\n",
    "acc_val /= 10\n",
    "train_time /= 10\n",
    "\n",
    "print(\"Validation Accuracy: {:.4f} Test Accuracy: {:.4f}\".format(acc_val, acc_test))\n",
    "print(\"Pre-compute time: {:.4f}s, train time: {:.4f}s, total: {:.4f}s\".format(precompute_time, train_time, precompute_time+train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c0567",
   "metadata": {},
   "source": [
    "## Large OGB-Products Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import load_citation, sgc_precompute, set_seed\n",
    "from models import get_model\n",
    "from metrics import accuracy\n",
    "import pickle as pkl\n",
    "#from args_cora import get_citation_args\n",
    "from time import perf_counter\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import torch_geometric.transforms as T\n",
    "from normalization import fetch_normalization, row_normalize\n",
    "from utils import *\n",
    "\n",
    "# Arguments\n",
    "#args = get_citation_args()\n",
    "\n",
    "#if args.tuned:\n",
    "#    if args.model == \"SGC\":\n",
    "#        with open(\"{}-tuning/{}.txt\".format(args.model, args.dataset), 'rb') as f:\n",
    "#            args.weight_decay = pkl.load(f)['weight_decay']\n",
    "#            print(\"using tuned weight decay: {}\".format(args.weight_decay))\n",
    "#    else:\n",
    "#        raise NotImplemented\n",
    "\n",
    "# setting random seeds\n",
    "#set_seed(args.seed, args.cuda)\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-products', transform=T.ToSparseTensor())\n",
    "data = dataset[0]\n",
    "#data.adj_t = data.adj_t.to_symmetric()\n",
    "features = data.x\n",
    "\n",
    "row, col, edge_attr = data.adj_t.t().coo()\n",
    "adj_values = [1]*len(row)\n",
    "adj = csr_matrix((adj_values, (row,col)), shape=(data.x.shape[0],data.x.shape[0]))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "adj, features = preprocess_citation(adj, features, \"AugNormAdj\")\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
    "features = torch.Tensor(features)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "idx_train = split_idx['train']\n",
    "idx_val = split_idx['valid']\n",
    "idx_test = split_idx['test']\n",
    "labels = torch.LongTensor([y[0] for y in data.y.tolist()])\n",
    "\n",
    "model = get_model(\"SGC\", features.shape[1], labels.max().item()+1, 0, 0, 0)\n",
    "\n",
    "features, precompute_time = sgc_precompute(features, adj, 16, 0.05)\n",
    "print(\"{:.4f}s\".format(precompute_time))\n",
    "\n",
    "def train_regression(model,\n",
    "                     train_features, train_labels,\n",
    "                     val_features, val_labels,\n",
    "                     epochs=100, weight_decay=1e-05,\n",
    "                     lr=0.2, dropout=0):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "    t = perf_counter()\n",
    "    best_acc_val = torch.zeros((1))\n",
    "    best_loss_val = 100.\n",
    "    best_model = None\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_features)\n",
    "        loss_train = F.cross_entropy(output, train_labels)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output = model(val_features)\n",
    "            acc_val = accuracy(output, val_labels)\n",
    "            loss_val = F.cross_entropy(output, val_labels)\n",
    "            if best_acc_val < acc_val:\n",
    "                 best_acc_val = acc_val\n",
    "            #     best_model = model\n",
    "            if best_loss_val > loss_val:\n",
    "                best_loss_val = loss_val\n",
    "                best_model = model\n",
    "\n",
    "    train_time = perf_counter()-t\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     model.eval()\n",
    "    #     output = model(val_features)\n",
    "    #     acc_val = accuracy(output, val_labels)\n",
    "\n",
    "    return best_model, best_acc_val, train_time\n",
    "\n",
    "def test_regression(model, test_features, test_labels):\n",
    "    model.eval()\n",
    "    return accuracy(model(test_features), test_labels)\n",
    "acc_test = 0\n",
    "acc_val = 0\n",
    "train_time = 0\n",
    "for _ in range(10):\n",
    "    model, acc_val_buf, train_time_buf = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
    "                        1000, 1e-05, 0.2, 0)\n",
    "    acc_test += test_regression(model, features[idx_test], labels[idx_test])\n",
    "    acc_val += acc_val_buf\n",
    "    train_time += train_time_buf\n",
    "acc_test /= 10\n",
    "acc_val /= 10\n",
    "train_time /= 10\n",
    "\n",
    "print(\"Validation Accuracy: {:.4f} Test Accuracy: {:.4f}\".format(acc_val, acc_test))\n",
    "print(\"Pre-compute time: {:.4f}s, train time: {:.4f}s, total: {:.4f}s\".format(precompute_time, train_time, precompute_time+train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa9898",
   "metadata": {},
   "source": [
    "## Large Reddit Dataset / Community Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea5fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import load_citation, sgc_precompute, set_seed\n",
    "from models import get_model\n",
    "from metrics import accuracy\n",
    "import pickle as pkl\n",
    "#from args_cora import get_citation_args\n",
    "from time import perf_counter\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import torch_geometric.transforms as T\n",
    "from normalization import fetch_normalization, row_normalize\n",
    "from utils import *\n",
    "from torch_geometric.datasets import Reddit\n",
    "\n",
    "# Arguments\n",
    "#args = get_citation_args()\n",
    "\n",
    "#if args.tuned:\n",
    "#    if args.model == \"SGC\":\n",
    "#        with open(\"{}-tuning/{}.txt\".format(args.model, args.dataset), 'rb') as f:\n",
    "#            args.weight_decay = pkl.load(f)['weight_decay']\n",
    "#            print(\"using tuned weight decay: {}\".format(args.weight_decay))\n",
    "#    else:\n",
    "#        raise NotImplemented\n",
    "\n",
    "# setting random seeds\n",
    "#set_seed(args.seed, args.cuda)\n",
    "\n",
    "dataset = Reddit(\"./dataset/reddit\")\n",
    "data = dataset[0]\n",
    "#data.adj_t = data.adj_t.to_symmetric()\n",
    "features = data.x\n",
    "\n",
    "#data = T.ToSparseTensor()(data)\n",
    "\n",
    "row, col, edge_attr = data.adj_t.t().coo()\n",
    "adj_values = [1]*len(row)\n",
    "adj = csr_matrix((adj_values, (row,col)), shape=(data.x.shape[0],data.x.shape[0]))\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "adj, features = preprocess_citation(adj, features, \"AugNormAdj\")\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
    "features = torch.Tensor(features)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "idx_train = split_idx['train']\n",
    "idx_val = split_idx['valid']\n",
    "idx_test = split_idx['test']\n",
    "labels = torch.LongTensor([y[0] for y in data.y.tolist()])\n",
    "\n",
    "model = get_model(\"SGC\", features.shape[1], labels.max().item()+1, 0, 0, 0)\n",
    "\n",
    "features, precompute_time = sgc_precompute(features, adj, 16, 0.05)\n",
    "#print(\"{:.4f}s\".format(precompute_time))\n",
    "\n",
    "def train_regression(model,\n",
    "                     train_features, train_labels,\n",
    "                     val_features, val_labels,\n",
    "                     epochs=100, weight_decay=1e-05,\n",
    "                     lr=0.2, dropout=0):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "    t = perf_counter()\n",
    "    best_acc_val = torch.zeros((1))\n",
    "    best_loss_val = 100.\n",
    "    best_model = None\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_features)\n",
    "        loss_train = F.cross_entropy(output, train_labels)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output = model(val_features)\n",
    "            acc_val = accuracy(output, val_labels)\n",
    "            loss_val = F.cross_entropy(output, val_labels)\n",
    "            if best_acc_val < acc_val:\n",
    "                 best_acc_val = acc_val\n",
    "            #     best_model = model\n",
    "            if best_loss_val > loss_val:\n",
    "                best_loss_val = loss_val\n",
    "                best_model = model\n",
    "\n",
    "    train_time = perf_counter()-t\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     model.eval()\n",
    "    #     output = model(val_features)\n",
    "    #     acc_val = accuracy(output, val_labels)\n",
    "\n",
    "    return best_model, best_acc_val, train_time\n",
    "\n",
    "def test_regression(model, test_features, test_labels):\n",
    "    model.eval()\n",
    "    return accuracy(model(test_features), test_labels)\n",
    "acc_test = 0\n",
    "acc_val = 0\n",
    "train_time = 0\n",
    "for _ in range(10):\n",
    "    model, acc_val_buf, train_time_buf = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
    "                        1000, 1e-05, 0.2, 0)\n",
    "    acc_test += test_regression(model, features[idx_test], labels[idx_test])\n",
    "    acc_val += acc_val_buf\n",
    "    train_time += train_time_buf\n",
    "acc_test /= 10\n",
    "acc_val /= 10\n",
    "train_time /= 10\n",
    "\n",
    "print(\"Validation Accuracy: {:.4f} Test Accuracy: {:.4f}\".format(acc_val, acc_test))\n",
    "print(\"Pre-compute time: {:.4f}s, train time: {:.4f}s, total: {:.4f}s\".format(precompute_time, train_time, precompute_time+train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83a311",
   "metadata": {},
   "source": [
    "## Large OGB ARXIV (SSGC + MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030020d4",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1a3f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.utils import dropout_adj\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "from typing import Optional\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "#from logger import Logger\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "\n",
    "\n",
    "class SGConv(MessagePassing):\n",
    "    r\"\"\"The simple graph convolutional operator from the `\"Simplifying Graph\n",
    "    Convolutional Networks\" <https://arxiv.org/abs/1902.07153>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = {\\left(\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X} \\mathbf{\\Theta},\n",
    "\n",
    "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
    "    adjacency matrix with inserted self-loops and\n",
    "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        K (int, optional): Number of hops :math:`K`. (default: :obj:`1`)\n",
    "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
    "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
    "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X}` on\n",
    "            first execution, and will use the cached version for further\n",
    "            executions.\n",
    "            This parameter should only be set to :obj:`True` in transductive\n",
    "            learning scenarios. (default: :obj:`False`)\n",
    "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
    "            self-loops to the input graph. (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "\n",
    "    _cached_x: Optional[Tensor]\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, K: int = 1,\n",
    "                 cached: bool = False, add_self_loops: bool = True,\n",
    "                 bias: bool = True, dropout: float = 0.05, **kwargs):\n",
    "        super(SGConv, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self._cached_x = None\n",
    "\n",
    "        self.lin = Linear(in_channels, out_channels, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #self.lin.reset_parameters()\n",
    "        self._cached_x = None\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_weight: OptTensor = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        cache = self._cached_x\n",
    "        if cache is None:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
    "                    self.add_self_loops, dtype=x.dtype)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                edge_index = gcn_norm(  # yapf: disable\n",
    "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
    "                    self.add_self_loops, dtype=x.dtype)\n",
    "\n",
    "            x_set = []\n",
    "            alpha = 0.05\n",
    "            output = alpha * x\n",
    "            #temp_edge_index, edge_weight = dropout_adj(edge_index, 0.5)\n",
    "            for k in range(self.K):\n",
    "                x = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
    "                                   size=None)\n",
    "                # x_set.append(x)\n",
    "                output = output + (1. / self.K) * x\n",
    "            # x = torch.stack(x_set,2)\n",
    "            # alpha = 0.05\n",
    "            # x = (1-alpha)*torch.mean(x,2).squeeze() + alpha*x_ori\n",
    "            x = output\n",
    "            if self.cached:\n",
    "                self._cached_x = x\n",
    "        else:\n",
    "            x = cache\n",
    "\n",
    "        return x#self.lin(x)\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
    "        return edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, K={})'.format(self.__class__.__name__,\n",
    "                                         self.in_channels, self.out_channels,\n",
    "                                         self.K)\n",
    "\n",
    "\n",
    "class SGC(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden, out_channels, num_layers, dropout):\n",
    "        super(SGC, self).__init__()\n",
    "        self.conv1 = SGConv(\n",
    "            in_channels, hidden, K=num_layers, cached=True)\n",
    "        self.lin = torch.nn.Linear(hidden, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "        #x = self.conv1(x, edge_index)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.conv1(x, edge_index)#F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.adj_t)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-arxiv',\n",
    "                                     transform=T.ToSparseTensor())\n",
    "\n",
    "    data = dataset[0]\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "    data = data.to(device)\n",
    "\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train'].to(device)\n",
    "\n",
    "    model = SGC(data.num_features, 256, dataset.num_classes, 10, 0.5).to(device)\n",
    "    \n",
    "    features = model(data.x, data.adj_t)\n",
    "    torch.save(features,'embedding.pt')\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948bd125",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96755de8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 01, Loss: 3.8672, Train: 28.98%, Valid: 30.73%, Test: 27.53%\n",
      "Run: 01, Epoch: 02, Loss: 2.7950, Train: 27.00%, Valid: 28.08%, Test: 25.22%\n",
      "Run: 01, Epoch: 03, Loss: 2.3513, Train: 28.27%, Valid: 30.29%, Test: 27.12%\n",
      "Run: 01, Epoch: 04, Loss: 2.1870, Train: 33.62%, Valid: 37.98%, Test: 38.53%\n",
      "Run: 01, Epoch: 05, Loss: 2.0279, Train: 37.96%, Valid: 32.85%, Test: 35.98%\n",
      "Run: 01, Epoch: 06, Loss: 1.9115, Train: 38.57%, Valid: 39.47%, Test: 42.06%\n",
      "Run: 01, Epoch: 07, Loss: 1.8079, Train: 38.73%, Valid: 39.45%, Test: 42.62%\n",
      "Run: 01, Epoch: 08, Loss: 1.7401, Train: 35.82%, Valid: 29.89%, Test: 33.48%\n",
      "Run: 01, Epoch: 09, Loss: 1.6700, Train: 36.90%, Valid: 29.46%, Test: 32.85%\n",
      "Run: 01, Epoch: 10, Loss: 1.6216, Train: 39.17%, Valid: 32.02%, Test: 35.20%\n",
      "Run: 01, Epoch: 11, Loss: 1.5720, Train: 39.76%, Valid: 32.57%, Test: 35.78%\n",
      "Run: 01, Epoch: 12, Loss: 1.5314, Train: 40.29%, Valid: 32.44%, Test: 35.79%\n",
      "Run: 01, Epoch: 13, Loss: 1.5028, Train: 43.15%, Valid: 36.26%, Test: 39.41%\n",
      "Run: 01, Epoch: 14, Loss: 1.4712, Train: 45.89%, Valid: 39.77%, Test: 42.65%\n",
      "Run: 01, Epoch: 15, Loss: 1.4433, Train: 45.78%, Valid: 37.96%, Test: 40.68%\n",
      "Run: 01, Epoch: 16, Loss: 1.4209, Train: 45.09%, Valid: 36.80%, Test: 39.48%\n",
      "Run: 01, Epoch: 17, Loss: 1.4027, Train: 46.40%, Valid: 39.06%, Test: 41.91%\n",
      "Run: 01, Epoch: 18, Loss: 1.3763, Train: 48.51%, Valid: 41.78%, Test: 44.39%\n",
      "Run: 01, Epoch: 19, Loss: 1.3586, Train: 48.74%, Valid: 40.81%, Test: 43.55%\n",
      "Run: 01, Epoch: 20, Loss: 1.3373, Train: 49.06%, Valid: 40.80%, Test: 43.63%\n",
      "Run: 01, Epoch: 21, Loss: 1.3260, Train: 50.97%, Valid: 44.16%, Test: 46.53%\n",
      "Run: 01, Epoch: 22, Loss: 1.3135, Train: 52.25%, Valid: 46.30%, Test: 48.31%\n",
      "Run: 01, Epoch: 23, Loss: 1.2995, Train: 51.85%, Valid: 45.07%, Test: 47.36%\n",
      "Run: 01, Epoch: 24, Loss: 1.2867, Train: 52.11%, Valid: 45.48%, Test: 47.73%\n",
      "Run: 01, Epoch: 25, Loss: 1.2717, Train: 54.66%, Valid: 49.70%, Test: 51.60%\n",
      "Run: 01, Epoch: 26, Loss: 1.2590, Train: 56.61%, Valid: 52.55%, Test: 54.15%\n",
      "Run: 01, Epoch: 27, Loss: 1.2527, Train: 57.17%, Valid: 52.87%, Test: 54.41%\n",
      "Run: 01, Epoch: 28, Loss: 1.2396, Train: 57.85%, Valid: 53.79%, Test: 55.32%\n",
      "Run: 01, Epoch: 29, Loss: 1.2300, Train: 59.26%, Valid: 56.10%, Test: 57.68%\n",
      "Run: 01, Epoch: 30, Loss: 1.2228, Train: 60.57%, Valid: 57.88%, Test: 59.47%\n",
      "Run: 01, Epoch: 31, Loss: 1.2106, Train: 61.35%, Valid: 58.58%, Test: 60.34%\n",
      "Run: 01, Epoch: 32, Loss: 1.2014, Train: 62.01%, Valid: 59.66%, Test: 61.29%\n",
      "Run: 01, Epoch: 33, Loss: 1.1955, Train: 63.05%, Valid: 61.43%, Test: 62.95%\n",
      "Run: 01, Epoch: 34, Loss: 1.1869, Train: 64.01%, Valid: 62.62%, Test: 64.04%\n",
      "Run: 01, Epoch: 35, Loss: 1.1821, Train: 64.65%, Valid: 63.41%, Test: 64.72%\n",
      "Run: 01, Epoch: 36, Loss: 1.1715, Train: 65.25%, Valid: 64.33%, Test: 65.68%\n",
      "Run: 01, Epoch: 37, Loss: 1.1687, Train: 65.44%, Valid: 64.68%, Test: 65.99%\n",
      "Run: 01, Epoch: 38, Loss: 1.1583, Train: 66.11%, Valid: 65.46%, Test: 66.59%\n",
      "Run: 01, Epoch: 39, Loss: 1.1534, Train: 66.53%, Valid: 66.02%, Test: 66.98%\n",
      "Run: 01, Epoch: 40, Loss: 1.1440, Train: 66.84%, Valid: 66.49%, Test: 67.35%\n",
      "Run: 01, Epoch: 41, Loss: 1.1435, Train: 67.40%, Valid: 67.25%, Test: 67.88%\n",
      "Run: 01, Epoch: 42, Loss: 1.1369, Train: 67.89%, Valid: 68.02%, Test: 68.45%\n",
      "Run: 01, Epoch: 43, Loss: 1.1338, Train: 68.18%, Valid: 68.36%, Test: 68.77%\n",
      "Run: 01, Epoch: 44, Loss: 1.1291, Train: 68.41%, Valid: 68.61%, Test: 68.99%\n",
      "Run: 01, Epoch: 45, Loss: 1.1234, Train: 68.66%, Valid: 68.72%, Test: 69.08%\n",
      "Run: 01, Epoch: 46, Loss: 1.1247, Train: 69.05%, Valid: 69.08%, Test: 69.31%\n",
      "Run: 01, Epoch: 47, Loss: 1.1128, Train: 69.04%, Valid: 69.11%, Test: 69.39%\n",
      "Run: 01, Epoch: 48, Loss: 1.1112, Train: 69.34%, Valid: 69.44%, Test: 69.60%\n",
      "Run: 01, Epoch: 49, Loss: 1.1066, Train: 69.69%, Valid: 69.55%, Test: 69.41%\n",
      "Run: 01, Epoch: 50, Loss: 1.1023, Train: 69.83%, Valid: 69.53%, Test: 69.23%\n",
      "Run: 01, Epoch: 51, Loss: 1.0964, Train: 69.68%, Valid: 69.50%, Test: 69.40%\n",
      "Run: 01, Epoch: 52, Loss: 1.0941, Train: 69.95%, Valid: 69.82%, Test: 69.74%\n",
      "Run: 01, Epoch: 53, Loss: 1.0919, Train: 70.27%, Valid: 70.01%, Test: 69.70%\n",
      "Run: 01, Epoch: 54, Loss: 1.0897, Train: 70.35%, Valid: 70.16%, Test: 69.70%\n",
      "Run: 01, Epoch: 55, Loss: 1.0858, Train: 70.40%, Valid: 69.94%, Test: 69.59%\n",
      "Run: 01, Epoch: 56, Loss: 1.0830, Train: 70.56%, Valid: 70.05%, Test: 69.55%\n",
      "Run: 01, Epoch: 57, Loss: 1.0780, Train: 70.71%, Valid: 70.34%, Test: 69.68%\n",
      "Run: 01, Epoch: 58, Loss: 1.0743, Train: 70.67%, Valid: 70.51%, Test: 70.00%\n",
      "Run: 01, Epoch: 59, Loss: 1.0761, Train: 70.88%, Valid: 70.46%, Test: 69.89%\n",
      "Run: 01, Epoch: 60, Loss: 1.0681, Train: 70.95%, Valid: 70.49%, Test: 69.78%\n",
      "Run: 01, Epoch: 61, Loss: 1.0676, Train: 70.86%, Valid: 70.52%, Test: 69.81%\n",
      "Run: 01, Epoch: 62, Loss: 1.0658, Train: 71.09%, Valid: 70.49%, Test: 69.61%\n",
      "Run: 01, Epoch: 63, Loss: 1.0624, Train: 71.14%, Valid: 70.66%, Test: 70.13%\n",
      "Run: 01, Epoch: 64, Loss: 1.0598, Train: 71.13%, Valid: 70.75%, Test: 70.40%\n",
      "Run: 01, Epoch: 65, Loss: 1.0583, Train: 71.29%, Valid: 70.71%, Test: 69.90%\n",
      "Run: 01, Epoch: 66, Loss: 1.0483, Train: 71.36%, Valid: 70.66%, Test: 69.83%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-44b5d89fdc56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m#logger.print_statistics()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-44b5d89fdc56>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m#logger.add_result(run, result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-44b5d89fdc56>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, x, y_true, train_idx, optimizer)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "#from logger import Logger\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "def train(model, x, y_true, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x[train_idx])\n",
    "    loss = F.nll_loss(out, y_true.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, x, y_true, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(x)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-arxiv')\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    data = dataset[0]\n",
    "\n",
    "    x = data.x\n",
    "    embedding = torch.load('embedding.pt', map_location='cpu')\n",
    "    #x = torch.cat([x, embedding], dim=-1)\n",
    "    x = embedding\n",
    "    x = x.to(device)\n",
    "\n",
    "    y_true = data.y.to(device)\n",
    "    train_idx = split_idx['train'].to(device)\n",
    "\n",
    "    model = MLP(x.size(-1), 256, dataset.num_classes,\n",
    "                3, 0.5).to(device)\n",
    "\n",
    "    evaluator = Evaluator(name='ogbn-arxiv')\n",
    "    #logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(10):\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        for epoch in range(1, 1 + 500):\n",
    "            loss = train(model, x, y_true, train_idx, optimizer)\n",
    "            result = test(model, x, y_true, split_idx, evaluator)\n",
    "            #logger.add_result(run, result)\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}%, '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        #logger.print_statistics(run)\n",
    "    #logger.print_statistics()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194e7dd0",
   "metadata": {},
   "source": [
    "# Large OGB MAG (SSGC + MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a0cce",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a531334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "from typing import Optional\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "#from logger import Logger\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "\n",
    "\n",
    "class SGConv(MessagePassing):\n",
    "    r\"\"\"The simple graph convolutional operator from the `\"Simplifying Graph\n",
    "    Convolutional Networks\" <https://arxiv.org/abs/1902.07153>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = {\\left(\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X} \\mathbf{\\Theta},\n",
    "\n",
    "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
    "    adjacency matrix with inserted self-loops and\n",
    "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        K (int, optional): Number of hops :math:`K`. (default: :obj:`1`)\n",
    "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
    "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
    "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X}` on\n",
    "            first execution, and will use the cached version for further\n",
    "            executions.\n",
    "            This parameter should only be set to :obj:`True` in transductive\n",
    "            learning scenarios. (default: :obj:`False`)\n",
    "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
    "            self-loops to the input graph. (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "\n",
    "    _cached_x: Optional[Tensor]\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, K: int = 1,\n",
    "                 cached: bool = False, add_self_loops: bool = True,\n",
    "                 bias: bool = True, dropout: float = 0.05, **kwargs):\n",
    "        super(SGConv, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self._cached_x = None\n",
    "\n",
    "        #self.lin = Linear(in_channels, out_channels, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #self.lin.reset_parameters()\n",
    "        self._cached_x = None\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_weight: OptTensor = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        cache = self._cached_x\n",
    "        if cache is None:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
    "                    self.add_self_loops, dtype=x.dtype)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                edge_index = gcn_norm(  # yapf: disable\n",
    "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
    "                    self.add_self_loops, dtype=x.dtype)\n",
    "\n",
    "            x = F.normalize(x,dim=1, p=2)\n",
    "            alpha = 0.05\n",
    "            output = alpha*x\n",
    "            for k in range(self.K):\n",
    "                x = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
    "                                   size=None)\n",
    "                #x_set.append(x)\n",
    "                output = output + (1./self.K)*x\n",
    "            #x = torch.stack(x_set,2)\n",
    "            #alpha = 0.05\n",
    "            #x = (1-alpha)*torch.mean(x,2).squeeze() + alpha*x_ori\n",
    "            x = output\n",
    "            if self.cached:\n",
    "                self._cached_x = x\n",
    "        else:\n",
    "            x = cache\n",
    "\n",
    "        return x#self.lin(x)\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
    "        return edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, K={})'.format(self.__class__.__name__,\n",
    "                                         self.in_channels, self.out_channels,\n",
    "                                         self.K)\n",
    "\n",
    "\n",
    "class SGC(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers):\n",
    "        super(SGC, self).__init__()\n",
    "        self.conv1 = SGConv(\n",
    "            in_channels, out_channels, K=num_layers, cached=True)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "        #x = self.conv1(x, edge_index)\n",
    "        return self.conv1(x, edge_index)#F.log_softmax(x, dim=1)\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            GCNConv(in_channels, hidden_channels, normalize=False))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, hidden_channels, normalize=False))\n",
    "        self.convs.append(\n",
    "            GCNConv(hidden_channels, out_channels, normalize=False))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.adj_t)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.adj_t)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']['paper']],\n",
    "        'y_pred': y_pred[split_idx['train']['paper']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']['paper']],\n",
    "        'y_pred': y_pred[split_idx['valid']['paper']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']['paper']],\n",
    "        'y_pred': y_pred[split_idx['test']['paper']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-mag')\n",
    "    rel_data = dataset[0]\n",
    "\n",
    "    # We are only interested in paper <-> paper relations.\n",
    "    data = Data(\n",
    "        x=rel_data.x_dict['paper'],\n",
    "        edge_index=rel_data.edge_index_dict[('paper', 'cites', 'paper')],\n",
    "        y=rel_data.y_dict['paper'])\n",
    "\n",
    "    data = T.ToSparseTensor()(data)\n",
    "    data.adj_t = data.adj_t.to_symmetric()\n",
    "\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train']['paper'].to(device)\n",
    "\n",
    "    model = SGC(data.num_features,\n",
    "                dataset.num_classes, 16,\n",
    "                ).to(device)\n",
    "\n",
    "    # Pre-compute GCN normalization.\n",
    "    adj_t = data.adj_t.set_diag()\n",
    "    deg = adj_t.sum(dim=1).to(torch.float)\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
    "    data.adj_t = adj_t\n",
    "\n",
    "    data = data.to(device)\n",
    "    features = model(data.x, data.adj_t)\n",
    "    torch.save(features,'embedding.pt')\n",
    "    # evaluator = Evaluator(name='ogbn-mag')\n",
    "    # logger = Logger(args.runs, args)\n",
    "    #\n",
    "    # for run in range(args.runs):\n",
    "    #     model.reset_parameters()\n",
    "    #     optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    #     for epoch in range(1, 1 + args.epochs):\n",
    "    #         loss = train(model, data, train_idx, optimizer)\n",
    "    #         result = test(model, data, split_idx, evaluator)\n",
    "    #         logger.add_result(run, result)\n",
    "    #\n",
    "    #         if epoch % args.log_steps == 0:\n",
    "    #             train_acc, valid_acc, test_acc = result\n",
    "    #             print(f'Run: {run + 1:02d}, '\n",
    "    #                   f'Epoch: {epoch:02d}, '\n",
    "    #                   f'Loss: {loss:.4f}, '\n",
    "    #                   f'Train: {100 * train_acc:.2f}%, '\n",
    "    #                   f'Valid: {100 * valid_acc:.2f}% '\n",
    "    #                   f'Test: {100 * test_acc:.2f}%')\n",
    "    #\n",
    "    #     logger.print_statistics(run)\n",
    "    #logger.print_statistics()\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477508c",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "#from logger import Logger\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "def train(model, x, y_true, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x[train_idx])\n",
    "    loss = F.nll_loss(out, y_true[train_idx].squeeze(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, x, y_true, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(x)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['train']['paper']],\n",
    "        'y_pred': y_pred[split_idx['train']['paper']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['valid']['paper']],\n",
    "        'y_pred': y_pred[split_idx['valid']['paper']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['test']['paper']],\n",
    "        'y_pred': y_pred[split_idx['test']['paper']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    dataset = PygNodePropPredDataset(name='ogbn-mag')\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    data = dataset[0]\n",
    "\n",
    "    x = data.x_dict['paper']\n",
    "    embedding = torch.load('embedding.pt', map_location='cpu')\n",
    "    x = torch.cat([x, embedding], dim=-1)\n",
    "    x = x.to(device)\n",
    "\n",
    "    y_true = data.y_dict['paper'].to(device)\n",
    "    train_idx = split_idx['train']['paper'].to(device)\n",
    "\n",
    "    model = MLP(x.size(-1), 256, dataset.num_classes,\n",
    "                3, 0.0).to(device)\n",
    "\n",
    "    evaluator = Evaluator(name='ogbn-mag')\n",
    "    #logger = Logger(args.runs, args)\n",
    "\n",
    "    for run in range(10):\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        for epoch in range(1, 1 + 500):\n",
    "            loss = train(model, x, y_true, train_idx, optimizer)\n",
    "            result = test(model, x, y_true, split_idx, evaluator)\n",
    "            #logger.add_result(run, result)\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                train_acc, valid_acc, test_acc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_acc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_acc:.2f}%, '\n",
    "                      f'Test: {100 * test_acc:.2f}%')\n",
    "\n",
    "        #logger.print_statistics(run)\n",
    "    #logger.print_statistics()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcdb36d",
   "metadata": {},
   "source": [
    "# Node Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c952f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: cora acc: 0.666220457902511  nmi: 0.5144215184197067 f1-score:  0.628469061350163\n",
      "dataset: citeseer acc: 0.6838315440847043  nmi: 0.4237276469138317 f1-score:  0.6406018803310946\n",
      "dataset: pubmed acc: 0.6967690359571546  nmi: 0.3179644272550732 f1-score:  0.6907774366144597\n",
      "dataset: wiki acc: 0.5201883331295096  nmi: 0.4899097867412388 f1-score:  0.4443854594424592\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.cluster import KMeans\n",
    "from NodeClustering.metrics import clustering_metrics\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "def normalize_adj(adj, type='sym'):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    if type == 'sym':\n",
    "        adj = sp.coo_matrix(adj)\n",
    "        rowsum = np.array(adj.sum(1))\n",
    "        # d_inv_sqrt = np.power(rowsum, -0.5)\n",
    "        # d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "        # return adj*d_inv_sqrt*d_inv_sqrt.flatten()\n",
    "        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "        return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
    "    elif type == 'rw':\n",
    "        rowsum = np.array(adj.sum(1))\n",
    "        d_inv = np.power(rowsum, -1.0).flatten()\n",
    "        d_inv[np.isinf(d_inv)] = 0.\n",
    "        d_mat_inv = sp.diags(d_inv)\n",
    "        adj_normalized = d_mat_inv.dot(adj)\n",
    "        return adj_normalized\n",
    "\n",
    "\n",
    "def preprocess_adj(adj, type='sym', loop=True):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    if loop:\n",
    "        adj = adj + sp.eye(adj.shape[0])\n",
    "    adj_normalized = normalize_adj(adj, type=type)\n",
    "    return adj_normalized\n",
    "\n",
    "\n",
    "def to_onehot(prelabel):\n",
    "    k = len(np.unique(prelabel))\n",
    "    label = np.zeros([prelabel.shape[0], k])\n",
    "    label[range(prelabel.shape[0]), prelabel] = 1\n",
    "    label = label.T\n",
    "    return label\n",
    "\n",
    "\n",
    "def square_dist(prelabel, feature):\n",
    "    if sp.issparse(feature):\n",
    "        feature = feature.todense()\n",
    "    feature = np.array(feature)\n",
    "\n",
    "\n",
    "    onehot = to_onehot(prelabel)\n",
    "\n",
    "    m, n = onehot.shape\n",
    "    count = onehot.sum(1).reshape(m, 1)\n",
    "    count[count==0] = 1\n",
    "\n",
    "    mean = onehot.dot(feature)/count\n",
    "    a2 = (onehot.dot(feature*feature)/count).sum(1)\n",
    "    pdist2 = np.array(a2 + a2.T - 2*mean.dot(mean.T))\n",
    "\n",
    "    intra_dist = pdist2.trace()\n",
    "    inter_dist = pdist2.sum() - intra_dist\n",
    "    intra_dist /= m\n",
    "    inter_dist /= m * (m - 1)\n",
    "    return intra_dist\n",
    "\n",
    "def dist(prelabel, feature):\n",
    "    k = len(np.unique(prelabel))\n",
    "    intra_dist = 0\n",
    "\n",
    "    for i in range(k):\n",
    "        Data_i = feature[np.where(prelabel == i)]\n",
    "\n",
    "        Dis = euclidean_distances(Data_i, Data_i)\n",
    "        n_i = Data_i.shape[0]\n",
    "        if n_i == 0 or n_i == 1:\n",
    "            intra_dist = intra_dist\n",
    "        else:\n",
    "            intra_dist = intra_dist + 1 / k * 1 / (n_i * (n_i - 1)) * sum(sum(Dis))\n",
    "\n",
    "\n",
    "    return intra_dist\n",
    "\n",
    "datasets = ['cora', 'citeseer', 'pubmed', 'wiki']\n",
    "for dataset in datasets:\n",
    "    \n",
    "    #for i in range(10): dont have the computational capacities\n",
    "    data = sio.loadmat('./NodeClustering/{}.mat'.format(dataset))\n",
    "    feature = data['fea']\n",
    "    if sp.issparse(feature):\n",
    "        feature = feature.todense()\n",
    "\n",
    "    adj = data['W']\n",
    "    gnd = data['gnd']\n",
    "    gnd = gnd.T\n",
    "    gnd = gnd - 1\n",
    "    gnd = gnd[0, :]\n",
    "    k = len(np.unique(gnd))\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    intra_list = []\n",
    "    intra_list.append(10000)\n",
    "\n",
    "\n",
    "    acc_list = []\n",
    "    nmi_list = []\n",
    "    f1_list = []\n",
    "    stdacc_list = []\n",
    "    stdnmi_list = []\n",
    "    stdf1_list = []\n",
    "    max_iter = 60\n",
    "    rep = 10\n",
    "    t = time.time()\n",
    "    #adj_normalized = preprocess_adj(adj)\n",
    "    #adj_normalized = (sp.eye(adj_normalized.shape[0]) + adj_normalized) / 2\n",
    "    adj_normalized = preprocess_adj(adj,loop=False)\n",
    "    # adj_normalized = (sp.eye(adj_normalized.shape[0]) + adj_normalized + adj_normalized.dot(adj_normalized)) / 3\n",
    "    total_dist = []\n",
    "\n",
    "    tt = 0\n",
    "    alpha = 0.05 #0.05 for pubmed, 0.1 for citeseer\n",
    "    feature_ori = feature.astype('float64')\n",
    "    #emb = feature.astype('float64')\n",
    "    #emb = feature.astype('float64')\n",
    "    emb = np.zeros_like(feature).astype('float64')\n",
    "    oneV = np.ones(feature.shape[0])\n",
    "    den = np.zeros_like(oneV)\n",
    "    while 1:\n",
    "        tt = tt + 1\n",
    "        power = tt\n",
    "        intraD = np.zeros(rep)\n",
    "\n",
    "\n",
    "        ac = np.zeros(rep)\n",
    "        nm = np.zeros(rep)\n",
    "        f1 = np.zeros(rep)\n",
    "\n",
    "\n",
    "\n",
    "        feature = adj_normalized.dot(feature)\n",
    "\n",
    "        emb += feature\n",
    "        emb_norm = emb/tt\n",
    "        u, s, v = sp.linalg.svds(emb_norm, k=16, which='LM')\n",
    "        u = normalize(emb_norm.dot(v.T))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(rep):\n",
    "            kmeans = KMeans(n_clusters=k).fit(u)\n",
    "            predict_labels = kmeans.predict(u)\n",
    "            intraD[i] = square_dist(predict_labels, u)\n",
    "            #intraD[i] = dist(predict_labels, feature)\n",
    "            cm = clustering_metrics(gnd, predict_labels)\n",
    "            ac[i], nm[i], f1[i] = cm.evaluationClusterModelFromLabel()\n",
    "\n",
    "        intramean = np.mean(intraD)\n",
    "        acc_means = np.mean(ac)\n",
    "        acc_stds = np.std(ac)\n",
    "        nmi_means = np.mean(nm)\n",
    "        nmi_stds = np.std(nm)\n",
    "        f1_means = np.mean(f1)\n",
    "        f1_stds = np.std(f1)\n",
    "\n",
    "        intra_list.append(intramean)\n",
    "        acc_list.append(acc_means)\n",
    "        stdacc_list.append(acc_stds)\n",
    "        nmi_list.append(nmi_means)\n",
    "        stdnmi_list.append(nmi_stds)\n",
    "        f1_list.append(f1_means)\n",
    "        stdf1_list.append(f1_stds)\n",
    "        #print('power: {}'.format(power),\n",
    "        #      'intra_dist: {}'.format(intramean),\n",
    "        #      'acc_mean: {}'.format(acc_means),\n",
    "        #      'acc_std: {}'.format(acc_stds),\n",
    "        #      'nmi_mean: {}'.format(nmi_means),\n",
    "        #      'nmi_std: {}'.format(nmi_stds),\n",
    "        #      'f1_mean: {}'.format(f1_means),\n",
    "        #      'f1_std: {}'.format(f1_stds))\n",
    "\n",
    "        if intra_list[tt] > intra_list[tt - 1] or tt > max_iter:\n",
    "            #print('bestpower: {}'.format(tt - 1))\n",
    "            t = time.time() - t\n",
    "            #print(t)\n",
    "            break\n",
    "    print(\"dataset:\", dataset, \"acc:\", str(sum(acc_list)/len(acc_list)), \" nmi:\", str(sum(nmi_list)/len(nmi_list)), \"f1-score: \", str(sum(f1_list)/len(f1_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a9409d",
   "metadata": {},
   "source": [
    "# Document Classification\n",
    "Here we also have the problem that our resources are not sufficient enough to load the matrices into memory. To battle this problem the author even said that he generated and linked the preprocessed files he used to do the experiments. However, the link to these preprocessed files is not a valid one (he linked his own google drive path, which we cannot access). Link given: https://drive.google.com/drive/u/0/my-drive\n",
    "<br>\n",
    "<br>\n",
    "The results that we could observe (2/5) are successfully achieving the results described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904afcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tabulate\n",
    "from functools import partial\n",
    "from DocumentClassification.utils import *\n",
    "from DocumentClassification.models import SGC\n",
    "\n",
    "\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "#set_seed(args.seed, args.cuda)\n",
    "\n",
    "def train_linear(model, feat_dict, weight_decay, binary=False):\n",
    "    if not binary:\n",
    "        act = partial(F.log_softmax, dim=1)\n",
    "        criterion = F.nll_loss\n",
    "    else:\n",
    "        act = torch.sigmoid\n",
    "        criterion = F.binary_cross_entropy\n",
    "    optimizer = optim.LBFGS(model.parameters())\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0\n",
    "    plateau = 0\n",
    "    start = time.perf_counter()\n",
    "    for epoch in range(3):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            #output = model(feat_dict[\"train\"].cuda()).squeeze()\n",
    "            output = model(feat_dict[\"train\"]).squeeze()\n",
    "            l2_reg = 0.5*weight_decay*(model.W.weight**2).sum()\n",
    "            #loss = criterion(act(output), label_dict[\"train\"].cuda())+l2_reg\n",
    "            loss = criterion(act(output), label_dict[\"train\"])+l2_reg\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    train_time = time.perf_counter()-start\n",
    "    val_res = eval_linear(model, feat_dict[\"val\"],\n",
    "                          label_dict[\"val\"], binary)\n",
    "    # val_res = eval_linear(model, feat_dict[\"val\"],\n",
    "    #                       label_dict[\"val\"].cuda(), binary)\n",
    "    return val_res['accuracy'], model, train_time\n",
    "\n",
    "def eval_linear(model, features, label, binary=False):\n",
    "    model.eval()\n",
    "    if not binary:\n",
    "        act = partial(F.log_softmax, dim=1)\n",
    "        criterion = F.nll_loss\n",
    "    else:\n",
    "        act = torch.sigmoid\n",
    "        criterion = F.binary_cross_entropy\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(features).squeeze()\n",
    "        loss = criterion(act(output), label)\n",
    "        if not binary: predict_class = output.max(1)[1]\n",
    "        else: predict_class = act(output).gt(0.5).float()\n",
    "        correct = torch.eq(predict_class, label).long().sum().item()\n",
    "        acc = correct/predict_class.size(0)\n",
    "\n",
    "    return {\n",
    "        'loss': loss.item(),\n",
    "        'accuracy': acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992228f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Total Time: 124.345445s, Train acc: 1.0000, Val acc: 0.9453, Test acc: 0.9557\n"
     ]
    }
   ],
   "source": [
    "dataset = 'R8'\n",
    "\n",
    "sp_adj, index_dict, label_dict = load_corpus(dataset)\n",
    "for k, v in label_dict.items():\n",
    "    if dataset == \"mr\":\n",
    "        label_dict[k] = torch.Tensor(v)\n",
    "    else:\n",
    "        label_dict[k] = torch.LongTensor(v)\n",
    "features = torch.arange(sp_adj.shape[0])\n",
    "\n",
    "adj = sparse_to_torch_sparse(sp_adj, device=\"cpu\")\n",
    "\n",
    "if dataset == \"mr\": nclass = 1\n",
    "else: nclass = label_dict[\"train\"].max().item()+1\n",
    "#if not args.preprocessed:\n",
    "adj_dense = sparse_to_torch_dense(sp_adj, device='cpu')\n",
    "feat_dict, precompute_time = sgc_precompute(adj, adj_dense, 5-1, index_dict)\n",
    "#else:\n",
    "#    # load the relased degree 2 features\n",
    "#    with open(os.path.join(\"preprocessed\",\n",
    "#        \"{}.pkl\".format(args.dataset)), \"rb\") as prep:\n",
    "#        feat_dict =  pkl.load(prep)\n",
    "#    precompute_time = 0\n",
    "\n",
    "model = SGC(nfeat=feat_dict[\"train\"].size(1),\n",
    "            nclass=nclass)\n",
    "\n",
    "model\n",
    "val_acc, best_model, train_time = train_linear(model, feat_dict, 0, dataset==\"mr\")\n",
    "test_res = eval_linear(best_model, feat_dict[\"test\"],\n",
    "                       label_dict[\"test\"], dataset==\"mr\")\n",
    "train_res = eval_linear(best_model, feat_dict[\"train\"],\n",
    "                        label_dict[\"train\"], dataset==\"mr\")\n",
    "print(\"Total Time: {:2f}s, Train acc: {:.4f}, Val acc: {:.4f}, Test acc: {:.4f}\".format(precompute_time+train_time, train_res[\"accuracy\"], val_acc, test_res[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758f0254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Total Time: 298.685317s, Train acc: 0.9993, Val acc: 0.9265, Test acc: 0.9412\n"
     ]
    }
   ],
   "source": [
    "dataset = 'R52'\n",
    "\n",
    "sp_adj, index_dict, label_dict = load_corpus(dataset)\n",
    "for k, v in label_dict.items():\n",
    "    if dataset == \"mr\":\n",
    "        label_dict[k] = torch.Tensor(v)\n",
    "    else:\n",
    "        label_dict[k] = torch.LongTensor(v)\n",
    "features = torch.arange(sp_adj.shape[0])\n",
    "\n",
    "adj = sparse_to_torch_sparse(sp_adj, device=\"cpu\")\n",
    "\n",
    "if dataset == \"mr\": nclass = 1\n",
    "else: nclass = label_dict[\"train\"].max().item()+1\n",
    "#if not args.preprocessed:\n",
    "adj_dense = sparse_to_torch_dense(sp_adj, device='cpu')\n",
    "feat_dict, precompute_time = sgc_precompute(adj, adj_dense, 5-1, index_dict)\n",
    "#else:\n",
    "#    # load the relased degree 2 features\n",
    "#    with open(os.path.join(\"preprocessed\",\n",
    "#        \"{}.pkl\".format(args.dataset)), \"rb\") as prep:\n",
    "#        feat_dict =  pkl.load(prep)\n",
    "#    precompute_time = 0\n",
    "\n",
    "model = SGC(nfeat=feat_dict[\"train\"].size(1),\n",
    "            nclass=nclass)\n",
    "\n",
    "model\n",
    "val_acc, best_model, train_time = train_linear(model, feat_dict, 0, dataset==\"mr\")\n",
    "test_res = eval_linear(best_model, feat_dict[\"test\"],\n",
    "                       label_dict[\"test\"], dataset==\"mr\")\n",
    "train_res = eval_linear(best_model, feat_dict[\"train\"],\n",
    "                        label_dict[\"train\"], dataset==\"mr\")\n",
    "print(\"Total Time: {:2f}s, Train acc: {:.4f}, Val acc: {:.4f}, Test acc: {:.4f}\".format(precompute_time+train_time, train_res[\"accuracy\"], val_acc, test_res[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77c54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "dataset = 'ohsumed'\n",
    "\n",
    "sp_adj, index_dict, label_dict = load_corpus(dataset)\n",
    "for k, v in label_dict.items():\n",
    "    if dataset == \"mr\":\n",
    "        label_dict[k] = torch.Tensor(v)\n",
    "    else:\n",
    "        label_dict[k] = torch.LongTensor(v)\n",
    "features = torch.arange(sp_adj.shape[0])\n",
    "\n",
    "adj = sparse_to_torch_sparse(sp_adj, device=\"cpu\")\n",
    "\n",
    "if dataset == \"mr\": nclass = 1\n",
    "else: nclass = label_dict[\"train\"].max().item()+1\n",
    "#if not args.preprocessed:\n",
    "adj_dense = sparse_to_torch_dense(sp_adj, device='cpu')\n",
    "feat_dict, precompute_time = sgc_precompute(adj, adj_dense, 5-1, index_dict)\n",
    "#else:\n",
    "#    # load the relased degree 2 features\n",
    "#    with open(os.path.join(\"preprocessed\",\n",
    "#        \"{}.pkl\".format(args.dataset)), \"rb\") as prep:\n",
    "#        feat_dict =  pkl.load(prep)\n",
    "#    precompute_time = 0\n",
    "\n",
    "model = SGC(nfeat=feat_dict[\"train\"].size(1),\n",
    "            nclass=nclass)\n",
    "\n",
    "model\n",
    "val_acc, best_model, train_time = train_linear(model, feat_dict, 0, dataset==\"mr\")\n",
    "test_res = eval_linear(best_model, feat_dict[\"test\"],\n",
    "                       label_dict[\"test\"], dataset==\"mr\")\n",
    "train_res = eval_linear(best_model, feat_dict[\"train\"],\n",
    "                        label_dict[\"train\"], dataset==\"mr\")\n",
    "print(\"Total Time: {:2f}s, Train acc: {:.4f}, Val acc: {:.4f}, Test acc: {:.4f}\".format(precompute_time+train_time, train_res[\"accuracy\"], val_acc, test_res[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004f59e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.22 GiB for an array with shape (31135, 31135) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cc30f2e58965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#if not args.preprocessed:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0madj_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_to_torch_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mfeat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgc_precompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_dense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myprojectdir/SSGC_new/SSGC/DocumentClassification/utils.py\u001b[0m in \u001b[0;36msparse_to_torch_dense\u001b[0;34m(sparse, device)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msparse_to_torch_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mtorch_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mtodense\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \"\"\"\n\u001b[0;32m--> 864\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 7.22 GiB for an array with shape (31135, 31135) and data type float64"
     ]
    }
   ],
   "source": [
    "dataset = 'mr'\n",
    "\n",
    "sp_adj, index_dict, label_dict = load_corpus(dataset)\n",
    "for k, v in label_dict.items():\n",
    "    if dataset == \"mr\":\n",
    "        label_dict[k] = torch.Tensor(v)\n",
    "    else:\n",
    "        label_dict[k] = torch.LongTensor(v)\n",
    "features = torch.arange(sp_adj.shape[0])\n",
    "\n",
    "adj = sparse_to_torch_sparse(sp_adj, device=\"cpu\")\n",
    "\n",
    "if dataset == \"mr\": nclass = 1\n",
    "else: nclass = label_dict[\"train\"].max().item()+1\n",
    "#if not args.preprocessed:\n",
    "adj_dense = sparse_to_torch_dense(sp_adj, device='cpu')\n",
    "feat_dict, precompute_time = sgc_precompute(adj, adj_dense, 5-1, index_dict)\n",
    "#else:\n",
    "#    # load the relased degree 2 features\n",
    "#    with open(os.path.join(\"preprocessed\",\n",
    "#        \"{}.pkl\".format(args.dataset)), \"rb\") as prep:\n",
    "#        feat_dict =  pkl.load(prep)\n",
    "#    precompute_time = 0\n",
    "\n",
    "model = SGC(nfeat=feat_dict[\"train\"].size(1),\n",
    "            nclass=nclass)\n",
    "\n",
    "model\n",
    "val_acc, best_model, train_time = train_linear(model, feat_dict, 0, dataset==\"mr\")\n",
    "test_res = eval_linear(best_model, feat_dict[\"test\"],\n",
    "                       label_dict[\"test\"], dataset==\"mr\")\n",
    "train_res = eval_linear(best_model, feat_dict[\"train\"],\n",
    "                        label_dict[\"train\"], dataset==\"mr\")\n",
    "print(\"Total Time: {:2f}s, Train acc: {:.4f}, Val acc: {:.4f}, Test acc: {:.4f}\".format(precompute_time+train_time, train_res[\"accuracy\"], val_acc, test_res[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e07781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 21.1 GiB for an array with shape (53210, 53210) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1005aa91c60a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#if not args.preprocessed:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0madj_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_to_torch_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mfeat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgc_precompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_dense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myprojectdir/SSGC_new/SSGC/DocumentClassification/utils.py\u001b[0m in \u001b[0;36msparse_to_torch_dense\u001b[0;34m(sparse, device)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msparse_to_torch_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mtorch_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mtodense\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \"\"\"\n\u001b[0;32m--> 864\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 21.1 GiB for an array with shape (53210, 53210) and data type float64"
     ]
    }
   ],
   "source": [
    "dataset = '20ng'\n",
    "\n",
    "sp_adj, index_dict, label_dict = load_corpus(dataset)\n",
    "for k, v in label_dict.items():\n",
    "    if dataset == \"mr\":\n",
    "        label_dict[k] = torch.Tensor(v)\n",
    "    else:\n",
    "        label_dict[k] = torch.LongTensor(v)\n",
    "features = torch.arange(sp_adj.shape[0])\n",
    "\n",
    "adj = sparse_to_torch_sparse(sp_adj, device=\"cpu\")\n",
    "\n",
    "if dataset == \"mr\": nclass = 1\n",
    "else: nclass = label_dict[\"train\"].max().item()+1\n",
    "#if not args.preprocessed:\n",
    "adj_dense = sparse_to_torch_dense(sp_adj, device='cpu')\n",
    "feat_dict, precompute_time = sgc_precompute(adj, adj_dense, 5-1, index_dict)\n",
    "#else:\n",
    "#    # load the relased degree 2 features\n",
    "#    with open(os.path.join(\"preprocessed\",\n",
    "#        \"{}.pkl\".format(args.dataset)), \"rb\") as prep:\n",
    "#        feat_dict =  pkl.load(prep)\n",
    "#    precompute_time = 0\n",
    "\n",
    "model = SGC(nfeat=feat_dict[\"train\"].size(1),\n",
    "            nclass=nclass)\n",
    "\n",
    "model\n",
    "val_acc, best_model, train_time = train_linear(model, feat_dict, 0, dataset==\"mr\")\n",
    "test_res = eval_linear(best_model, feat_dict[\"test\"],\n",
    "                       label_dict[\"test\"], dataset==\"mr\")\n",
    "train_res = eval_linear(best_model, feat_dict[\"train\"],\n",
    "                        label_dict[\"train\"], dataset==\"mr\")\n",
    "print(\"Total Time: {:2f}s, Train acc: {:.4f}, Val acc: {:.4f}, Test acc: {:.4f}\".format(precompute_time+train_time, train_res[\"accuracy\"], val_acc, test_res[\"accuracy\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
