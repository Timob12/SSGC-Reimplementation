{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcea5174"
      },
      "source": [
        "# Node Classification\n",
        "Sadly, we do not have the computational power to execute the code for the OGB-mag and OGB-products dataset. The memory that is needed exceeds the 12GB given by google colab. However, the code is fully functional and could be used to check the results if the resources are there."
      ],
      "id": "fcea5174"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcqapXMtbjOn",
        "outputId": "baf1aeb0-bb07-4f71-dcef-cb6a1b63b8ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SSGC-Reimplementation' already exists and is not an empty directory.\n",
            "Requirement already satisfied: scipy~=1.6.2 in /usr/local/lib/python3.7/dist-packages (1.6.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy~=1.6.2) (1.19.5)\n",
            "Requirement already satisfied: nni~=2.3 in /usr/local/lib/python3.7/dist-packages (2.6)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (3.4.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (1.6.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (1.3.0)\n",
            "Requirement already satisfied: hyperopt==0.1.2 in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (1.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (5.4.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (6.0)\n",
            "Requirement already satisfied: schema in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (0.7.5)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (3.0.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (0.4.4)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (2.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (1.19.5)\n",
            "Requirement already satisfied: responses in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (0.17.0)\n",
            "Requirement already satisfied: PythonWebHDFS in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (0.2.3)\n",
            "Requirement already satisfied: websockets>=10.1 in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (2.23.0)\n",
            "Requirement already satisfied: json-tricks>=3.15.5 in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (3.15.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nni~=2.3) (1.3.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->nni~=2.3) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->nni~=2.3) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->nni~=2.3) (1.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->nni~=2.3) (2.6.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->nni~=2.3) (4.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->nni~=2.3) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->nni~=2.3) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nni~=2.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nni~=2.3) (2018.9)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->nni~=2.3) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable->nni~=2.3) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable->nni~=2.3) (3.7.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from PythonWebHDFS->nni~=2.3) (3.17.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->nni~=2.3) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->nni~=2.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->nni~=2.3) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->nni~=2.3) (2.10)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from schema->nni~=2.3) (0.5.5)\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.7/dist-packages (1.3.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.62.3)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (0.2.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.25.11)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.6.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.6.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.5.9)\n",
            "fatal: destination path 'SSGC' already exists and is not an empty directory.\n",
            "'/content/SSGC/data' -> '/content/SSGC-Reimplementation/data/data'\n",
            "'/content/SSGC/data/ind.citeseer.allx' -> '/content/SSGC-Reimplementation/data/data/ind.citeseer.allx'\n",
            "'/content/SSGC/data/ind.citeseer.ally' -> '/content/SSGC-Reimplementation/data/data/ind.citeseer.ally'\n",
            "'/content/SSGC/data/ind.citeseer.graph' -> '/content/SSGC-Reimplementation/data/data/ind.citeseer.graph'\n",
            "'/content/SSGC/data/ind.citeseer.test.index' -> '/content/SSGC-Reimplementation/data/data/ind.citeseer.test.index'\n",
            "'/content/SSGC/data/ind.citeseer.tx' -> '/content/SSGC-Reimplementation/data/data/ind.citeseer.tx'\n",
            "'/content/SSGC/data/ind.citeseer.ty' -> '/content/SSGC-Reimplementation/data/data/ind.citeseer.ty'\n",
            "'/content/SSGC/data/ind.citeseer.x' -> '/content/SSGC-Reimplementation/data/data/ind.citeseer.x'\n",
            "'/content/SSGC/data/ind.citeseer.y' -> '/content/SSGC-Reimplementation/data/data/ind.citeseer.y'\n",
            "'/content/SSGC/data/ind.cora.allx' -> '/content/SSGC-Reimplementation/data/data/ind.cora.allx'\n",
            "'/content/SSGC/data/ind.cora.ally' -> '/content/SSGC-Reimplementation/data/data/ind.cora.ally'\n",
            "'/content/SSGC/data/ind.cora.graph' -> '/content/SSGC-Reimplementation/data/data/ind.cora.graph'\n",
            "'/content/SSGC/data/ind.cora.test.index' -> '/content/SSGC-Reimplementation/data/data/ind.cora.test.index'\n",
            "'/content/SSGC/data/ind.cora.tx' -> '/content/SSGC-Reimplementation/data/data/ind.cora.tx'\n",
            "'/content/SSGC/data/ind.cora.ty' -> '/content/SSGC-Reimplementation/data/data/ind.cora.ty'\n",
            "'/content/SSGC/data/ind.cora.x' -> '/content/SSGC-Reimplementation/data/data/ind.cora.x'\n",
            "'/content/SSGC/data/ind.cora.y' -> '/content/SSGC-Reimplementation/data/data/ind.cora.y'\n",
            "'/content/SSGC/data/ind.pubmed.allx' -> '/content/SSGC-Reimplementation/data/data/ind.pubmed.allx'\n",
            "'/content/SSGC/data/ind.pubmed.ally' -> '/content/SSGC-Reimplementation/data/data/ind.pubmed.ally'\n",
            "'/content/SSGC/data/ind.pubmed.graph' -> '/content/SSGC-Reimplementation/data/data/ind.pubmed.graph'\n",
            "'/content/SSGC/data/ind.pubmed.test.index' -> '/content/SSGC-Reimplementation/data/data/ind.pubmed.test.index'\n",
            "'/content/SSGC/data/ind.pubmed.tx' -> '/content/SSGC-Reimplementation/data/data/ind.pubmed.tx'\n",
            "'/content/SSGC/data/ind.pubmed.ty' -> '/content/SSGC-Reimplementation/data/data/ind.pubmed.ty'\n",
            "'/content/SSGC/data/ind.pubmed.x' -> '/content/SSGC-Reimplementation/data/data/ind.pubmed.x'\n",
            "'/content/SSGC/data/ind.pubmed.y' -> '/content/SSGC-Reimplementation/data/data/ind.pubmed.y'\n",
            "cp: cannot stat '/content/SSGC/model': No such file or directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=10kx3z3bjYFoeRjjg1_DZOAP39Jln0BCh\n",
            "To: /content/SSGC-Reimplementation/DocumentClassification/data.tar.gz\n",
            "100%|██████████| 348M/348M [00:01<00:00, 221MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SSGC-Reimplementation\n"
          ]
        }
      ],
      "source": [
        "#@title Setup For Google Colab\n",
        "\n",
        "REPO_URL = 'https://github.com/Timob12/SSGC-Reimplementation.git'\n",
        "\n",
        "!git clone \"{REPO_URL}\"\n",
        "\n",
        "# install dependencies\n",
        "!pip install scipy~=1.6.2\n",
        "!pip install nni~=2.3\n",
        "!pip install ogb\n",
        "\n",
        "import torch\n",
        "\n",
        "TORCH = torch.__version__.split('+')[0]\n",
        "CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "MAIN_DIR = '/content/SSGC-Reimplementation'\n",
        "\n",
        "# pull the node classification data for using the models from another repository\n",
        "DATA_REPO_URL = 'https://github.com/allenhaozhu/SSGC.git'\n",
        "\n",
        "!git clone \"{DATA_REPO_URL}\"\n",
        "\n",
        "%cp -rv /content/SSGC/data /content/SSGC-Reimplementation/data\n",
        "%cp -rv /content/SSGC/model /content/SSGC-Reimplementation\n",
        "\n",
        "# download the document classification data\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "DOC_CLASSIFICATION_DATA_URL = 'https://drive.google.com/u/0/uc?id=10kx3z3bjYFoeRjjg1_DZOAP39Jln0BCh'\n",
        "DOC_CLASSIFICATION_DIR = os.path.join(MAIN_DIR, 'DocumentClassification') \n",
        "DATA_DEST = os.path.join(DOC_CLASSIFICATION_DIR, 'data.tar.gz')\n",
        "\n",
        "gdown.download(DOC_CLASSIFICATION_DATA_URL, output=DATA_DEST, quiet=False)\n",
        "\n",
        "!tar -xf \"{DATA_DEST}\" -C \"{DOC_CLASSIFICATION_DIR}\"\n",
        "\n",
        "%cd \"{MAIN_DIR}\""
      ],
      "id": "IcqapXMtbjOn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f8e1b01"
      },
      "source": [
        "## Small/Medium Dataset"
      ],
      "id": "3f8e1b01"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f280ef6d",
        "outputId": "b6576f0f-5d8d-4956-c73f-c4b56448366b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset name: cora\n",
            "Validation Accuracy: 0.8000 Test Accuracy: 0.8110\n",
            "Pre-compute time: 0.4643s, train time: 1.3363s, total: 1.8006s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/SSGC-Reimplementation/normalization.py:34: RuntimeWarning: divide by zero encountered in power\n",
            "  r_inv = np.power(rowsum, -1).flatten()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset name: citeseer\n",
            "Validation Accuracy: 0.7080 Test Accuracy: 0.6987\n",
            "Pre-compute time: 1.6117s, train time: 2.6039s, total: 4.2156s\n",
            "dataset name: pubmed\n",
            "Validation Accuracy: 0.8020 Test Accuracy: 0.7960\n",
            "Pre-compute time: 1.7902s, train time: 1.2669s, total: 3.0571s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from utils import load_citation, sgc_precompute, set_seed\n",
        "from models import get_model\n",
        "from metrics import accuracy\n",
        "import pickle as pkl\n",
        "#from args import get_citation_args\n",
        "from time import perf_counter\n",
        "\n",
        "# Arguments\n",
        "\n",
        "# setting random seeds\n",
        "#set_seed(seed, cuda)\n",
        "\n",
        "#tuned = False\n",
        "#arg_model = \"SGC\"\n",
        "#dataset = \"cora\"\n",
        "weight_decay = 5e-6\n",
        "#seed = 42\n",
        "#cuda = False\n",
        "normalization = 'NormAdj'\n",
        "hidden = 0\n",
        "dropout = 0\n",
        "degree = 16\n",
        "alpha = 0.05\n",
        "epochs = 1000\n",
        "lr = 0.02\n",
        "\n",
        "def setup(dataset, normalization, degree, alpha):\n",
        "    adj, features, labels, idx_train, idx_val, idx_test = load_citation(dataset, normalization, False)\n",
        "\n",
        "    model = get_model(\"SGC\", features.size(1), labels.max().item()+1, 0, 0, False)\n",
        "\n",
        "    features, precompute_time = sgc_precompute(features, adj, degree, alpha)\n",
        "    \n",
        "    return precompute_time, features, labels, idx_train, idx_val, idx_test, model\n",
        "\n",
        "def train_regression(model,\n",
        "                     train_features, train_labels,\n",
        "                     val_features, val_labels,\n",
        "                     epochs=epochs, weight_decay=weight_decay,\n",
        "                     lr=lr, dropout=dropout):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
        "                           weight_decay=weight_decay)\n",
        "    t = perf_counter()\n",
        "    best_acc_val = torch.zeros((1))\n",
        "    best_model = None\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(train_features)\n",
        "        loss_train = F.cross_entropy(output, train_labels)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            output = model(val_features)\n",
        "            acc_val = accuracy(output, val_labels)\n",
        "            if best_acc_val < acc_val:\n",
        "                best_acc_val = acc_val\n",
        "                best_model = model\n",
        "\n",
        "    train_time = perf_counter()-t\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     model.eval()\n",
        "    #     output = model(val_features)\n",
        "    #     acc_val = accuracy(output, val_labels)\n",
        "\n",
        "    return best_model, best_acc_val, train_time\n",
        "\n",
        "def test_regression(model, test_features, test_labels):\n",
        "    model.eval()\n",
        "    return accuracy(model(test_features), test_labels)\n",
        "\n",
        "\n",
        "datasets = [\"cora\", \"citeseer\", \"pubmed\"]\n",
        "for ds in datasets:\n",
        "    acc_test = 0\n",
        "    total_precompute_time = 0\n",
        "    total_train_time = 0\n",
        "    for _ in range(10):\n",
        "        precompute_time, features, labels, idx_train, idx_val, idx_test, model = setup(ds, normalization, degree, alpha)\n",
        "\n",
        "        model, acc_val, train_time = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
        "                    epochs, weight_decay, lr, dropout)\n",
        "        acc_test += test_regression(model, features[idx_test], labels[idx_test])\n",
        "        total_precompute_time += precompute_time\n",
        "        total_train_time += train_time\n",
        "    acc_test /= 10\n",
        "    total_precompute_time /= 10\n",
        "    total_train_time /= 10\n",
        "    print(\"dataset name:\", ds)\n",
        "    print(\"Validation Accuracy: {:.4f} Test Accuracy: {:.4f}\".format(acc_val, acc_test))\n",
        "    print(\"Pre-compute time: {:.4f}s, train time: {:.4f}s, total: {:.4f}s\".format(total_precompute_time, total_train_time, total_precompute_time+total_train_time))"
      ],
      "id": "f280ef6d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "217cb5d4"
      },
      "source": [
        "## Alpha and K Comparison on Cora, Citeseer, and Pubmed"
      ],
      "id": "217cb5d4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29052d02",
        "outputId": "62902d76-7adb-43af-c3cc-ec1d9b905893",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset: cora , average accuracy: tensor(0.8030, dtype=torch.float64) alpha: 0.0\n",
            "dataset: cora , average accuracy: tensor(0.8110, dtype=torch.float64) alpha: 0.05\n",
            "dataset: cora , average accuracy: tensor(0.8130, dtype=torch.float64) alpha: 0.1\n",
            "dataset: cora , average accuracy: tensor(0.8180, dtype=torch.float64) alpha: 0.15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/SSGC-Reimplementation/normalization.py:34: RuntimeWarning: divide by zero encountered in power\n",
            "  r_inv = np.power(rowsum, -1).flatten()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset: citeseer , average accuracy: tensor(0.6990, dtype=torch.float64) alpha: 0.0\n",
            "dataset: citeseer , average accuracy: tensor(0.6990, dtype=torch.float64) alpha: 0.05\n",
            "dataset: citeseer , average accuracy: tensor(0.6960, dtype=torch.float64) alpha: 0.1\n",
            "dataset: citeseer , average accuracy: tensor(0.6960, dtype=torch.float64) alpha: 0.15\n",
            "dataset: pubmed , average accuracy: tensor(0.7900, dtype=torch.float64) alpha: 0.0\n",
            "dataset: pubmed , average accuracy: tensor(0.7960, dtype=torch.float64) alpha: 0.05\n",
            "dataset: pubmed , average accuracy: tensor(0.7950, dtype=torch.float64) alpha: 0.1\n",
            "dataset: pubmed , average accuracy: tensor(0.7950, dtype=torch.float64) alpha: 0.15\n",
            "dataset: cora , average accuracy: tensor(0.7790, dtype=torch.float64) K: 2\n",
            "dataset: cora , average accuracy: tensor(0.8040, dtype=torch.float64) K: 4\n",
            "dataset: cora , average accuracy: tensor(0.8080, dtype=torch.float64) K: 8\n",
            "dataset: cora , average accuracy: tensor(0.8110, dtype=torch.float64) K: 16\n",
            "dataset: cora , average accuracy: tensor(0.8130, dtype=torch.float64) K: 32\n",
            "dataset: cora , average accuracy: tensor(0.7980, dtype=torch.float64) K: 64\n",
            "dataset: citeseer , average accuracy: tensor(0.6760, dtype=torch.float64) K: 2\n",
            "dataset: citeseer , average accuracy: tensor(0.6810, dtype=torch.float64) K: 4\n",
            "dataset: citeseer , average accuracy: tensor(0.6910, dtype=torch.float64) K: 8\n",
            "dataset: citeseer , average accuracy: tensor(0.6980, dtype=torch.float64) K: 16\n",
            "dataset: citeseer , average accuracy: tensor(0.7000, dtype=torch.float64) K: 32\n",
            "dataset: citeseer , average accuracy: tensor(0.6930, dtype=torch.float64) K: 64\n",
            "dataset: pubmed , average accuracy: tensor(0.7720, dtype=torch.float64) K: 2\n",
            "dataset: pubmed , average accuracy: tensor(0.7820, dtype=torch.float64) K: 4\n",
            "dataset: pubmed , average accuracy: tensor(0.7900, dtype=torch.float64) K: 8\n",
            "dataset: pubmed , average accuracy: tensor(0.7970, dtype=torch.float64) K: 16\n",
            "dataset: pubmed , average accuracy: tensor(0.7840, dtype=torch.float64) K: 32\n",
            "dataset: pubmed , average accuracy: tensor(0.7640, dtype=torch.float64) K: 64\n"
          ]
        }
      ],
      "source": [
        "datasets = [\"cora\", \"citeseer\", \"pubmed\"]\n",
        "alpha_ = [0.0, 0.05, 0.1, 0.15]\n",
        "K_ = [2, 4, 8, 16, 32, 64]\n",
        "alpha_standard = 0.05\n",
        "K_standard = 16\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "alpha_results = []\n",
        "k_results = []\n",
        "\n",
        "for dataset in datasets:\n",
        "    K = K_standard\n",
        "    alpha_to_acc = {}\n",
        "    for alpha in alpha_:\n",
        "        precompute_time, features, labels, idx_train, idx_val, idx_test, model = setup(dataset, normalization, K_standard, alpha)\n",
        "\n",
        "        model, acc_val, train_time = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
        "                    epochs, weight_decay, lr, dropout)\n",
        "        acc = test_regression(model, features[idx_test], labels[idx_test])\n",
        "\n",
        "        print(\"dataset:\", dataset, \", average accuracy:\", acc, \"alpha:\", alpha)\n",
        "\n",
        "        alpha_to_acc[alpha] = acc.item()\n",
        "    \n",
        "    row = {'Dataset': dataset}\n",
        "    row.update(alpha_to_acc)\n",
        "    alpha_results.append(row)\n",
        "\n",
        "for dataset in datasets:\n",
        "    alpha = alpha_standard\n",
        "    k_to_acc = {}\n",
        "    for K in K_:\n",
        "        precompute_time, features, labels, idx_train, idx_val, idx_test, model = setup(dataset, normalization, K, alpha_standard)\n",
        "\n",
        "        model, acc_val, train_time = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
        "                    epochs, weight_decay, lr, dropout)\n",
        "        acc = test_regression(model, features[idx_test], labels[idx_test])\n",
        "\n",
        "        print(\"dataset:\", dataset, \", average accuracy:\", acc, \"K:\", K)\n",
        "\n",
        "        k_to_acc[K] = acc.item()\n",
        "\n",
        "    row = {'Dataset': dataset}\n",
        "    row.update(k_to_acc)\n",
        "    k_results.append(row)"
      ],
      "id": "29052d02"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "2buuCoa2ej9l",
        "outputId": "8c6cae9e-4ec2-4e18-97ad-7b62dde04779"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8e8d6b1c-8dce-4b42-a4f7-01f092818ea5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.05</th>\n",
              "      <th>0.1</th>\n",
              "      <th>0.15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cora</td>\n",
              "      <td>0.803</td>\n",
              "      <td>0.811</td>\n",
              "      <td>0.813</td>\n",
              "      <td>0.818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>citeseer</td>\n",
              "      <td>0.699</td>\n",
              "      <td>0.699</td>\n",
              "      <td>0.696</td>\n",
              "      <td>0.696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pubmed</td>\n",
              "      <td>0.790</td>\n",
              "      <td>0.796</td>\n",
              "      <td>0.795</td>\n",
              "      <td>0.795</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e8d6b1c-8dce-4b42-a4f7-01f092818ea5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e8d6b1c-8dce-4b42-a4f7-01f092818ea5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e8d6b1c-8dce-4b42-a4f7-01f092818ea5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Dataset    0.0   0.05    0.1   0.15\n",
              "0      cora  0.803  0.811  0.813  0.818\n",
              "1  citeseer  0.699  0.699  0.696  0.696\n",
              "2    pubmed  0.790  0.796  0.795  0.795"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alpha_df = pd.DataFrame(alpha_results)\n",
        "alpha_df.to_csv('alpha_results.csv', index=False)\n",
        "alpha_df"
      ],
      "id": "2buuCoa2ej9l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "EPAbWrPHekbB",
        "outputId": "18efcf21-9576-472d-d252-9a6e98cb0f23"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c435803a-8627-4ebe-997f-e7f2d35a88b8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th>2</th>\n",
              "      <th>4</th>\n",
              "      <th>8</th>\n",
              "      <th>16</th>\n",
              "      <th>32</th>\n",
              "      <th>64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cora</td>\n",
              "      <td>0.779</td>\n",
              "      <td>0.804</td>\n",
              "      <td>0.808</td>\n",
              "      <td>0.811</td>\n",
              "      <td>0.813</td>\n",
              "      <td>0.798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>citeseer</td>\n",
              "      <td>0.676</td>\n",
              "      <td>0.681</td>\n",
              "      <td>0.691</td>\n",
              "      <td>0.698</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pubmed</td>\n",
              "      <td>0.772</td>\n",
              "      <td>0.782</td>\n",
              "      <td>0.790</td>\n",
              "      <td>0.797</td>\n",
              "      <td>0.784</td>\n",
              "      <td>0.764</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c435803a-8627-4ebe-997f-e7f2d35a88b8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c435803a-8627-4ebe-997f-e7f2d35a88b8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c435803a-8627-4ebe-997f-e7f2d35a88b8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Dataset      2      4      8     16     32     64\n",
              "0      cora  0.779  0.804  0.808  0.811  0.813  0.798\n",
              "1  citeseer  0.676  0.681  0.691  0.698  0.700  0.693\n",
              "2    pubmed  0.772  0.782  0.790  0.797  0.784  0.764"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k_df = pd.DataFrame(k_results)\n",
        "k_df.to_csv('k_results.csv', index=False)\n",
        "k_df"
      ],
      "id": "EPAbWrPHekbB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e748cd3"
      },
      "source": [
        "# Large OGB-ARXIV Dataset"
      ],
      "id": "0e748cd3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "030020d4"
      },
      "source": [
        "## Embedding"
      ],
      "id": "030020d4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1a3f22e",
        "outputId": "7da0652b-cbcf-46ce-90d1-411746a7e4fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:02<00:00, 30.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 6700.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1760.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.utils import dropout_adj\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "from typing import Optional\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "#from logger import Logger\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from time import perf_counter\n",
        "\n",
        "\n",
        "\n",
        "class SGConv(MessagePassing):\n",
        "    r\"\"\"The simple graph convolutional operator from the `\"Simplifying Graph\n",
        "    Convolutional Networks\" <https://arxiv.org/abs/1902.07153>`_ paper\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{X}^{\\prime} = {\\left(\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
        "        \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X} \\mathbf{\\Theta},\n",
        "\n",
        "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
        "    adjacency matrix with inserted self-loops and\n",
        "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        K (int, optional): Number of hops :math:`K`. (default: :obj:`1`)\n",
        "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
        "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
        "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X}` on\n",
        "            first execution, and will use the cached version for further\n",
        "            executions.\n",
        "            This parameter should only be set to :obj:`True` in transductive\n",
        "            learning scenarios. (default: :obj:`False`)\n",
        "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
        "            self-loops to the input graph. (default: :obj:`True`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "\n",
        "    _cached_x: Optional[Tensor]\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, K: int = 1,\n",
        "                 cached: bool = False, add_self_loops: bool = True,\n",
        "                 bias: bool = True, dropout: float = 0.05, **kwargs):\n",
        "        super(SGConv, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.K = K\n",
        "        self.cached = cached\n",
        "        self.add_self_loops = add_self_loops\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self._cached_x = None\n",
        "\n",
        "        self.lin = Linear(in_channels, out_channels, bias=bias)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        #self.lin.reset_parameters()\n",
        "        self._cached_x = None\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj,\n",
        "                edge_weight: OptTensor = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        cache = self._cached_x\n",
        "        if cache is None:\n",
        "            if isinstance(edge_index, Tensor):\n",
        "                edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
        "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                    self.add_self_loops, dtype=x.dtype)\n",
        "            elif isinstance(edge_index, SparseTensor):\n",
        "                edge_index = gcn_norm(  # yapf: disable\n",
        "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                    self.add_self_loops, dtype=x.dtype)\n",
        "\n",
        "            x_set = []\n",
        "            alpha = 0.05\n",
        "            output = alpha * x\n",
        "            #temp_edge_index, edge_weight = dropout_adj(edge_index, 0.5)\n",
        "            for k in range(self.K):\n",
        "                x = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
        "                                   size=None)\n",
        "                # x_set.append(x)\n",
        "                output = output + (1. / self.K) * x\n",
        "            # x = torch.stack(x_set,2)\n",
        "            # alpha = 0.05\n",
        "            # x = (1-alpha)*torch.mean(x,2).squeeze() + alpha*x_ori\n",
        "            x = output\n",
        "            if self.cached:\n",
        "                self._cached_x = x\n",
        "        else:\n",
        "            x = cache\n",
        "\n",
        "        return x#self.lin(x)\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
        "        return edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {}, K={})'.format(self.__class__.__name__,\n",
        "                                         self.in_channels, self.out_channels,\n",
        "                                         self.K)\n",
        "\n",
        "\n",
        "class SGC(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden, out_channels, num_layers, dropout):\n",
        "        super(SGC, self).__init__()\n",
        "        self.conv1 = SGConv(\n",
        "            in_channels, hidden, K=num_layers, cached=True)\n",
        "        self.lin = torch.nn.Linear(hidden, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        self.lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x, edge_index = data.x, data.edge_index\n",
        "        #x = self.conv1(x, edge_index)\n",
        "        # x = F.relu(x)\n",
        "        # x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv1(x, edge_index)#F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class SAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(SAGE, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "def train(model, data, train_idx, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.adj_t)[train_idx]\n",
        "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "    model.eval()\n",
        "\n",
        "    out = model(data.x, data.adj_t)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc\n",
        "\n",
        "precompute_time = 0.0\n",
        "def main():\n",
        "\n",
        "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    dataset = PygNodePropPredDataset(name='ogbn-arxiv',\n",
        "                                     transform=T.ToSparseTensor())\n",
        "\n",
        "    data = dataset[0]\n",
        "    data.adj_t = data.adj_t.to_symmetric()\n",
        "    data = data.to(device)\n",
        "\n",
        "    split_idx = dataset.get_idx_split()\n",
        "    train_idx = split_idx['train'].to(device)\n",
        "    \n",
        "    precompute_time = perf_counter()\n",
        "    model = SGC(data.num_features, 256, dataset.num_classes, 16, 0.5).to(device)\n",
        "    \n",
        "    features = model(data.x, data.adj_t)\n",
        "    torch.save(features,'embedding_arxiv.pt')\n",
        "    precompute_time = perf_counter() - precompute_time\n",
        "\n",
        "    return precompute_time\n",
        "precompute_time = main()"
      ],
      "id": "b1a3f22e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f0d5cb5"
      },
      "source": [
        "## SSGC"
      ],
      "id": "0f0d5cb5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cab2170",
        "outputId": "d0884f41-198d-4008-85c9-a7636a80d0c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.6746 Test Accuracy: 0.6588\n",
            "Pre-compute time: 0.8215s, train time: 9.2973s, total: 10.1188s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from utils import load_citation, sgc_precompute, set_seed\n",
        "from models import get_model\n",
        "from metrics import accuracy\n",
        "import pickle as pkl\n",
        "#from args_cora import get_citation_args\n",
        "from time import perf_counter\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "import torch_geometric.transforms as T\n",
        "from normalization import fetch_normalization, row_normalize\n",
        "from utils import *\n",
        "\n",
        "# Arguments\n",
        "#args = get_citation_args()\n",
        "\n",
        "#if args.tuned:\n",
        "#    if args.model == \"SGC\":\n",
        "#        with open(\"{}-tuning/{}.txt\".format(args.model, args.dataset), 'rb') as f:\n",
        "#            args.weight_decay = pkl.load(f)['weight_decay']\n",
        "#            print(\"using tuned weight decay: {}\".format(args.weight_decay))\n",
        "#    else:\n",
        "#        raise NotImplemented\n",
        "\n",
        "# setting random seeds\n",
        "#set_seed(args.seed, args.cuda)\n",
        "\n",
        "dataset = PygNodePropPredDataset(name='ogbn-arxiv', transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "idx_train = split_idx['train']\n",
        "idx_val = split_idx['valid']\n",
        "idx_test = split_idx['test']\n",
        "labels = torch.LongTensor([y[0] for y in data.y.tolist()])\n",
        "\n",
        "features = torch.load('embedding_arxiv.pt', map_location='cpu')\n",
        "\n",
        "model = get_model(\"SGC\", features.shape[1], labels.max().item()+1, 0, 0, 0)\n",
        "\n",
        "def train_regression(model,\n",
        "                     train_features, train_labels,\n",
        "                     val_features, val_labels,\n",
        "                     epochs=100, weight_decay=1e-05,\n",
        "                     lr=0.2, dropout=0):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
        "                           weight_decay=weight_decay)\n",
        "    t = perf_counter()\n",
        "    best_acc_val = torch.zeros((1))\n",
        "    best_loss_val = 100.\n",
        "    best_model = None\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(train_features)\n",
        "        loss_train = F.cross_entropy(output, train_labels)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            output = model(val_features)\n",
        "            acc_val = accuracy(output, val_labels)\n",
        "            loss_val = F.cross_entropy(output, val_labels)\n",
        "            if best_acc_val < acc_val:\n",
        "                 best_acc_val = acc_val\n",
        "            #     best_model = model\n",
        "            if best_loss_val > loss_val:\n",
        "                best_loss_val = loss_val\n",
        "                best_model = model\n",
        "\n",
        "    train_time = perf_counter()-t\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     model.eval()\n",
        "    #     output = model(val_features)\n",
        "    #     acc_val = accuracy(output, val_labels)\n",
        "\n",
        "    return best_model, best_acc_val, train_time\n",
        "\n",
        "def test_regression(model, test_features, test_labels):\n",
        "    model.eval()\n",
        "    return accuracy(model(test_features), test_labels)\n",
        "\n",
        "acc_test = 0\n",
        "acc_val = 0\n",
        "train_time = 0\n",
        "for _ in range(10):\n",
        "    model, acc_val_buf, train_time_buf = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
        "                        100, 1e-05, 0.02, 0)\n",
        "    acc_test += test_regression(model, features[idx_test], labels[idx_test])\n",
        "    acc_val += acc_val_buf\n",
        "    train_time += train_time_buf\n",
        "acc_test /= 10\n",
        "acc_val /= 10\n",
        "train_time /= 10\n",
        "\n",
        "print(\"Validation Accuracy: {:.4f} Test Accuracy: {:.4f}\".format(acc_val, acc_test))\n",
        "print(\"Pre-compute time: {:.4f}s, train time: {:.4f}s, total: {:.4f}s\".format(precompute_time, train_time, precompute_time+train_time))"
      ],
      "id": "9cab2170"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "948bd125"
      },
      "source": [
        "## SSGC + MLP"
      ],
      "id": "948bd125"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96755de8",
        "outputId": "517911dc-7eeb-4739-9de6-eeb8bcb46a02",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 01, Epoch: 10, Loss: 1.7556, Train: 40.45%, Valid: 40.53%, Test: 43.80%\n",
            "Run: 01, Epoch: 20, Loss: 1.4259, Train: 58.60%, Valid: 58.05%, Test: 59.07%\n",
            "Run: 01, Epoch: 30, Loss: 1.2920, Train: 64.55%, Valid: 64.40%, Test: 64.70%\n",
            "Run: 01, Epoch: 40, Loss: 1.2113, Train: 67.49%, Valid: 67.85%, Test: 67.81%\n",
            "Run: 01, Epoch: 50, Loss: 1.1575, Train: 69.26%, Valid: 69.05%, Test: 68.21%\n",
            "Run: 01, Epoch: 60, Loss: 1.1116, Train: 70.23%, Valid: 69.73%, Test: 68.89%\n",
            "Run: 01, Epoch: 70, Loss: 1.0832, Train: 70.83%, Valid: 70.38%, Test: 69.60%\n",
            "Run: 01, Epoch: 80, Loss: 1.0592, Train: 71.49%, Valid: 70.90%, Test: 69.84%\n",
            "Run: 01, Epoch: 90, Loss: 1.0393, Train: 71.45%, Valid: 70.76%, Test: 70.36%\n",
            "Run: 02, Epoch: 10, Loss: 1.7567, Train: 43.22%, Valid: 39.08%, Test: 43.03%\n",
            "Run: 02, Epoch: 20, Loss: 1.4289, Train: 59.84%, Valid: 59.20%, Test: 60.99%\n",
            "Run: 02, Epoch: 30, Loss: 1.2996, Train: 65.34%, Valid: 65.85%, Test: 65.78%\n",
            "Run: 02, Epoch: 40, Loss: 1.2188, Train: 67.56%, Valid: 67.62%, Test: 66.99%\n",
            "Run: 02, Epoch: 50, Loss: 1.1600, Train: 69.09%, Valid: 68.82%, Test: 67.72%\n",
            "Run: 02, Epoch: 60, Loss: 1.1184, Train: 70.16%, Valid: 69.92%, Test: 69.17%\n",
            "Run: 02, Epoch: 70, Loss: 1.0860, Train: 70.67%, Valid: 69.60%, Test: 67.84%\n",
            "Run: 02, Epoch: 80, Loss: 1.0627, Train: 71.32%, Valid: 70.78%, Test: 70.15%\n",
            "Run: 02, Epoch: 90, Loss: 1.0439, Train: 71.59%, Valid: 70.64%, Test: 69.35%\n",
            "Run: 03, Epoch: 10, Loss: 1.8014, Train: 43.89%, Valid: 46.43%, Test: 49.05%\n",
            "Run: 03, Epoch: 20, Loss: 1.4555, Train: 57.68%, Valid: 56.60%, Test: 58.52%\n",
            "Run: 03, Epoch: 30, Loss: 1.3150, Train: 65.41%, Valid: 65.62%, Test: 64.67%\n",
            "Run: 03, Epoch: 40, Loss: 1.2292, Train: 67.52%, Valid: 67.02%, Test: 65.61%\n",
            "Run: 03, Epoch: 50, Loss: 1.1676, Train: 69.10%, Valid: 68.33%, Test: 67.52%\n",
            "Run: 03, Epoch: 60, Loss: 1.1285, Train: 69.99%, Valid: 69.02%, Test: 67.59%\n",
            "Run: 03, Epoch: 70, Loss: 1.0979, Train: 70.57%, Valid: 69.19%, Test: 68.28%\n",
            "Run: 03, Epoch: 80, Loss: 1.0690, Train: 71.32%, Valid: 70.24%, Test: 69.01%\n",
            "Run: 03, Epoch: 90, Loss: 1.0466, Train: 71.36%, Valid: 69.61%, Test: 67.64%\n",
            "Run: 04, Epoch: 10, Loss: 1.7955, Train: 41.87%, Valid: 32.25%, Test: 34.49%\n",
            "Run: 04, Epoch: 20, Loss: 1.4437, Train: 56.60%, Valid: 53.46%, Test: 55.29%\n",
            "Run: 04, Epoch: 30, Loss: 1.3077, Train: 65.02%, Valid: 65.40%, Test: 65.62%\n",
            "Run: 04, Epoch: 40, Loss: 1.2244, Train: 67.55%, Valid: 67.51%, Test: 67.17%\n",
            "Run: 04, Epoch: 50, Loss: 1.1637, Train: 69.34%, Valid: 68.72%, Test: 67.41%\n",
            "Run: 04, Epoch: 60, Loss: 1.1196, Train: 70.14%, Valid: 69.56%, Test: 68.38%\n",
            "Run: 04, Epoch: 70, Loss: 1.0868, Train: 70.81%, Valid: 70.36%, Test: 69.92%\n",
            "Run: 04, Epoch: 80, Loss: 1.0622, Train: 71.27%, Valid: 70.76%, Test: 70.10%\n",
            "Run: 04, Epoch: 90, Loss: 1.0440, Train: 70.86%, Valid: 68.64%, Test: 66.44%\n",
            "Run: 05, Epoch: 10, Loss: 1.7800, Train: 42.11%, Valid: 34.50%, Test: 37.75%\n",
            "Run: 05, Epoch: 20, Loss: 1.4389, Train: 58.99%, Valid: 58.16%, Test: 59.31%\n",
            "Run: 05, Epoch: 30, Loss: 1.2956, Train: 64.70%, Valid: 65.34%, Test: 65.86%\n",
            "Run: 05, Epoch: 40, Loss: 1.2152, Train: 67.65%, Valid: 67.75%, Test: 68.02%\n",
            "Run: 05, Epoch: 50, Loss: 1.1548, Train: 69.15%, Valid: 69.01%, Test: 68.64%\n",
            "Run: 05, Epoch: 60, Loss: 1.1149, Train: 70.17%, Valid: 69.80%, Test: 69.47%\n",
            "Run: 05, Epoch: 70, Loss: 1.0841, Train: 70.93%, Valid: 70.18%, Test: 68.75%\n",
            "Run: 05, Epoch: 80, Loss: 1.0625, Train: 71.36%, Valid: 70.46%, Test: 69.79%\n",
            "Run: 05, Epoch: 90, Loss: 1.0392, Train: 71.73%, Valid: 70.50%, Test: 69.29%\n",
            "Run: 06, Epoch: 10, Loss: 1.8512, Train: 41.53%, Valid: 40.71%, Test: 44.29%\n",
            "Run: 06, Epoch: 20, Loss: 1.4766, Train: 58.20%, Valid: 57.41%, Test: 59.00%\n",
            "Run: 06, Epoch: 30, Loss: 1.3237, Train: 65.09%, Valid: 65.79%, Test: 65.94%\n",
            "Run: 06, Epoch: 40, Loss: 1.2338, Train: 67.37%, Valid: 67.60%, Test: 67.58%\n",
            "Run: 06, Epoch: 50, Loss: 1.1755, Train: 69.10%, Valid: 68.97%, Test: 68.57%\n",
            "Run: 06, Epoch: 60, Loss: 1.1259, Train: 69.99%, Valid: 69.82%, Test: 69.19%\n",
            "Run: 06, Epoch: 70, Loss: 1.0925, Train: 70.58%, Valid: 70.17%, Test: 70.01%\n",
            "Run: 06, Epoch: 80, Loss: 1.0734, Train: 71.24%, Valid: 70.41%, Test: 69.02%\n",
            "Run: 06, Epoch: 90, Loss: 1.0504, Train: 71.70%, Valid: 70.86%, Test: 69.87%\n",
            "Run: 07, Epoch: 10, Loss: 1.8092, Train: 48.13%, Valid: 50.97%, Test: 53.04%\n",
            "Run: 07, Epoch: 20, Loss: 1.4513, Train: 57.76%, Valid: 55.84%, Test: 57.67%\n",
            "Run: 07, Epoch: 30, Loss: 1.3099, Train: 65.08%, Valid: 65.18%, Test: 65.57%\n",
            "Run: 07, Epoch: 40, Loss: 1.2208, Train: 67.57%, Valid: 67.57%, Test: 67.25%\n",
            "Run: 07, Epoch: 50, Loss: 1.1645, Train: 69.23%, Valid: 69.09%, Test: 68.53%\n",
            "Run: 07, Epoch: 60, Loss: 1.1206, Train: 70.17%, Valid: 69.57%, Test: 68.68%\n",
            "Run: 07, Epoch: 70, Loss: 1.0867, Train: 70.72%, Valid: 70.27%, Test: 69.77%\n",
            "Run: 07, Epoch: 80, Loss: 1.0659, Train: 71.28%, Valid: 70.84%, Test: 69.66%\n",
            "Run: 07, Epoch: 90, Loss: 1.0429, Train: 71.88%, Valid: 70.95%, Test: 70.20%\n",
            "Run: 08, Epoch: 10, Loss: 1.7721, Train: 47.35%, Valid: 40.56%, Test: 43.69%\n",
            "Run: 08, Epoch: 20, Loss: 1.4235, Train: 59.16%, Valid: 58.05%, Test: 59.55%\n",
            "Run: 08, Epoch: 30, Loss: 1.2899, Train: 64.74%, Valid: 64.28%, Test: 64.27%\n",
            "Run: 08, Epoch: 40, Loss: 1.2124, Train: 67.69%, Valid: 67.42%, Test: 66.67%\n",
            "Run: 08, Epoch: 50, Loss: 1.1591, Train: 69.24%, Valid: 69.04%, Test: 68.10%\n",
            "Run: 08, Epoch: 60, Loss: 1.1180, Train: 70.19%, Valid: 69.83%, Test: 68.87%\n",
            "Run: 08, Epoch: 70, Loss: 1.0912, Train: 70.62%, Valid: 70.47%, Test: 70.27%\n",
            "Run: 08, Epoch: 80, Loss: 1.0659, Train: 71.29%, Valid: 70.72%, Test: 69.56%\n",
            "Run: 08, Epoch: 90, Loss: 1.0508, Train: 71.54%, Valid: 71.17%, Test: 70.85%\n",
            "Run: 09, Epoch: 10, Loss: 1.7462, Train: 46.82%, Valid: 44.57%, Test: 47.11%\n",
            "Run: 09, Epoch: 20, Loss: 1.4175, Train: 59.76%, Valid: 58.72%, Test: 60.18%\n",
            "Run: 09, Epoch: 30, Loss: 1.2857, Train: 65.09%, Valid: 65.12%, Test: 65.10%\n",
            "Run: 09, Epoch: 40, Loss: 1.2059, Train: 67.61%, Valid: 66.86%, Test: 66.11%\n",
            "Run: 09, Epoch: 50, Loss: 1.1476, Train: 69.28%, Valid: 68.95%, Test: 68.61%\n",
            "Run: 09, Epoch: 60, Loss: 1.1081, Train: 70.49%, Valid: 69.99%, Test: 69.32%\n",
            "Run: 09, Epoch: 70, Loss: 1.0812, Train: 70.97%, Valid: 70.39%, Test: 69.17%\n",
            "Run: 09, Epoch: 80, Loss: 1.0546, Train: 71.74%, Valid: 70.76%, Test: 69.41%\n",
            "Run: 09, Epoch: 90, Loss: 1.0339, Train: 71.77%, Valid: 71.18%, Test: 70.58%\n",
            "Run: 10, Epoch: 10, Loss: 1.7261, Train: 43.59%, Valid: 39.95%, Test: 43.44%\n",
            "Run: 10, Epoch: 20, Loss: 1.4031, Train: 56.08%, Valid: 53.32%, Test: 55.49%\n",
            "Run: 10, Epoch: 30, Loss: 1.2796, Train: 64.97%, Valid: 65.12%, Test: 66.09%\n",
            "Run: 10, Epoch: 40, Loss: 1.1995, Train: 67.74%, Valid: 68.00%, Test: 68.16%\n",
            "Run: 10, Epoch: 50, Loss: 1.1459, Train: 69.26%, Valid: 68.79%, Test: 68.02%\n",
            "Run: 10, Epoch: 60, Loss: 1.1127, Train: 70.35%, Valid: 69.89%, Test: 69.46%\n",
            "Run: 10, Epoch: 70, Loss: 1.0793, Train: 71.10%, Valid: 70.63%, Test: 69.79%\n",
            "Run: 10, Epoch: 80, Loss: 1.0580, Train: 71.40%, Valid: 70.61%, Test: 69.17%\n",
            "Run: 10, Epoch: 90, Loss: 1.0370, Train: 71.83%, Valid: 70.35%, Test: 68.52%\n",
            "average train_acc: 0.72\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "\n",
        "#from logger import Logger\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lin in self.lins:\n",
        "            lin.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, lin in enumerate(self.lins[:-1]):\n",
        "            x = lin(x)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lins[-1](x)\n",
        "        return torch.log_softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "def train(model, x, y_true, train_idx, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x[train_idx])\n",
        "    loss = F.nll_loss(out, y_true.squeeze(1)[train_idx])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, x, y_true, split_idx, evaluator):\n",
        "    model.eval()\n",
        "\n",
        "    out = model(x)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    dataset = PygNodePropPredDataset(name='ogbn-arxiv')\n",
        "    split_idx = dataset.get_idx_split()\n",
        "    data = dataset[0]\n",
        "\n",
        "    x = data.x\n",
        "    embedding = torch.load('embedding_arxiv.pt', map_location='cpu')\n",
        "    #x = torch.cat([x, embedding], dim=-1)\n",
        "    x = embedding\n",
        "    x = x.to(device)\n",
        "\n",
        "    y_true = data.y.to(device)\n",
        "    train_idx = split_idx['train'].to(device)\n",
        "\n",
        "    model = MLP(x.size(-1), 256, dataset.num_classes,\n",
        "                3, 0.5).to(device)\n",
        "\n",
        "    evaluator = Evaluator(name='ogbn-arxiv')\n",
        "    #logger = Logger(args.runs, args)\n",
        "\n",
        "    train_accs, valid_accs, test_accs = [], [], []\n",
        "    for run in range(10):\n",
        "        model.reset_parameters()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
        "        for epoch in range(1, 100):\n",
        "            loss = train(model, x, y_true, train_idx, optimizer)\n",
        "            result = test(model, x, y_true, split_idx, evaluator)\n",
        "            #logger.add_result(run, result)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                train_acc, valid_acc, test_acc = result\n",
        "                print(f'Run: {1 + run:02d}, '\n",
        "                      f'Epoch: {epoch:02d}, '\n",
        "                      f'Loss: {loss:.4f}, '\n",
        "                      f'Train: {100 * train_acc:.2f}%, '\n",
        "                      f'Valid: {100 * valid_acc:.2f}%, '\n",
        "                      f'Test: {100 * test_acc:.2f}%')\n",
        "            \n",
        "        result = test(model, x, y_true, split_idx, evaluator)\n",
        "        train_acc, valid_acc, test_acc = result\n",
        "        train_accs.append(train_acc), valid_accs.append(valid_acc), test_accs.append(test_accs)\n",
        "\n",
        "        #logger.print_statistics(run)\n",
        "    #logger.print_statistics()\n",
        "\n",
        "    print(f'average train_acc: {sum(train_accs) / len(train_accs):.2f}')\n",
        "\n",
        "main()\n",
        "\n"
      ],
      "id": "96755de8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "486b56ab"
      },
      "source": [
        "# Large OGB-MAG Dataset"
      ],
      "id": "486b56ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "825a0cce"
      },
      "source": [
        "## Embedding"
      ],
      "id": "825a0cce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a531334"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "from typing import Optional\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "#from logger import Logger\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "\n",
        "\n",
        "class SGConv(MessagePassing):\n",
        "    r\"\"\"The simple graph convolutional operator from the `\"Simplifying Graph\n",
        "    Convolutional Networks\" <https://arxiv.org/abs/1902.07153>`_ paper\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{X}^{\\prime} = {\\left(\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
        "        \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X} \\mathbf{\\Theta},\n",
        "\n",
        "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
        "    adjacency matrix with inserted self-loops and\n",
        "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        K (int, optional): Number of hops :math:`K`. (default: :obj:`1`)\n",
        "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
        "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
        "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X}` on\n",
        "            first execution, and will use the cached version for further\n",
        "            executions.\n",
        "            This parameter should only be set to :obj:`True` in transductive\n",
        "            learning scenarios. (default: :obj:`False`)\n",
        "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
        "            self-loops to the input graph. (default: :obj:`True`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "\n",
        "    _cached_x: Optional[Tensor]\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, K: int = 1,\n",
        "                 cached: bool = False, add_self_loops: bool = True,\n",
        "                 bias: bool = True, dropout: float = 0.05, **kwargs):\n",
        "        super(SGConv, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.K = K\n",
        "        self.cached = cached\n",
        "        self.add_self_loops = add_self_loops\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self._cached_x = None\n",
        "\n",
        "        #self.lin = Linear(in_channels, out_channels, bias=bias)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        #self.lin.reset_parameters()\n",
        "        self._cached_x = None\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj,\n",
        "                edge_weight: OptTensor = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        cache = self._cached_x\n",
        "        if cache is None:\n",
        "            if isinstance(edge_index, Tensor):\n",
        "                edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
        "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                    self.add_self_loops, dtype=x.dtype)\n",
        "            elif isinstance(edge_index, SparseTensor):\n",
        "                edge_index = gcn_norm(  # yapf: disable\n",
        "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                    self.add_self_loops, dtype=x.dtype)\n",
        "\n",
        "            x = F.normalize(x,dim=1, p=2)\n",
        "            alpha = 0.05\n",
        "            output = alpha*x\n",
        "            for k in range(self.K):\n",
        "                x = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
        "                                   size=None)\n",
        "                #x_set.append(x)\n",
        "                output = output + (1./self.K)*x\n",
        "            #x = torch.stack(x_set,2)\n",
        "            #alpha = 0.05\n",
        "            #x = (1-alpha)*torch.mean(x,2).squeeze() + alpha*x_ori\n",
        "            x = output\n",
        "            if self.cached:\n",
        "                self._cached_x = x\n",
        "        else:\n",
        "            x = cache\n",
        "\n",
        "        return x#self.lin(x)\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
        "        return edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {}, K={})'.format(self.__class__.__name__,\n",
        "                                         self.in_channels, self.out_channels,\n",
        "                                         self.K)\n",
        "\n",
        "\n",
        "class SGC(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_layers):\n",
        "        super(SGC, self).__init__()\n",
        "        self.conv1 = SGConv(\n",
        "            in_channels, out_channels, K=num_layers, cached=True)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x, edge_index = data.x, data.edge_index\n",
        "        #x = self.conv1(x, edge_index)\n",
        "        return self.conv1(x, edge_index)#F.log_softmax(x, dim=1)\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(\n",
        "            GCNConv(in_channels, hidden_channels, normalize=False))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(\n",
        "                GCNConv(hidden_channels, hidden_channels, normalize=False))\n",
        "        self.convs.append(\n",
        "            GCNConv(hidden_channels, out_channels, normalize=False))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "class SAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(SAGE, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "def train(model, data, train_idx, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.adj_t)[train_idx]\n",
        "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "    model.eval()\n",
        "\n",
        "    out = model(data.x, data.adj_t)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']['paper']],\n",
        "        'y_pred': y_pred[split_idx['train']['paper']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']['paper']],\n",
        "        'y_pred': y_pred[split_idx['valid']['paper']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']['paper']],\n",
        "        'y_pred': y_pred[split_idx['test']['paper']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc\n",
        "\n",
        "precompute_time = 0.0\n",
        "def main():\n",
        "\n",
        "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    dataset = PygNodePropPredDataset(name='ogbn-mag')\n",
        "    rel_data = dataset[0]\n",
        "\n",
        "    # We are only interested in paper <-> paper relations.\n",
        "    data = Data(\n",
        "        x=rel_data.x_dict['paper'],\n",
        "        edge_index=rel_data.edge_index_dict[('paper', 'cites', 'paper')],\n",
        "        y=rel_data.y_dict['paper'])\n",
        "\n",
        "    data = T.ToSparseTensor()(data)\n",
        "    data.adj_t = data.adj_t.to_symmetric()\n",
        "\n",
        "    split_idx = dataset.get_idx_split()\n",
        "    train_idx = split_idx['train']['paper'].to(device)\n",
        "\n",
        "    model = SGC(data.num_features,\n",
        "                dataset.num_classes, 16,\n",
        "                ).to(device)\n",
        "\n",
        "    # Pre-compute GCN normalization.\n",
        "    adj_t = data.adj_t.set_diag()\n",
        "    deg = adj_t.sum(dim=1).to(torch.float)\n",
        "    deg_inv_sqrt = deg.pow(-0.5)\n",
        "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "    adj_t = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
        "    data.adj_t = adj_t\n",
        "\n",
        "    data = data.to(device)\n",
        "    precompute_time = perf_counter()\n",
        "    features = model(data.x, data.adj_t)\n",
        "    torch.save(features,'embedding_mag.pt')\n",
        "    precompute_time = perf_counter() - precompute_time\n",
        "\n",
        "    return precompute_time\n",
        "\n",
        "precompute_time = main()"
      ],
      "id": "1a531334"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83952166"
      },
      "source": [
        "## SSGC"
      ],
      "id": "83952166"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6919e26c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from utils import load_citation, sgc_precompute, set_seed\n",
        "from models import get_model\n",
        "from metrics import accuracy\n",
        "import pickle as pkl\n",
        "#from args_cora import get_citation_args\n",
        "from time import perf_counter\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "import torch_geometric.transforms as T\n",
        "from normalization import fetch_normalization, row_normalize\n",
        "from utils import *\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Arguments\n",
        "#args = get_citation_args()\n",
        "\n",
        "#if args.tuned:\n",
        "#    if args.model == \"SGC\":\n",
        "#        with open(\"{}-tuning/{}.txt\".format(args.model, args.dataset), 'rb') as f:\n",
        "#            args.weight_decay = pkl.load(f)['weight_decay']\n",
        "#            print(\"using tuned weight decay: {}\".format(args.weight_decay))\n",
        "#    else:\n",
        "#        raise NotImplemented\n",
        "\n",
        "# setting random seeds\n",
        "#set_seed(args.seed, args.cuda)\n",
        "\n",
        "dataset = PygNodePropPredDataset(name='ogbn-mag')\n",
        "data = dataset[0]\n",
        "\n",
        "data = Data(\n",
        "    x=data.x_dict['paper'],\n",
        "    edge_index=data.edge_index_dict[('paper', 'cites', 'paper')],\n",
        "    y=data.y_dict['paper'])\n",
        "\n",
        "data = T.ToSparseTensor()(data)\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "idx_train = split_idx['train']['paper']\n",
        "idx_val = split_idx['valid']['paper']\n",
        "idx_test = split_idx['test']['paper']\n",
        "labels = torch.LongTensor([y[0] for y in data.y.tolist()])\n",
        "\n",
        "features = torch.load('embedding_mag.pt', map_location='cpu')\n",
        "\n",
        "model = get_model(\"SGC\", features.shape[1], labels.max().item()+1, 0, 0, 0)\n",
        "\n",
        "#print(\"{:.4f}s\".format(precompute_time))\n",
        "\n",
        "def train_regression(model,\n",
        "                     train_features, train_labels,\n",
        "                     val_features, val_labels,\n",
        "                     epochs=100, weight_decay=1e-05,\n",
        "                     lr=0.2, dropout=0):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
        "                           weight_decay=weight_decay)\n",
        "    t = perf_counter()\n",
        "    best_acc_val = torch.zeros((1))\n",
        "    best_loss_val = 100.\n",
        "    best_model = None\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(train_features)\n",
        "        loss_train = F.cross_entropy(output, train_labels)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            output = model(val_features)\n",
        "            acc_val = accuracy(output, val_labels)\n",
        "            loss_val = F.cross_entropy(output, val_labels)\n",
        "            if best_acc_val < acc_val:\n",
        "                 best_acc_val = acc_val\n",
        "            #     best_model = model\n",
        "            if best_loss_val > loss_val:\n",
        "                best_loss_val = loss_val\n",
        "                best_model = model\n",
        "\n",
        "    train_time = perf_counter()-t\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     model.eval()\n",
        "    #     output = model(val_features)\n",
        "    #     acc_val = accuracy(output, val_labels)\n",
        "\n",
        "    return best_model, best_acc_val, train_time\n",
        "\n",
        "def test_regression(model, test_features, test_labels):\n",
        "    model.eval()\n",
        "    return accuracy(model(test_features), test_labels)\n",
        "\n",
        "acc_test = 0\n",
        "acc_val = 0\n",
        "train_time = 0\n",
        "for _ in range(10):\n",
        "    model, acc_val_buf, train_time_buf = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
        "                        100, 1e-05, 0.02, 0)\n",
        "    acc_test += test_regression(model, features[idx_test], labels[idx_test])\n",
        "    acc_val += acc_val_buf\n",
        "    train_time += train_time_buf\n",
        "acc_test /= 10\n",
        "acc_val /= 10\n",
        "train_time /= 10\n",
        "\n",
        "print(\"Validation Accuracy: {:.4f} Test Accuracy: {:.4f}\".format(acc_val, acc_test))\n",
        "print(\"Pre-compute time: {:.4f}s, train time: {:.4f}s, total: {:.4f}s\".format(precompute_time, train_time, precompute_time+train_time))"
      ],
      "id": "6919e26c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2477508c"
      },
      "source": [
        "## SSGC + MLP"
      ],
      "id": "2477508c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87d8eb58"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "\n",
        "#from logger import Logger\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
        "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lin in self.lins:\n",
        "            lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, lin in enumerate(self.lins[:-1]):\n",
        "            x = lin(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lins[-1](x)\n",
        "        return torch.log_softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "def train(model, x, y_true, train_idx, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x[train_idx])\n",
        "    loss = F.nll_loss(out, y_true[train_idx].squeeze(1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, x, y_true, split_idx, evaluator):\n",
        "    model.eval()\n",
        "\n",
        "    out = model(x)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['train']['paper']],\n",
        "        'y_pred': y_pred[split_idx['train']['paper']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['valid']['paper']],\n",
        "        'y_pred': y_pred[split_idx['valid']['paper']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['test']['paper']],\n",
        "        'y_pred': y_pred[split_idx['test']['paper']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    dataset = PygNodePropPredDataset(name='ogbn-mag')\n",
        "    split_idx = dataset.get_idx_split()\n",
        "    data = dataset[0]\n",
        "\n",
        "    x = data.x_dict['paper']\n",
        "    embedding = torch.load('embedding_mag.pt', map_location='cpu')\n",
        "    x = torch.cat([x, embedding], dim=-1)\n",
        "    x = x.to(device)\n",
        "\n",
        "    y_true = data.y_dict['paper'].to(device)\n",
        "    train_idx = split_idx['train']['paper'].to(device)\n",
        "\n",
        "    model = MLP(x.size(-1), 256, dataset.num_classes,\n",
        "                3, 0.0).to(device)\n",
        "\n",
        "    evaluator = Evaluator(name='ogbn-mag')\n",
        "    #logger = Logger(args.runs, args)\n",
        "\n",
        "    for run in range(10):\n",
        "        model.reset_parameters()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "        for epoch in range(1, 100):\n",
        "            loss = train(model, x, y_true, train_idx, optimizer)\n",
        "            result = test(model, x, y_true, split_idx, evaluator)\n",
        "            #logger.add_result(run, result)\n",
        "\n",
        "            if epoch % 1 == 0:\n",
        "                train_acc, valid_acc, test_acc = result\n",
        "                print(f'Run: {run + 1:02d}, '\n",
        "                      f'Epoch: {epoch:02d}, '\n",
        "                      f'Loss: {loss:.4f}, '\n",
        "                      f'Train: {100 * train_acc:.2f}%, '\n",
        "                      f'Valid: {100 * valid_acc:.2f}%, '\n",
        "                      f'Test: {100 * test_acc:.2f}%')\n",
        "\n",
        "        #logger.print_statistics(run)\n",
        "    #logger.print_statistics()\n",
        "\n",
        "main()"
      ],
      "id": "87d8eb58"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "065c0567"
      },
      "source": [
        "# Large OGB-Products Dataset"
      ],
      "id": "065c0567"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2be5daba"
      },
      "source": [
        "## Embedding"
      ],
      "id": "2be5daba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c41faf87",
        "outputId": "5b86390f-3c26-4b69-9cc7-d2ff5238d244"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.utils import dropout_adj\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "from typing import Optional\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "#from logger import Logger\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "\n",
        "\n",
        "class SGConv(MessagePassing):\n",
        "    r\"\"\"The simple graph convolutional operator from the `\"Simplifying Graph\n",
        "    Convolutional Networks\" <https://arxiv.org/abs/1902.07153>`_ paper\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{X}^{\\prime} = {\\left(\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
        "        \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X} \\mathbf{\\Theta},\n",
        "\n",
        "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
        "    adjacency matrix with inserted self-loops and\n",
        "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        K (int, optional): Number of hops :math:`K`. (default: :obj:`1`)\n",
        "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
        "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
        "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X}` on\n",
        "            first execution, and will use the cached version for further\n",
        "            executions.\n",
        "            This parameter should only be set to :obj:`True` in transductive\n",
        "            learning scenarios. (default: :obj:`False`)\n",
        "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
        "            self-loops to the input graph. (default: :obj:`True`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "\n",
        "    _cached_x: Optional[Tensor]\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, K: int = 1,\n",
        "                 cached: bool = False, add_self_loops: bool = True,\n",
        "                 bias: bool = True, dropout: float = 0.05, **kwargs):\n",
        "        super(SGConv, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.K = K\n",
        "        self.cached = cached\n",
        "        self.add_self_loops = add_self_loops\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self._cached_x = None\n",
        "\n",
        "        self.lin = Linear(in_channels, out_channels, bias=bias)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        #self.lin.reset_parameters()\n",
        "        self._cached_x = None\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj,\n",
        "                edge_weight: OptTensor = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        cache = self._cached_x\n",
        "        if cache is None:\n",
        "            if isinstance(edge_index, Tensor):\n",
        "                edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
        "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                    self.add_self_loops, dtype=x.dtype)\n",
        "            elif isinstance(edge_index, SparseTensor):\n",
        "                edge_index = gcn_norm(  # yapf: disable\n",
        "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                    self.add_self_loops, dtype=x.dtype)\n",
        "\n",
        "            x_set = []\n",
        "            alpha = 0.05\n",
        "            output = alpha * x\n",
        "            #temp_edge_index, edge_weight = dropout_adj(edge_index, 0.5)\n",
        "            for k in range(self.K):\n",
        "                x = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
        "                                   size=None)\n",
        "                # x_set.append(x)\n",
        "                output = output + (1. / self.K) * x\n",
        "            # x = torch.stack(x_set,2)\n",
        "            # alpha = 0.05\n",
        "            # x = (1-alpha)*torch.mean(x,2).squeeze() + alpha*x_ori\n",
        "            x = output\n",
        "            if self.cached:\n",
        "                self._cached_x = x\n",
        "        else:\n",
        "            x = cache\n",
        "\n",
        "        return x#self.lin(x)\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
        "        return edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {}, K={})'.format(self.__class__.__name__,\n",
        "                                         self.in_channels, self.out_channels,\n",
        "                                         self.K)\n",
        "\n",
        "\n",
        "class SGC(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden, out_channels, num_layers, dropout):\n",
        "        super(SGC, self).__init__()\n",
        "        self.conv1 = SGConv(\n",
        "            in_channels, hidden, K=num_layers, cached=True)\n",
        "        self.lin = torch.nn.Linear(hidden, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        self.lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x, edge_index = data.x, data.edge_index\n",
        "        #x = self.conv1(x, edge_index)\n",
        "        # x = F.relu(x)\n",
        "        # x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv1(x, edge_index)#F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class SAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(SAGE, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "def train(model, data, train_idx, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.adj_t)[train_idx]\n",
        "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "    model.eval()\n",
        "\n",
        "    out = model(data.x, data.adj_t)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc\n",
        "\n",
        "precompute_time = 0.0\n",
        "def main():\n",
        "\n",
        "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    dataset = PygNodePropPredDataset(name='ogbn-products',\n",
        "                                     transform=T.ToSparseTensor())\n",
        "\n",
        "    data = dataset[0]\n",
        "    data.adj_t = data.adj_t.to_symmetric()\n",
        "    data = data.to(device)\n",
        "\n",
        "    split_idx = dataset.get_idx_split()\n",
        "    train_idx = split_idx['train'].to(device)\n",
        "\n",
        "    precompute_time = perf_counter()\n",
        "    model = SGC(data.num_features, 256, dataset.num_classes, 16, 0.5).to(device)\n",
        "    \n",
        "    features = model(data.x, data.adj_t)\n",
        "    torch.save(features,'embedding_products.pt')\n",
        "    precompute_time = perf_counter() - precompute_time\n",
        "    \n",
        "    return precompute_time\n",
        "precompute_time = main()"
      ],
      "id": "c41faf87"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "921e8e39"
      },
      "source": [
        "## SSGC"
      ],
      "id": "921e8e39"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16aa51d3"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from utils import load_citation, sgc_precompute, set_seed\n",
        "from models import get_model\n",
        "from metrics import accuracy\n",
        "import pickle as pkl\n",
        "#from args_cora import get_citation_args\n",
        "from time import perf_counter\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "import torch_geometric.transforms as T\n",
        "from normalization import fetch_normalization, row_normalize\n",
        "from utils import *\n",
        "\n",
        "# Arguments\n",
        "#args = get_citation_args()\n",
        "\n",
        "#if args.tuned:\n",
        "#    if args.model == \"SGC\":\n",
        "#        with open(\"{}-tuning/{}.txt\".format(args.model, args.dataset), 'rb') as f:\n",
        "#            args.weight_decay = pkl.load(f)['weight_decay']\n",
        "#            print(\"using tuned weight decay: {}\".format(args.weight_decay))\n",
        "#    else:\n",
        "#        raise NotImplemented\n",
        "\n",
        "# setting random seeds\n",
        "#set_seed(args.seed, args.cuda)\n",
        "\n",
        "dataset = PygNodePropPredDataset(name='ogbn-products', transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "idx_train = split_idx['train']\n",
        "idx_val = split_idx['valid']\n",
        "idx_test = split_idx['test']\n",
        "labels = torch.LongTensor([y[0] for y in data.y.tolist()])\n",
        "\n",
        "features = torch.load('embedding_products.pt', map_location='cpu')\n",
        "\n",
        "model = get_model(\"SGC\", features.shape[1], labels.max().item()+1, 0, 0, 0)\n",
        "\n",
        "def train_regression(model,\n",
        "                     train_features, train_labels,\n",
        "                     val_features, val_labels,\n",
        "                     epochs=100, weight_decay=1e-05,\n",
        "                     lr=0.2, dropout=0):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
        "                           weight_decay=weight_decay)\n",
        "    t = perf_counter()\n",
        "    best_acc_val = torch.zeros((1))\n",
        "    best_loss_val = 100.\n",
        "    best_model = None\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(train_features)\n",
        "        loss_train = F.cross_entropy(output, train_labels)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            output = model(val_features)\n",
        "            acc_val = accuracy(output, val_labels)\n",
        "            loss_val = F.cross_entropy(output, val_labels)\n",
        "            if best_acc_val < acc_val:\n",
        "                 best_acc_val = acc_val\n",
        "            #     best_model = model\n",
        "            if best_loss_val > loss_val:\n",
        "                best_loss_val = loss_val\n",
        "                best_model = model\n",
        "\n",
        "    train_time = perf_counter()-t\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     model.eval()\n",
        "    #     output = model(val_features)\n",
        "    #     acc_val = accuracy(output, val_labels)\n",
        "\n",
        "    return best_model, best_acc_val, train_time\n",
        "\n",
        "def test_regression(model, test_features, test_labels):\n",
        "    model.eval()\n",
        "    return accuracy(model(test_features), test_labels)\n",
        "acc_test = 0\n",
        "acc_val = 0\n",
        "train_time = 0\n",
        "for _ in range(10):\n",
        "    model, acc_val_buf, train_time_buf = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
        "                        1000, 1e-05, 0.02, 0)\n",
        "    acc_test += test_regression(model, features[idx_test], labels[idx_test])\n",
        "    acc_val += acc_val_buf\n",
        "    train_time += train_time_buf\n",
        "acc_test /= 10\n",
        "acc_val /= 10\n",
        "train_time /= 10\n",
        "\n",
        "print(\"Validation Accuracy: {:.4f} Test Accuracy: {:.4f}\".format(acc_val, acc_test))\n",
        "print(\"Pre-compute time: {:.4f}s, train time: {:.4f}s, total: {:.4f}s\".format(precompute_time, train_time, precompute_time+train_time))"
      ],
      "id": "16aa51d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc18db3d"
      },
      "source": [
        "## SSGC + MLP"
      ],
      "id": "fc18db3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "id": "93e37af8",
        "outputId": "50717a88-cc94-413a-9980-c6c1c4589bfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This will download 1.38GB. Will you proceed? (y/N)\n",
            "y\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloaded 1.38 GB: 100%|██████████| 1414/1414 [02:03<00:00, 11.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset/products.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 293.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2580d45640c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m#logger.print_statistics()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-2580d45640c5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embedding_products.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;31m#x = torch.cat([x, embedding], dim=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'embedding_products.pt'"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "\n",
        "#from logger import Logger\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lin in self.lins:\n",
        "            lin.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, lin in enumerate(self.lins[:-1]):\n",
        "            x = lin(x)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lins[-1](x)\n",
        "        return torch.log_softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "def train(model, x, y_true, train_idx, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x[train_idx])\n",
        "    loss = F.nll_loss(out, y_true.squeeze(1)[train_idx])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, x, y_true, split_idx, evaluator):\n",
        "    model.eval()\n",
        "\n",
        "    out = model(x)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': y_true[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    dataset = PygNodePropPredDataset(name='ogbn-products')\n",
        "    split_idx = dataset.get_idx_split()\n",
        "    data = dataset[0]\n",
        "\n",
        "    x = data.x\n",
        "    embedding = torch.load('embedding_products.pt', map_location='cpu')\n",
        "    #x = torch.cat([x, embedding], dim=-1)\n",
        "    x = embedding\n",
        "    x = x.to(device)\n",
        "\n",
        "    y_true = data.y.to(device)\n",
        "    train_idx = split_idx['train'].to(device)\n",
        "\n",
        "    model = MLP(x.size(-1), 256, dataset.num_classes,\n",
        "                3, 0.5).to(device)\n",
        "\n",
        "    evaluator = Evaluator(name='ogbn-arxiv')\n",
        "    #logger = Logger(args.runs, args)\n",
        "\n",
        "    for run in range(10):\n",
        "        model.reset_parameters()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "        for epoch in range(1, 1 + 500):\n",
        "            loss = train(model, x, y_true, train_idx, optimizer)\n",
        "            result = test(model, x, y_true, split_idx, evaluator)\n",
        "            #logger.add_result(run, result)\n",
        "\n",
        "            if epoch % 1 == 0:\n",
        "                train_acc, valid_acc, test_acc = result\n",
        "                print(f'Run: {run + 1:02d}, '\n",
        "                      f'Epoch: {epoch:02d}, '\n",
        "                      f'Loss: {loss:.4f}, '\n",
        "                      f'Train: {100 * train_acc:.2f}%, '\n",
        "                      f'Valid: {100 * valid_acc:.2f}%, '\n",
        "                      f'Test: {100 * test_acc:.2f}%')\n",
        "\n",
        "        #logger.print_statistics(run)\n",
        "    #logger.print_statistics()\n",
        "\n",
        "main()"
      ],
      "id": "93e37af8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5aa9898"
      },
      "source": [
        "# Large Reddit Dataset / Community Prediction"
      ],
      "id": "e5aa9898"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25a14222"
      },
      "source": [
        "## Embedding"
      ],
      "id": "25a14222"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb0faf5a",
        "outputId": "6a583624-e412-4b08-a92a-fa276e33e306"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29.876081878999912"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import argparse\n",
        "from time import perf_counter\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.utils import dropout_adj\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "from typing import Optional\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "#from logger import Logger\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.datasets import Reddit\n",
        "\n",
        "\n",
        "\n",
        "class SGConv(MessagePassing):\n",
        "    r\"\"\"The simple graph convolutional operator from the `\"Simplifying Graph\n",
        "    Convolutional Networks\" <https://arxiv.org/abs/1902.07153>`_ paper\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{X}^{\\prime} = {\\left(\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
        "        \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X} \\mathbf{\\Theta},\n",
        "\n",
        "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
        "    adjacency matrix with inserted self-loops and\n",
        "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        K (int, optional): Number of hops :math:`K`. (default: :obj:`1`)\n",
        "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
        "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
        "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}^K \\mathbf{X}` on\n",
        "            first execution, and will use the cached version for further\n",
        "            executions.\n",
        "            This parameter should only be set to :obj:`True` in transductive\n",
        "            learning scenarios. (default: :obj:`False`)\n",
        "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
        "            self-loops to the input graph. (default: :obj:`True`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "\n",
        "    _cached_x: Optional[Tensor]\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, K: int = 1,\n",
        "                 cached: bool = False, add_self_loops: bool = True,\n",
        "                 bias: bool = True, dropout: float = 0.05, **kwargs):\n",
        "        super(SGConv, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.K = K\n",
        "        self.cached = cached\n",
        "        self.add_self_loops = add_self_loops\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self._cached_x = None\n",
        "\n",
        "        self.lin = Linear(in_channels, out_channels, bias=bias)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        #self.lin.reset_parameters()\n",
        "        self._cached_x = None\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj,\n",
        "                edge_weight: OptTensor = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        cache = self._cached_x\n",
        "        if cache is None:\n",
        "            if isinstance(edge_index, Tensor):\n",
        "                edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
        "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                    self.add_self_loops, dtype=x.dtype)\n",
        "            elif isinstance(edge_index, SparseTensor):\n",
        "                edge_index = gcn_norm(  # yapf: disable\n",
        "                    edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                    self.add_self_loops, dtype=x.dtype)\n",
        "\n",
        "            x_set = []\n",
        "            alpha = 0.05\n",
        "            output = alpha * x\n",
        "            #temp_edge_index, edge_weight = dropout_adj(edge_index, 0.5)\n",
        "            for k in range(self.K):\n",
        "                x = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
        "                                   size=None)\n",
        "                # x_set.append(x)\n",
        "                output = output + (1. / self.K) * x\n",
        "            # x = torch.stack(x_set,2)\n",
        "            # alpha = 0.05\n",
        "            # x = (1-alpha)*torch.mean(x,2).squeeze() + alpha*x_ori\n",
        "            x = output\n",
        "            if self.cached:\n",
        "                self._cached_x = x\n",
        "        else:\n",
        "            x = cache\n",
        "\n",
        "        return x#self.lin(x)\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
        "        return edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {}, K={})'.format(self.__class__.__name__,\n",
        "                                         self.in_channels, self.out_channels,\n",
        "                                         self.K)\n",
        "\n",
        "\n",
        "class SGC(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden, out_channels, num_layers, dropout):\n",
        "        super(SGC, self).__init__()\n",
        "        self.conv1 = SGConv(\n",
        "            in_channels, hidden, K=num_layers, cached=True)\n",
        "        self.lin = torch.nn.Linear(hidden, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        self.lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x, edge_index = data.x, data.edge_index\n",
        "        #x = self.conv1(x, edge_index)\n",
        "        # x = F.relu(x)\n",
        "        # x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv1(x, edge_index)#F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class SAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(SAGE, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "\n",
        "def train(model, data, train_idx, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.adj_t)[train_idx]\n",
        "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "    model.eval()\n",
        "\n",
        "    out = model(data.x, data.adj_t)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc\n",
        "\n",
        "precompute_time = 0.0\n",
        "def main():\n",
        "\n",
        "    device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    #data = dataset[0]\n",
        "    dataset = Reddit(\"./dataset/reddit\")\n",
        "    data = dataset[0]\n",
        "    data = T.ToSparseTensor()(data)\n",
        "    \n",
        "    #data.adj_t = data.adj_t.to_symmetric()\n",
        "    data = data.to(device)\n",
        "\n",
        "    # split_idx = dataset.get_idx_split()\n",
        "\n",
        "    idx_train = torch.tensor([i for i, is_train in enumerate(data.train_mask) if is_train]).to(device)\n",
        "    idx_val = torch.tensor([i for i, is_valid in enumerate(data.val_mask) if is_valid]).to(device)\n",
        "    idx_test = torch.tensor([i for i, is_test in enumerate(data.test_mask) if is_test]).to(device)\n",
        "\n",
        "    # train_idx = split_idx['train'].to(device)\n",
        "\n",
        "    precompute_time = perf_counter()\n",
        "    model = SGC(data.num_features, 256, dataset.num_classes, 16, 0.5).to(device)\n",
        "    \n",
        "    features = model(data.x, data.adj_t)\n",
        "    torch.save(features,'embedding_reddit.pt')\n",
        "    precompute_time = perf_counter() - precompute_time\n",
        "\n",
        "    return precompute_time\n",
        "\n",
        "main()"
      ],
      "id": "cb0faf5a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b682197a"
      },
      "source": [
        "## SSGC"
      ],
      "id": "b682197a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beea5fe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16bdc1d7-9e22-4ed9-b62c-4a4754eebe8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 finished\n",
            "epoch: 11 finished\n",
            "epoch: 21 finished\n",
            "epoch: 31 finished\n",
            "epoch: 41 finished\n",
            "epoch: 51 finished\n",
            "epoch: 61 finished\n",
            "epoch: 71 finished\n",
            "epoch: 81 finished\n",
            "epoch: 91 finished\n",
            "epoch: 101 finished\n",
            "epoch: 111 finished\n",
            "epoch: 121 finished\n",
            "epoch: 131 finished\n",
            "epoch: 141 finished\n",
            "epoch: 151 finished\n",
            "epoch: 161 finished\n",
            "epoch: 171 finished\n",
            "epoch: 181 finished\n",
            "epoch: 191 finished\n",
            "run: 1 finished\n",
            "epoch: 1 finished\n",
            "epoch: 11 finished\n",
            "epoch: 21 finished\n",
            "epoch: 31 finished\n",
            "epoch: 41 finished\n",
            "epoch: 51 finished\n",
            "epoch: 61 finished\n",
            "epoch: 71 finished\n",
            "epoch: 81 finished\n",
            "epoch: 91 finished\n",
            "epoch: 101 finished\n",
            "epoch: 111 finished\n",
            "epoch: 121 finished\n",
            "epoch: 131 finished\n",
            "epoch: 141 finished\n",
            "epoch: 151 finished\n",
            "epoch: 161 finished\n",
            "epoch: 171 finished\n",
            "epoch: 181 finished\n",
            "epoch: 191 finished\n",
            "run: 2 finished\n",
            "epoch: 1 finished\n",
            "epoch: 11 finished\n",
            "epoch: 21 finished\n",
            "epoch: 31 finished\n",
            "epoch: 41 finished\n",
            "epoch: 51 finished\n",
            "epoch: 61 finished\n",
            "epoch: 71 finished\n",
            "epoch: 81 finished\n",
            "epoch: 91 finished\n",
            "epoch: 101 finished\n",
            "epoch: 111 finished\n",
            "epoch: 121 finished\n",
            "epoch: 131 finished\n",
            "epoch: 141 finished\n",
            "epoch: 151 finished\n",
            "epoch: 161 finished\n",
            "epoch: 171 finished\n",
            "epoch: 181 finished\n",
            "epoch: 191 finished\n",
            "run: 3 finished\n",
            "epoch: 1 finished\n",
            "epoch: 11 finished\n",
            "epoch: 21 finished\n",
            "epoch: 31 finished\n",
            "epoch: 41 finished\n",
            "epoch: 51 finished\n",
            "epoch: 61 finished\n",
            "epoch: 71 finished\n",
            "epoch: 81 finished\n",
            "epoch: 91 finished\n",
            "epoch: 101 finished\n",
            "epoch: 111 finished\n",
            "epoch: 121 finished\n",
            "epoch: 131 finished\n",
            "epoch: 141 finished\n",
            "epoch: 151 finished\n",
            "epoch: 161 finished\n",
            "epoch: 171 finished\n",
            "epoch: 181 finished\n",
            "epoch: 191 finished\n",
            "run: 4 finished\n",
            "epoch: 1 finished\n",
            "epoch: 11 finished\n",
            "epoch: 21 finished\n",
            "epoch: 31 finished\n",
            "epoch: 41 finished\n",
            "epoch: 51 finished\n",
            "epoch: 61 finished\n",
            "epoch: 71 finished\n",
            "epoch: 81 finished\n",
            "epoch: 91 finished\n",
            "epoch: 101 finished\n",
            "epoch: 111 finished\n",
            "epoch: 121 finished\n",
            "epoch: 131 finished\n",
            "epoch: 141 finished\n",
            "epoch: 151 finished\n",
            "epoch: 161 finished\n",
            "epoch: 171 finished\n",
            "epoch: 181 finished\n",
            "epoch: 191 finished\n",
            "run: 5 finished\n",
            "Validation Accuracy: 0.9482 Test F1: 0.9461\n",
            "Pre-compute time: 0.0000s, train time: 141.1817s, total: 141.1817s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from utils import load_citation, sgc_precompute, set_seed\n",
        "from models import get_model\n",
        "from metrics import accuracy, f1\n",
        "import pickle as pkl\n",
        "#from args_cora import get_citation_args\n",
        "from time import perf_counter\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "import torch_geometric.transforms as T\n",
        "from normalization import fetch_normalization, row_normalize\n",
        "from utils import *\n",
        "from torch_geometric.datasets import Reddit\n",
        "\n",
        "# Arguments\n",
        "#args = get_citation_args()\n",
        "\n",
        "#if args.tuned:\n",
        "#    if args.model == \"SGC\":\n",
        "#        with open(\"{}-tuning/{}.txt\".format(args.model, args.dataset), 'rb') as f:\n",
        "#            args.weight_decay = pkl.load(f)['weight_decay']\n",
        "#            print(\"using tuned weight decay: {}\".format(args.weight_decay))\n",
        "#    else:\n",
        "#        raise NotImplemented\n",
        "\n",
        "# setting random seeds\n",
        "#set_seed(args.seed, args.cuda)\n",
        "\n",
        "dataset = Reddit(\"./dataset/reddit\")\n",
        "data = dataset[0]\n",
        "\n",
        "data = T.ToSparseTensor()(data)\n",
        "\n",
        "# split_idx = dataset.get_idx_split()\n",
        "# idx_train = split_idx['train']\n",
        "# idx_val = split_idx['valid']\n",
        "# idx_test = split_idx['test']\n",
        "idx_train = torch.tensor([i for i, is_train in enumerate(data.train_mask) if is_train])\n",
        "idx_val = torch.tensor([i for i, is_valid in enumerate(data.val_mask) if is_valid])\n",
        "idx_test = torch.tensor([i for i, is_test in enumerate(data.test_mask) if is_test])\n",
        "# labels = torch.LongTensor([y[0] for y in data.y.tolist()])\n",
        "labels = data.y\n",
        "\n",
        "features = torch.load('embedding_reddit.pt', map_location='cpu')\n",
        "\n",
        "model = get_model(\"SGC\", features.shape[1], labels.max().item()+1, 0, 0, 0)\n",
        "\n",
        "def train_regression(model,\n",
        "                     train_features, train_labels,\n",
        "                     val_features, val_labels,\n",
        "                     epochs=100, weight_decay=1e-05,\n",
        "                     lr=0.2, dropout=0):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr,\n",
        "                           weight_decay=weight_decay)\n",
        "    t = perf_counter()\n",
        "    best_acc_val = torch.zeros((1))\n",
        "    best_loss_val = 100.\n",
        "    best_model = None\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(train_features)\n",
        "        loss_train = F.cross_entropy(output, train_labels)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            output = model(val_features)\n",
        "            acc_val = accuracy(output, val_labels)\n",
        "            loss_val = F.cross_entropy(output, val_labels)\n",
        "            if best_acc_val < acc_val:\n",
        "                 best_acc_val = acc_val\n",
        "            #     best_model = model\n",
        "            if best_loss_val > loss_val:\n",
        "                best_loss_val = loss_val\n",
        "                best_model = model\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'epoch: {epoch + 1} finished')\n",
        "\n",
        "    train_time = perf_counter()-t\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     model.eval()\n",
        "    #     output = model(val_features)\n",
        "    #     acc_val = accuracy(output, val_labels)\n",
        "\n",
        "    return best_model, best_acc_val, train_time\n",
        "\n",
        "def test_regression(model, test_features, test_labels):\n",
        "    model.eval()\n",
        "    # return accuracy(model(test_features), test_labels)\n",
        "    return f1(model(test_features), test_labels)[0]\n",
        "\n",
        "NUM_EPOCHS = 200\n",
        "NUM_RUNS = 5\n",
        "\n",
        "acc_test = 0\n",
        "acc_val = 0\n",
        "train_time = 0\n",
        "for run in range(NUM_RUNS):\n",
        "    model, acc_val_buf, train_time_buf = train_regression(model, features[idx_train], labels[idx_train], features[idx_val], labels[idx_val],\n",
        "                        NUM_EPOCHS, 1e-05, 0.02, 0)\n",
        "    acc_test += test_regression(model, features[idx_test], labels[idx_test])\n",
        "    acc_val += acc_val_buf\n",
        "    train_time += train_time_buf\n",
        "\n",
        "    print(f'run: {run + 1} finished')\n",
        "\n",
        "acc_test /= NUM_RUNS\n",
        "acc_val /= NUM_RUNS\n",
        "train_time /= NUM_RUNS\n",
        "\n",
        "print(\"Validation Accuracy: {:.4f} Test F1: {:.4f}\".format(acc_val, acc_test))\n",
        "print(\"Pre-compute time: {:.4f}s, train time: {:.4f}s, total: {:.4f}s\".format(precompute_time, train_time, precompute_time+train_time))"
      ],
      "id": "beea5fe9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Node Clustering"
      ],
      "metadata": {
        "id": "CcTDmUBHOSOa"
      },
      "id": "CcTDmUBHOSOa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5c952f3",
        "outputId": "942383d4-a521-4df9-ef97-ee60d72de5f6",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset: cora acc: 0.6652511078286558  nmi: 0.5109362256429308 f1-score:  0.6312434441582484\n",
            "dataset: citeseer acc: 0.687883064779603  nmi: 0.42606741824229544 f1-score:  0.6443867309397274\n",
            "dataset: pubmed acc: 0.6967810917931353  nmi: 0.317986134801085 f1-score:  0.6907845693766443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset: wiki acc: 0.5187281399046105  nmi: 0.48933923797813084 f1-score:  0.4437493461822928\n"
          ]
        }
      ],
      "source": [
        "import scipy.io as sio\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.cluster import KMeans\n",
        "from NodeClustering.metrics import clustering_metrics\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import normalize\n",
        "def normalize_adj(adj, type='sym'):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    if type == 'sym':\n",
        "        adj = sp.coo_matrix(adj)\n",
        "        rowsum = np.array(adj.sum(1))\n",
        "        # d_inv_sqrt = np.power(rowsum, -0.5)\n",
        "        # d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "        # return adj*d_inv_sqrt*d_inv_sqrt.flatten()\n",
        "        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "        return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
        "    elif type == 'rw':\n",
        "        rowsum = np.array(adj.sum(1))\n",
        "        d_inv = np.power(rowsum, -1.0).flatten()\n",
        "        d_inv[np.isinf(d_inv)] = 0.\n",
        "        d_mat_inv = sp.diags(d_inv)\n",
        "        adj_normalized = d_mat_inv.dot(adj)\n",
        "        return adj_normalized\n",
        "\n",
        "\n",
        "def preprocess_adj(adj, type='sym', loop=True):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    if loop:\n",
        "        adj = adj + sp.eye(adj.shape[0])\n",
        "    adj_normalized = normalize_adj(adj, type=type)\n",
        "    return adj_normalized\n",
        "\n",
        "\n",
        "def to_onehot(prelabel):\n",
        "    k = len(np.unique(prelabel))\n",
        "    label = np.zeros([prelabel.shape[0], k])\n",
        "    label[range(prelabel.shape[0]), prelabel] = 1\n",
        "    label = label.T\n",
        "    return label\n",
        "\n",
        "\n",
        "def square_dist(prelabel, feature):\n",
        "    if sp.issparse(feature):\n",
        "        feature = feature.todense()\n",
        "    feature = np.array(feature)\n",
        "\n",
        "\n",
        "    onehot = to_onehot(prelabel)\n",
        "\n",
        "    m, n = onehot.shape\n",
        "    count = onehot.sum(1).reshape(m, 1)\n",
        "    count[count==0] = 1\n",
        "\n",
        "    mean = onehot.dot(feature)/count\n",
        "    a2 = (onehot.dot(feature*feature)/count).sum(1)\n",
        "    pdist2 = np.array(a2 + a2.T - 2*mean.dot(mean.T))\n",
        "\n",
        "    intra_dist = pdist2.trace()\n",
        "    inter_dist = pdist2.sum() - intra_dist\n",
        "    intra_dist /= m\n",
        "    inter_dist /= m * (m - 1)\n",
        "    return intra_dist\n",
        "\n",
        "def dist(prelabel, feature):\n",
        "    k = len(np.unique(prelabel))\n",
        "    intra_dist = 0\n",
        "\n",
        "    for i in range(k):\n",
        "        Data_i = feature[np.where(prelabel == i)]\n",
        "\n",
        "        Dis = euclidean_distances(Data_i, Data_i)\n",
        "        n_i = Data_i.shape[0]\n",
        "        if n_i == 0 or n_i == 1:\n",
        "            intra_dist = intra_dist\n",
        "        else:\n",
        "            intra_dist = intra_dist + 1 / k * 1 / (n_i * (n_i - 1)) * sum(sum(Dis))\n",
        "\n",
        "\n",
        "    return intra_dist\n",
        "\n",
        "datasets = ['cora', 'citeseer', 'pubmed', 'wiki']\n",
        "for dataset in datasets:\n",
        "    \n",
        "    #for i in range(10): dont have the computational capacities\n",
        "    data = sio.loadmat('./NodeClustering/{}.mat'.format(dataset))\n",
        "    feature = data['fea']\n",
        "    if sp.issparse(feature):\n",
        "        feature = feature.todense()\n",
        "\n",
        "    adj = data['W']\n",
        "    gnd = data['gnd']\n",
        "    gnd = gnd.T\n",
        "    gnd = gnd - 1\n",
        "    gnd = gnd[0, :]\n",
        "    k = len(np.unique(gnd))\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    intra_list = []\n",
        "    intra_list.append(10000)\n",
        "\n",
        "\n",
        "    acc_list = []\n",
        "    nmi_list = []\n",
        "    f1_list = []\n",
        "    stdacc_list = []\n",
        "    stdnmi_list = []\n",
        "    stdf1_list = []\n",
        "    max_iter = 60\n",
        "    rep = 10\n",
        "    t = time.time()\n",
        "    #adj_normalized = preprocess_adj(adj)\n",
        "    #adj_normalized = (sp.eye(adj_normalized.shape[0]) + adj_normalized) / 2\n",
        "    adj_normalized = preprocess_adj(adj,loop=False)\n",
        "    # adj_normalized = (sp.eye(adj_normalized.shape[0]) + adj_normalized + adj_normalized.dot(adj_normalized)) / 3\n",
        "    total_dist = []\n",
        "\n",
        "    tt = 0\n",
        "    alpha = 0.05 #0.05 for pubmed, 0.1 for citeseer\n",
        "    feature_ori = feature.astype('float64')\n",
        "    #emb = feature.astype('float64')\n",
        "    #emb = feature.astype('float64')\n",
        "    emb = np.zeros_like(feature).astype('float64')\n",
        "    oneV = np.ones(feature.shape[0])\n",
        "    den = np.zeros_like(oneV)\n",
        "    while 1:\n",
        "        tt = tt + 1\n",
        "        power = tt\n",
        "        intraD = np.zeros(rep)\n",
        "\n",
        "\n",
        "        ac = np.zeros(rep)\n",
        "        nm = np.zeros(rep)\n",
        "        f1 = np.zeros(rep)\n",
        "\n",
        "\n",
        "\n",
        "        feature = adj_normalized.dot(feature)\n",
        "\n",
        "        emb += feature\n",
        "        emb_norm = emb/tt\n",
        "        u, s, v = sp.linalg.svds(emb_norm, k=16, which='LM')\n",
        "        u = normalize(emb_norm.dot(v.T))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(rep):\n",
        "            kmeans = KMeans(n_clusters=k).fit(u)\n",
        "            predict_labels = kmeans.predict(u)\n",
        "            intraD[i] = square_dist(predict_labels, u)\n",
        "            #intraD[i] = dist(predict_labels, feature)\n",
        "            cm = clustering_metrics(gnd, predict_labels)\n",
        "            ac[i], nm[i], f1[i] = cm.evaluationClusterModelFromLabel()\n",
        "\n",
        "        intramean = np.mean(intraD)\n",
        "        acc_means = np.mean(ac)\n",
        "        acc_stds = np.std(ac)\n",
        "        nmi_means = np.mean(nm)\n",
        "        nmi_stds = np.std(nm)\n",
        "        f1_means = np.mean(f1)\n",
        "        f1_stds = np.std(f1)\n",
        "\n",
        "        intra_list.append(intramean)\n",
        "        acc_list.append(acc_means)\n",
        "        stdacc_list.append(acc_stds)\n",
        "        nmi_list.append(nmi_means)\n",
        "        stdnmi_list.append(nmi_stds)\n",
        "        f1_list.append(f1_means)\n",
        "        stdf1_list.append(f1_stds)\n",
        "        #print('power: {}'.format(power),\n",
        "        #      'intra_dist: {}'.format(intramean),\n",
        "        #      'acc_mean: {}'.format(acc_means),\n",
        "        #      'acc_std: {}'.format(acc_stds),\n",
        "        #      'nmi_mean: {}'.format(nmi_means),\n",
        "        #      'nmi_std: {}'.format(nmi_stds),\n",
        "        #      'f1_mean: {}'.format(f1_means),\n",
        "        #      'f1_std: {}'.format(f1_stds))\n",
        "\n",
        "        if intra_list[tt] > intra_list[tt - 1] or tt > max_iter:\n",
        "            #print('bestpower: {}'.format(tt - 1))\n",
        "            t = time.time() - t\n",
        "            #print(t)\n",
        "            break\n",
        "    print(\"dataset:\", dataset, \"acc:\", str(sum(acc_list)/len(acc_list)), \" nmi:\", str(sum(nmi_list)/len(nmi_list)), \"f1-score: \", str(sum(f1_list)/len(f1_list)))"
      ],
      "id": "a5c952f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62a9409d"
      },
      "source": [
        "# Document Classification\n",
        "Here we also have the problem that our resources are not sufficient enough to load the matrices into memory. To battle this problem the author even said that he generated and linked the preprocessed files he used to do the experiments. However, the link to these preprocessed files is not a valid one (he linked his own google drive path, which we cannot access). Link given: https://drive.google.com/drive/u/0/my-drive\n",
        "<br>\n",
        "<br>\n",
        "The results that we could observe (2/5) are successfully achieving the results described in the paper."
      ],
      "id": "62a9409d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "904afcea"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import tabulate\n",
        "from functools import partial\n",
        "from DocumentClassification.utils import *\n",
        "from DocumentClassification.models import SGC\n",
        "\n",
        "\n",
        "#torch.backends.cudnn.benchmark = True\n",
        "#set_seed(args.seed, args.cuda)\n",
        "\n",
        "def train_linear(model, feat_dict, weight_decay, binary=False):\n",
        "    if not binary:\n",
        "        act = partial(F.log_softmax, dim=1)\n",
        "        criterion = F.nll_loss\n",
        "    else:\n",
        "        act = torch.sigmoid\n",
        "        criterion = F.binary_cross_entropy\n",
        "    optimizer = optim.LBFGS(model.parameters())\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0\n",
        "    plateau = 0\n",
        "    start = time.perf_counter()\n",
        "    for epoch in range(10):\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            #output = model(feat_dict[\"train\"].cuda()).squeeze()\n",
        "            output = model(feat_dict[\"train\"]).squeeze()\n",
        "            l2_reg = 0.5*weight_decay*(model.W.weight**2).sum()\n",
        "            #loss = criterion(act(output), label_dict[\"train\"].cuda())+l2_reg\n",
        "            loss = criterion(act(output), label_dict[\"train\"])+l2_reg\n",
        "            loss.backward()\n",
        "            return loss\n",
        "\n",
        "        optimizer.step(closure)\n",
        "\n",
        "    train_time = time.perf_counter()-start\n",
        "    val_res = eval_linear(model, feat_dict[\"val\"],\n",
        "                          label_dict[\"val\"], binary)\n",
        "    # val_res = eval_linear(model, feat_dict[\"val\"],\n",
        "    #                       label_dict[\"val\"].cuda(), binary)\n",
        "    return val_res['accuracy'], model, train_time\n",
        "\n",
        "def eval_linear(model, features, label, binary=False):\n",
        "    model.eval()\n",
        "    if not binary:\n",
        "        act = partial(F.log_softmax, dim=1)\n",
        "        criterion = F.nll_loss\n",
        "    else:\n",
        "        act = torch.sigmoid\n",
        "        criterion = F.binary_cross_entropy\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(features).squeeze()\n",
        "        loss = criterion(act(output), label)\n",
        "        if not binary: predict_class = output.max(1)[1]\n",
        "        else: predict_class = act(output).gt(0.5).float()\n",
        "        correct = torch.eq(predict_class, label).long().sum().item()\n",
        "        acc = correct/predict_class.size(0)\n",
        "\n",
        "    return {\n",
        "        'loss': loss.item(),\n",
        "        'accuracy': acc\n",
        "    }"
      ],
      "id": "904afcea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "992228f2",
        "outputId": "5abd4b7e-c0f4-4d7b-bf23-aaa2512790be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Total Time: 97.069985s, Train acc: 1.0000, Val acc: 0.9471, Test acc: 0.9557\n"
          ]
        }
      ],
      "source": [
        "dataset = 'R8'\n",
        "\n",
        "sp_adj, index_dict, label_dict = load_corpus(dataset)\n",
        "for k, v in label_dict.items():\n",
        "    if dataset == \"mr\":\n",
        "        label_dict[k] = torch.Tensor(v)\n",
        "    else:\n",
        "        label_dict[k] = torch.LongTensor(v)\n",
        "features = torch.arange(sp_adj.shape[0])\n",
        "\n",
        "adj = sparse_to_torch_sparse(sp_adj, device=\"cpu\")\n",
        "\n",
        "if dataset == \"mr\": nclass = 1\n",
        "else: nclass = label_dict[\"train\"].max().item()+1\n",
        "#if not args.preprocessed:\n",
        "adj_dense = sparse_to_torch_dense(sp_adj, device='cpu')\n",
        "feat_dict, precompute_time = sgc_precompute(adj, adj_dense, 5-1, index_dict)\n",
        "#else:\n",
        "#    # load the relased degree 2 features\n",
        "#    with open(os.path.join(\"preprocessed\",\n",
        "#        \"{}.pkl\".format(args.dataset)), \"rb\") as prep:\n",
        "#        feat_dict =  pkl.load(prep)\n",
        "#    precompute_time = 0\n",
        "\n",
        "model = SGC(nfeat=feat_dict[\"train\"].size(1),\n",
        "            nclass=nclass)\n",
        "\n",
        "model\n",
        "val_acc, best_model, train_time = train_linear(model, feat_dict, 0, dataset==\"mr\")\n",
        "test_res = eval_linear(best_model, feat_dict[\"test\"],\n",
        "                       label_dict[\"test\"], dataset==\"mr\")\n",
        "train_res = eval_linear(best_model, feat_dict[\"train\"],\n",
        "                        label_dict[\"train\"], dataset==\"mr\")\n",
        "print(\"Total Time: {:2f}s, Train acc: {:.4f}, Val acc: {:.4f}, Test acc: {:.4f}\".format(precompute_time+train_time, train_res[\"accuracy\"], val_acc, test_res[\"accuracy\"]))"
      ],
      "id": "992228f2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "758f0254",
        "outputId": "81c53e94-759a-43c9-8edc-500bf9ffdb3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Total Time: 292.449571s, Train acc: 1.0000, Val acc: 0.9265, Test acc: 0.9330\n"
          ]
        }
      ],
      "source": [
        "dataset = 'R52'\n",
        "\n",
        "sp_adj, index_dict, label_dict = load_corpus(dataset)\n",
        "for k, v in label_dict.items():\n",
        "    if dataset == \"mr\":\n",
        "        label_dict[k] = torch.Tensor(v)\n",
        "    else:\n",
        "        label_dict[k] = torch.LongTensor(v)\n",
        "features = torch.arange(sp_adj.shape[0])\n",
        "\n",
        "adj = sparse_to_torch_sparse(sp_adj, device=\"cpu\")\n",
        "\n",
        "if dataset == \"mr\": nclass = 1\n",
        "else: nclass = label_dict[\"train\"].max().item()+1\n",
        "#if not args.preprocessed:\n",
        "adj_dense = sparse_to_torch_dense(sp_adj, device='cpu')\n",
        "feat_dict, precompute_time = sgc_precompute(adj, adj_dense, 5-1, index_dict)\n",
        "#else:\n",
        "#    # load the relased degree 2 features\n",
        "#    with open(os.path.join(\"preprocessed\",\n",
        "#        \"{}.pkl\".format(args.dataset)), \"rb\") as prep:\n",
        "#        feat_dict =  pkl.load(prep)\n",
        "#    precompute_time = 0\n",
        "\n",
        "model = SGC(nfeat=feat_dict[\"train\"].size(1),\n",
        "            nclass=nclass)\n",
        "\n",
        "model\n",
        "val_acc, best_model, train_time = train_linear(model, feat_dict, 0, dataset==\"mr\")\n",
        "test_res = eval_linear(best_model, feat_dict[\"test\"],\n",
        "                       label_dict[\"test\"], dataset==\"mr\")\n",
        "train_res = eval_linear(best_model, feat_dict[\"train\"],\n",
        "                        label_dict[\"train\"], dataset==\"mr\")\n",
        "print(\"Total Time: {:2f}s, Train acc: {:.4f}, Val acc: {:.4f}, Test acc: {:.4f}\".format(precompute_time+train_time, train_res[\"accuracy\"], val_acc, test_res[\"accuracy\"]))"
      ],
      "id": "758f0254"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c77c54d",
        "outputId": "65043fc9-77fe-4c18-c1a1-ae72c9ca2fe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Total Time: 232.598604s, Train acc: 1.0000, Val acc: 0.6776, Test acc: 0.6579\n"
          ]
        }
      ],
      "source": [
        "dataset = 'ohsumed'\n",
        "\n",
        "sp_adj, index_dict, label_dict = load_corpus(dataset)\n",
        "for k, v in label_dict.items():\n",
        "    if dataset == \"mr\":\n",
        "        label_dict[k] = torch.Tensor(v)\n",
        "    else:\n",
        "        label_dict[k] = torch.LongTensor(v)\n",
        "features = torch.arange(sp_adj.shape[0])\n",
        "\n",
        "adj = sparse_to_torch_sparse(sp_adj, device=\"cpu\")\n",
        "\n",
        "if dataset == \"mr\": nclass = 1\n",
        "else: nclass = label_dict[\"train\"].max().item()+1\n",
        "#if not args.preprocessed:\n",
        "adj_dense = sparse_to_torch_dense(sp_adj, device='cpu')\n",
        "feat_dict, precompute_time = sgc_precompute(adj, adj_dense, 5-1, index_dict)\n",
        "#else:\n",
        "#    # load the relased degree 2 features\n",
        "#    with open(os.path.join(\"preprocessed\",\n",
        "#        \"{}.pkl\".format(args.dataset)), \"rb\") as prep:\n",
        "#        feat_dict =  pkl.load(prep)\n",
        "#    precompute_time = 0\n",
        "\n",
        "model = SGC(nfeat=feat_dict[\"train\"].size(1),\n",
        "            nclass=nclass)\n",
        "\n",
        "model\n",
        "val_acc, best_model, train_time = train_linear(model, feat_dict, 0, dataset==\"mr\")\n",
        "test_res = eval_linear(best_model, feat_dict[\"test\"],\n",
        "                       label_dict[\"test\"], dataset==\"mr\")\n",
        "train_res = eval_linear(best_model, feat_dict[\"train\"],\n",
        "                        label_dict[\"train\"], dataset==\"mr\")\n",
        "print(\"Total Time: {:2f}s, Train acc: {:.4f}, Val acc: {:.4f}, Test acc: {:.4f}\".format(precompute_time+train_time, train_res[\"accuracy\"], val_acc, test_res[\"accuracy\"]))"
      ],
      "id": "7c77c54d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "004f59e0",
        "outputId": "0af7b132-9336-476a-c1e9-e62cd56e6e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Total Time: 84.841316s, Train acc: 1.0000, Val acc: 0.7042, Test acc: 0.6998\n"
          ]
        }
      ],
      "source": [
        "dataset = 'mr'\n",
        "\n",
        "sp_adj, index_dict, label_dict = load_corpus(dataset)\n",
        "for k, v in label_dict.items():\n",
        "    if dataset == \"mr\":\n",
        "        label_dict[k] = torch.Tensor(v)\n",
        "    else:\n",
        "        label_dict[k] = torch.LongTensor(v)\n",
        "features = torch.arange(sp_adj.shape[0])\n",
        "\n",
        "adj = sparse_to_torch_sparse(sp_adj, device=\"cpu\")\n",
        "\n",
        "if dataset == \"mr\": nclass = 1\n",
        "else: nclass = label_dict[\"train\"].max().item()+1\n",
        "#if not args.preprocessed:\n",
        "adj_dense = sparse_to_torch_dense(sp_adj, device='cpu')\n",
        "feat_dict, precompute_time = sgc_precompute(adj, adj_dense, 5-1, index_dict)\n",
        "#else:\n",
        "#    # load the relased degree 2 features\n",
        "#    with open(os.path.join(\"preprocessed\",\n",
        "#        \"{}.pkl\".format(args.dataset)), \"rb\") as prep:\n",
        "#        feat_dict =  pkl.load(prep)\n",
        "#    precompute_time = 0\n",
        "\n",
        "model = SGC(nfeat=feat_dict[\"train\"].size(1),\n",
        "            nclass=nclass)\n",
        "\n",
        "model\n",
        "val_acc, best_model, train_time = train_linear(model, feat_dict, 0, dataset==\"mr\")\n",
        "test_res = eval_linear(best_model, feat_dict[\"test\"],\n",
        "                       label_dict[\"test\"], dataset==\"mr\")\n",
        "train_res = eval_linear(best_model, feat_dict[\"train\"],\n",
        "                        label_dict[\"train\"], dataset==\"mr\")\n",
        "print(\"Total Time: {:2f}s, Train acc: {:.4f}, Val acc: {:.4f}, Test acc: {:.4f}\".format(precompute_time+train_time, train_res[\"accuracy\"], val_acc, test_res[\"accuracy\"]))"
      ],
      "id": "004f59e0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66e07781",
        "outputId": "004b7575-8b14-4c27-868f-b2d662d780c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "dataset = '20ng'\n",
        "\n",
        "sp_adj, index_dict, label_dict = load_corpus(dataset)\n",
        "for k, v in label_dict.items():\n",
        "    if dataset == \"mr\":\n",
        "        label_dict[k] = torch.Tensor(v)\n",
        "    else:\n",
        "        label_dict[k] = torch.LongTensor(v)\n",
        "features = torch.arange(sp_adj.shape[0])\n",
        "\n",
        "adj = sparse_to_torch_sparse(sp_adj, device=\"cpu\")\n",
        "\n",
        "if dataset == \"mr\": nclass = 1\n",
        "else: nclass = label_dict[\"train\"].max().item()+1\n",
        "#if not args.preprocessed:\n",
        "adj_dense = sparse_to_torch_dense(sp_adj, device='cpu')\n",
        "feat_dict, precompute_time = sgc_precompute(adj, adj_dense, 5-1, index_dict)\n",
        "#else:\n",
        "#    # load the relased degree 2 features\n",
        "#    with open(os.path.join(\"preprocessed\",\n",
        "#        \"{}.pkl\".format(args.dataset)), \"rb\") as prep:\n",
        "#        feat_dict =  pkl.load(prep)\n",
        "#    precompute_time = 0\n",
        "\n",
        "model = SGC(nfeat=feat_dict[\"train\"].size(1),\n",
        "            nclass=nclass)\n",
        "\n",
        "model\n",
        "val_acc, best_model, train_time = train_linear(model, feat_dict, 0, dataset==\"mr\")\n",
        "test_res = eval_linear(best_model, feat_dict[\"test\"],\n",
        "                       label_dict[\"test\"], dataset==\"mr\")\n",
        "train_res = eval_linear(best_model, feat_dict[\"train\"],\n",
        "                        label_dict[\"train\"], dataset==\"mr\")\n",
        "print(\"Total Time: {:2f}s, Train acc: {:.4f}, Val acc: {:.4f}, Test acc: {:.4f}\".format(precompute_time+train_time, train_res[\"accuracy\"], val_acc, test_res[\"accuracy\"]))"
      ],
      "id": "66e07781"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SSGC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}